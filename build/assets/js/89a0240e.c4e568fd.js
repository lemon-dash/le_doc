"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1210],{4186:(n,r,e)=>{e.r(r),e.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>f,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"graphics/atmosphere","title":"atmosphere","description":"\u6c14\u8c61","source":"@site/docs/graphics/atmosphere.md","sourceDirName":"graphics","slug":"/graphics/atmosphere","permalink":"/docs/graphics/atmosphere","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"video_gen","permalink":"/docs/graphics/video_gen"}}');var i=e(4848),t=e(8453);const a={},o=void 0,l={},c=[{value:"\u6c14\u8c61",id:"\u6c14\u8c61",level:2},{value:"retinaNet",id:"rn",level:4},{value:"Yolov5\u68c0\u6d4b\u6a21\u578b",id:"yv5",level:4},{value:"yolov7",id:"yv7",level:4},{value:"detectron2",id:"d2",level:4},{value:"FastMaskRCNN-master",id:"fmrcm",level:4},{value:"efficientNet80",id:"en80",level:4},{value:"CNN",id:"cnn",level:4},{value:"\u96c6\u6210\u6a21\u578b(lr, svm,rf,knn,dt,nb,mlp)",id:"jcmx",level:4},{value:"vit_L16",id:"vl16",level:4},{value:"densenet",id:"dn",level:4},{value:"googlenet",id:"gn",level:4},{value:"MoblieNetV3",id:"mnv3",level:4},{value:"ResNet18",id:"rn18",level:4},{value:"ResNet50",id:"rn50",level:4},{value:"VGG16",id:"v16",level:4}];function d(n){const r={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.h2,{id:"\u6c14\u8c61",children:"\u6c14\u8c61"}),"\n",(0,i.jsx)(r.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d]../../atmosphere.zip)"}),"\n",(0,i.jsx)(r.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d(resnet)]../../resnet.zip)"}),"\n",(0,i.jsx)(r.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d(mask-rcnn.zip)]../../mask_rcnn.zip)"}),"\n",(0,i.jsx)(r.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d(yolov5-master)](../../yolov5-master.zip"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#rn",children:"retinaNet"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#yv5",children:"Yolov5"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#yv7",children:"Yolov7"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#d2",children:"detectron2"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#fmrcm",children:"fastMaskRCNN-master"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#en80",children:"efficientNet80"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#cnn",children:"CNN"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#jcmx",children:"\u96c6\u6210\u6a21\u578b(lr,svm,rf,knn,dt,nb,mlp)"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#vl16",children:"vit_L16"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#dn",children:"densenet"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#gn",children:"googlenet"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#mnv3",children:"mobileNetV3"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#rn",children:"resNet"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#rn50",children:"resNet50"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"#v16",children:"VGG16"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.h4,{id:"rn",children:"retinaNet"}),"\n",(0,i.jsx)(r.p,{children:"\u7ec4\u5408\u591a\u4e2atransformer\u6a21\u578b"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'import random\r\nfrom torchvision.transforms import functional as F\r\n\r\n\r\nclass Compose(object):\r\n    """\u7ec4\u5408\u591a\u4e2atransform\u51fd\u6570"""\r\n    def __init__(self, transforms):\r\n        self.transforms = transforms\r\n\r\n    def __call__(self, image, target):\r\n        for t in self.transforms:\r\n            image, target = t(image, target)\r\n        return image, target\r\n\r\n\r\nclass ToTensor(object):\r\n    """\u5c06PIL\u56fe\u50cf\u8f6c\u4e3aTensor"""\r\n    def __call__(self, image, target):\r\n        image = F.to_tensor(image)\r\n        return image, target\r\n\r\n\r\nclass RandomHorizontalFlip(object):\r\n    """\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf\u4ee5\u53cabboxes"""\r\n    def __init__(self, prob=0.5):\r\n        self.prob = prob\r\n\r\n    def __call__(self, image, target):\r\n        if random.random() < self.prob:\r\n            height, width = image.shape[-2:]\r\n            image = image.flip(-1)  # \u6c34\u5e73\u7ffb\u8f6c\u56fe\u7247\r\n            bbox = target["boxes"]\r\n            # bbox: xmin, ymin, xmax, ymax\r\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]  # \u7ffb\u8f6c\u5bf9\u5e94bbox\u5750\u6807\u4fe1\u606f\r\n            target["boxes"] = bbox\r\n        return image, target\r\n\n'})}),"\n",(0,i.jsx)(r.p,{children:"\u521b\u5efaretinanet_res50_fpn\u6a21\u578b"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import os\r\nimport time\r\nimport datetime\r\n\r\nimport torch\r\n\r\nimport transforms\r\nfrom backbone import resnet50_fpn_backbone, LastLevelP6P7\r\nfrom network_files import RetinaNet\r\nfrom my_dataset import VOCDataSet\r\nfrom train_utils import train_eval_utils as utils\r\nfrom train_utils import GroupedBatchSampler, create_aspect_ratio_groups, init_distributed_mode, save_on_master, mkdir\r\n\r\n\r\ndef create_model(num_classes):\r\n    # \u521b\u5efaretinanet_res50_fpn\u6a21\u578b\r\n    # skip P2 because it generates too many anchors (according to their paper)\r\n    # \u6ce8\u610f\uff0c\u8fd9\u91cc\u7684backbone\u9ed8\u8ba4\u4f7f\u7528\u7684\u662fFrozenBatchNorm2d\uff0c\u5373\u4e0d\u4f1a\u53bb\u66f4\u65b0bn\u53c2\u6570\r\n    # \u76ee\u7684\u662f\u4e3a\u4e86\u9632\u6b62batch_size\u592a\u5c0f\u5bfc\u81f4\u6548\u679c\u66f4\u5dee(\u5982\u679c\u663e\u5b58\u5f88\u5c0f\uff0c\u5efa\u8bae\u4f7f\u7528\u9ed8\u8ba4\u7684FrozenBatchNorm2d)\r\n    # \u5982\u679cGPU\u663e\u5b58\u5f88\u5927\u53ef\u4ee5\u8bbe\u7f6e\u6bd4\u8f83\u5927\u7684batch_size\u5c31\u53ef\u4ee5\u5c06norm_layer\u8bbe\u7f6e\u4e3a\u666e\u901a\u7684BatchNorm2d\r\n    backbone = resnet50_fpn_backbone(norm_layer=torch.nn.BatchNorm2d,\r\n                                     returned_layers=[2, 3, 4],\r\n                                     extra_blocks=LastLevelP6P7(256, 256),\r\n                                     trainable_layers=3)\r\n    model = RetinaNet(backbone, num_classes)\r\n\r\n    # \u8f7d\u5165\u9884\u8bad\u7ec3\u6743\u91cd\r\n    # https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\r\n    weights_dict = torch.load(\"./backbone/retinanet_resnet50_fpn.pth\", map_location='cpu')\r\n    # \u5220\u9664\u5206\u7c7b\u5668\u90e8\u5206\u7684\u6743\u91cd\uff0c\u56e0\u4e3a\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b\u522b\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7c7b\u522b(91)\u4e0d\u4e00\u5b9a\u81f4\uff0c\u5982\u679c\u8f7d\u5165\u4f1a\u51fa\u73b0\u51b2\u7a81\r\n    del_keys = [\"head.classification_head.cls_logits.weight\", \"head.classification_head.cls_logits.bias\"]\r\n    for k in del_keys:\r\n        del weights_dict[k]\r\n    print(model.load_state_dict(weights_dict, strict=False))\r\n\r\n    return model\r\n\r\n\r\ndef main(args):\r\n    init_distributed_mode(args)\r\n    print(args)\r\n\r\n    device = torch.device(args.device)\r\n\r\n    # \u7528\u6765\u4fdd\u5b58coco_info\u7684\u6587\u4ef6\r\n    results_file = \"results{}.txt\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n\r\n    # Data loading code\r\n    print(\"Loading data\")\r\n\r\n    data_transform = {\r\n        \"train\": transforms.Compose([transforms.ToTensor(),\r\n                                     transforms.RandomHorizontalFlip(0.5)]),\r\n        \"val\": transforms.Compose([transforms.ToTensor()])\r\n    }\r\n\r\n    VOC_root = args.data_path\r\n    # check voc root\r\n    if os.path.exists(os.path.join(VOC_root, \"VOCdevkit\")) is False:\r\n        raise FileNotFoundError(\"VOCdevkit dose not in path:'{}'.\".format(VOC_root))\r\n\r\n    # load train data set\r\n    # VOCdevkit -> VOC2012 -> ImageSets -> Main -> train.txt\r\n    train_dataset = VOCDataSet(VOC_root, \"2012\", data_transform[\"train\"], \"train.txt\")\r\n\r\n    # load validation data set\r\n    # VOCdevkit -> VOC2012 -> ImageSets -> Main -> val.txt\r\n    val_dataset = VOCDataSet(VOC_root, \"2012\", data_transform[\"val\"], \"val.txt\")\r\n\r\n    print(\"Creating data loaders\")\r\n    if args.distributed:\r\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n        test_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\r\n    else:\r\n        train_sampler = torch.utils.data.RandomSampler(train_dataset)\r\n        test_sampler = torch.utils.data.SequentialSampler(val_dataset)\r\n\r\n    if args.aspect_ratio_group_factor >= 0:\r\n        # \u7edf\u8ba1\u6240\u6709\u56fe\u50cf\u6bd4\u4f8b\u5728bins\u533a\u95f4\u4e2d\u7684\u4f4d\u7f6e\u7d22\u5f15\r\n        group_ids = create_aspect_ratio_groups(train_dataset, k=args.aspect_ratio_group_factor)\r\n        train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, args.batch_size)\r\n    else:\r\n        train_batch_sampler = torch.utils.data.BatchSampler(\r\n            train_sampler, args.batch_size, drop_last=True)\r\n\r\n    data_loader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_sampler=train_batch_sampler, num_workers=args.workers,\r\n        collate_fn=train_dataset.collate_fn)\r\n\r\n    data_loader_test = torch.utils.data.DataLoader(\r\n        val_dataset, batch_size=1,\r\n        sampler=test_sampler, num_workers=args.workers,\r\n        collate_fn=train_dataset.collate_fn)\r\n\r\n    print(\"Creating model\")\r\n    # create model\r\n    # \u6ce8\u610f\uff1a\u4e0d\u5305\u542b\u80cc\u666f\r\n    model = create_model(num_classes=args.num_classes)\r\n    model.to(device)\r\n\r\n    model_without_ddp = model\r\n    if args.distributed:\r\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\r\n        model_without_ddp = model.module\r\n\r\n    params = [p for p in model.parameters() if p.requires_grad]\r\n    optimizer = torch.optim.SGD(\r\n        params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\r\n\r\n    scaler = torch.cuda.amp.GradScaler() if args.amp else None\r\n\r\n    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)\r\n    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma)\r\n\r\n    # \u5982\u679c\u4f20\u5165resume\u53c2\u6570\uff0c\u5373\u4e0a\u6b21\u8bad\u7ec3\u7684\u6743\u91cd\u5730\u5740\uff0c\u5219\u63a5\u7740\u4e0a\u6b21\u7684\u53c2\u6570\u8bad\u7ec3\r\n    if args.resume:\r\n        # If map_location is missing, torch.load will first load the module to CPU\r\n        # and then copy each parameter to where it was saved,\r\n        # which would result in all processes on the same machine using the same set of devices.\r\n        checkpoint = torch.load(args.resume, map_location='cpu')  # \u8bfb\u53d6\u4e4b\u524d\u4fdd\u5b58\u7684\u6743\u91cd\u6587\u4ef6(\u5305\u62ec\u4f18\u5316\u5668\u4ee5\u53ca\u5b66\u4e60\u7387\u7b56\u7565)\r\n        model_without_ddp.load_state_dict(checkpoint['model'])\r\n        optimizer.load_state_dict(checkpoint['optimizer'])\r\n        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\r\n        args.start_epoch = checkpoint['epoch'] + 1\r\n        if args.amp and \"scaler\" in checkpoint:\r\n            scaler.load_state_dict(checkpoint[\"scaler\"])\r\n\r\n    if args.test_only:\r\n        utils.evaluate(model, data_loader_test, device=device)\r\n        return\r\n\r\n    train_loss = []\r\n    learning_rate = []\r\n    val_map = []\r\n\r\n    print(\"Start training\")\r\n    start_time = time.time()\r\n    for epoch in range(args.start_epoch, args.epochs):\r\n        if args.distributed:\r\n            train_sampler.set_epoch(epoch)\r\n        mean_loss, lr = utils.train_one_epoch(model, optimizer, data_loader,\r\n                                              device, epoch, args.print_freq,\r\n                                              warmup=True, scaler=scaler)\r\n        train_loss.append(mean_loss.item())\r\n        learning_rate.append(lr)\r\n\r\n        # update learning rate\r\n        lr_scheduler.step()\r\n\r\n        # evaluate after every epoch\r\n        coco_info = utils.evaluate(model, data_loader_test, device=device)\r\n        val_map.append(coco_info[1])  # pascal mAP\r\n\r\n        # \u53ea\u5728\u4e3b\u8fdb\u7a0b\u4e0a\u8fdb\u884c\u5199\u64cd\u4f5c\r\n        if args.rank in [-1, 0]:\r\n            # write into txt\r\n            with open(results_file, \"a\") as f:\r\n                # \u5199\u5165\u7684\u6570\u636e\u5305\u62eccoco\u6307\u6807\u8fd8\u6709loss\u548clearning rate\r\n                result_info = [f\"{i:.4f}\" for i in coco_info + [mean_loss.item()]] + [f\"{lr:.6f}\"]\r\n                txt = \"epoch:{} {}\".format(epoch, '  '.join(result_info))\r\n                f.write(txt + \"\\n\")\r\n\r\n        if args.output_dir:\r\n            # \u53ea\u5728\u4e3b\u8282\u70b9\u4e0a\u6267\u884c\u4fdd\u5b58\u6743\u91cd\u64cd\u4f5c\r\n            save_files = {\r\n                'model': model_without_ddp.state_dict(),\r\n                'optimizer': optimizer.state_dict(),\r\n                'lr_scheduler': lr_scheduler.state_dict(),\r\n                'args': args,\r\n                'epoch': epoch}\r\n            if args.amp:\r\n                save_files[\"scaler\"] = scaler.state_dict()\r\n            save_on_master(save_files,\r\n                           os.path.join(args.output_dir, f'model_{epoch}.pth'))\r\n\r\n    total_time = time.time() - start_time\r\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\r\n    print('Training time {}'.format(total_time_str))\r\n\r\n    if args.rank in [-1, 0]:\r\n        # plot loss and lr curve\r\n        if len(train_loss) != 0 and len(learning_rate) != 0:\r\n            from plot_curve import plot_loss_and_lr\r\n            plot_loss_and_lr(train_loss, learning_rate)\r\n\r\n        # plot mAP curve\r\n        if len(val_map) != 0:\r\n            from plot_curve import plot_map\r\n            plot_map(val_map)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser(\r\n        description=__doc__)\r\n\r\n    # \u8bad\u7ec3\u6587\u4ef6\u7684\u6839\u76ee\u5f55(VOCdevkit)\r\n    parser.add_argument('--data-path', default='/data', help='dataset')\r\n    # \u8bad\u7ec3\u8bbe\u5907\u7c7b\u578b\r\n    parser.add_argument('--device', default='cuda', help='device')\r\n    # \u68c0\u6d4b\u76ee\u6807\u7c7b\u522b\u6570(\u4e0d\u5305\u542b\u80cc\u666f)\r\n    parser.add_argument('--num-classes', default=20, type=int, help='num_classes')\r\n    # \u6bcf\u5757GPU\u4e0a\u7684batch_size\r\n    parser.add_argument('-b', '--batch-size', default=4, type=int,\r\n                        help='images per gpu, the total batch size is $NGPU x batch_size')\r\n    # \u6307\u5b9a\u63a5\u7740\u4ece\u54ea\u4e2aepoch\u6570\u5f00\u59cb\u8bad\u7ec3\r\n    parser.add_argument('--start_epoch', default=0, type=int, help='start epoch')\r\n    # \u8bad\u7ec3\u7684\u603bepoch\u6570\r\n    parser.add_argument('--epochs', default=20, type=int, metavar='N',\r\n                        help='number of total epochs to run')\r\n    # \u6570\u636e\u52a0\u8f7d\u4ee5\u53ca\u9884\u5904\u7406\u7684\u7ebf\u7a0b\u6570\r\n    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\r\n                        help='number of data loading workers (default: 4)')\r\n    # \u5b66\u4e60\u7387\uff0c\u8fd9\u4e2a\u9700\u8981\u6839\u636egpu\u7684\u6570\u91cf\u4ee5\u53cabatch_size\u8fdb\u884c\u8bbe\u7f6e0.02 / 8 * num_GPU\r\n    parser.add_argument('--lr', default=0.02, type=float,\r\n                        help='initial learning rate, 0.02 is the default value for training '\r\n                             'on 8 gpus and 2 images_per_gpu')\r\n    # SGD\u7684momentum\u53c2\u6570\r\n    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\r\n                        help='momentum')\r\n    # SGD\u7684weight_decay\u53c2\u6570\r\n    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\r\n                        metavar='W', help='weight decay (default: 1e-4)',\r\n                        dest='weight_decay')\r\n    # \u9488\u5bf9torch.optim.lr_scheduler.StepLR\u7684\u53c2\u6570\r\n    parser.add_argument('--lr-step-size', default=8, type=int, help='decrease lr every step-size epochs')\r\n    # \u9488\u5bf9torch.optim.lr_scheduler.MultiStepLR\u7684\u53c2\u6570\r\n    parser.add_argument('--lr-steps', default=[7, 12], nargs='+', type=int, help='decrease lr every step-size epochs')\r\n    # \u9488\u5bf9torch.optim.lr_scheduler.MultiStepLR\u7684\u53c2\u6570\r\n    parser.add_argument('--lr-gamma', default=0.1, type=float, help='decrease lr by a factor of lr-gamma')\r\n    # \u8bad\u7ec3\u8fc7\u7a0b\u6253\u5370\u4fe1\u606f\u7684\u9891\u7387\r\n    parser.add_argument('--print-freq', default=20, type=int, help='print frequency')\r\n    # \u6587\u4ef6\u4fdd\u5b58\u5730\u5740\r\n    parser.add_argument('--output-dir', default='./multi_train', help='path where to save')\r\n    # \u57fa\u4e8e\u4e0a\u6b21\u7684\u8bad\u7ec3\u7ed3\u679c\u63a5\u7740\u8bad\u7ec3\r\n    parser.add_argument('--resume', default='', help='resume from checkpoint')\r\n    parser.add_argument('--aspect-ratio-group-factor', default=3, type=int)\r\n    # \u4e0d\u8bad\u7ec3\uff0c\u4ec5\u6d4b\u8bd5\r\n    parser.add_argument(\r\n        \"--test-only\",\r\n        dest=\"test_only\",\r\n        help=\"Only test the model\",\r\n        action=\"store_true\",\r\n    )\r\n\r\n    # \u5f00\u542f\u7684\u8fdb\u7a0b\u6570(\u6ce8\u610f\u4e0d\u662f\u7ebf\u7a0b)\r\n    parser.add_argument('--world-size', default=4, type=int,\r\n                        help='number of distributed processes')\r\n    parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')\r\n    # \u662f\u5426\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3(\u9700\u8981GPU\u652f\u6301\u6df7\u5408\u7cbe\u5ea6)\r\n    parser.add_argument(\"--amp\", default=False, help=\"Use torch.cuda.amp for mixed precision training\")\r\n\r\n    args = parser.parse_args()\r\n\r\n    # \u5982\u679c\u6307\u5b9a\u4e86\u4fdd\u5b58\u6587\u4ef6\u5730\u5740\uff0c\u68c0\u67e5\u6587\u4ef6\u5939\u662f\u5426\u5b58\u5728\uff0c\u82e5\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\r\n    if args.output_dir:\r\n        mkdir(args.output_dir)\r\n\r\n    main(args)\n"})}),"\n",(0,i.jsx)(r.h4,{id:"yv5",children:"Yolov5\u68c0\u6d4b\u6a21\u578b"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"# YOLOv5 \ud83d\ude80 by Ultralytics, AGPL-3.0 license\r\n\"\"\"\r\nYOLO-specific modules\r\n\r\nUsage:\r\n    $ python models/yolo.py --cfg yolov5s.yaml\r\n\"\"\"\r\n\r\nimport argparse\r\nimport contextlib\r\nimport os\r\nimport platform\r\nimport sys\r\nfrom copy import deepcopy\r\nfrom pathlib import Path\r\n\r\nFILE = Path(__file__).resolve()\r\nROOT = FILE.parents[1]  # YOLOv5 root directory\r\nif str(ROOT) not in sys.path:\r\n    sys.path.append(str(ROOT))  # add ROOT to PATH\r\nif platform.system() != 'Windows':\r\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\r\n\r\nfrom models.common import *  # noqa\r\nfrom models.experimental import *  # noqa\r\nfrom utils.autoanchor import check_anchor_order\r\nfrom utils.general import LOGGER, check_version, check_yaml, make_divisible, print_args\r\nfrom utils.plots import feature_visualization\r\nfrom utils.torch_utils import (fuse_conv_and_bn, initialize_weights, model_info, profile, scale_img, select_device,\r\n                               time_sync)\r\n\r\ntry:\r\n    import thop  # for FLOPs computation\r\nexcept ImportError:\r\n    thop = None\r\n\r\n\r\nclass Detect(nn.Module):\r\n    # YOLOv5 Detect head for detection models\r\n    stride = None  # strides computed during build\r\n    dynamic = False  # force grid reconstruction\r\n    export = False  # export mode\r\n\r\n    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\r\n        super().__init__()\r\n        self.nc = nc  # number of classes\r\n        self.no = nc + 5  # number of outputs per anchor\r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.empty(0) for _ in range(self.nl)]  # init grid\r\n        self.anchor_grid = [torch.empty(0) for _ in range(self.nl)]  # init anchor grid\r\n        self.register_buffer('anchors', torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\r\n        self.inplace = inplace  # use inplace ops (e.g. slice assignment)\r\n\r\n    def forward(self, x):\r\n        z = []  # inference output\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](x[i])  # conv\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\r\n\r\n                if isinstance(self, Segment):  # (boxes + masks)\r\n                    xy, wh, conf, mask = x[i].split((2, 2, self.nc + 1, self.no - self.nc - 5), 4)\r\n                    xy = (xy.sigmoid() * 2 + self.grid[i]) * self.stride[i]  # xy\r\n                    wh = (wh.sigmoid() * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                    y = torch.cat((xy, wh, conf.sigmoid(), mask), 4)\r\n                else:  # Detect (boxes only)\r\n                    xy, wh, conf = x[i].sigmoid().split((2, 2, self.nc + 1), 4)\r\n                    xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy\r\n                    wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                    y = torch.cat((xy, wh, conf), 4)\r\n                z.append(y.view(bs, self.na * nx * ny, self.no))\r\n\r\n        return x if self.training else (torch.cat(z, 1), ) if self.export else (torch.cat(z, 1), x)\r\n\r\n    def _make_grid(self, nx=20, ny=20, i=0, torch_1_10=check_version(torch.__version__, '1.10.0')):\r\n        d = self.anchors[i].device\r\n        t = self.anchors[i].dtype\r\n        shape = 1, self.na, ny, nx, 2  # grid shape\r\n        y, x = torch.arange(ny, device=d, dtype=t), torch.arange(nx, device=d, dtype=t)\r\n        yv, xv = torch.meshgrid(y, x, indexing='ij') if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility\r\n        grid = torch.stack((xv, yv), 2).expand(shape) - 0.5  # add grid offset, i.e. y = 2.0 * x - 0.5\r\n        anchor_grid = (self.anchors[i] * self.stride[i]).view((1, self.na, 1, 1, 2)).expand(shape)\r\n        return grid, anchor_grid\r\n\r\n\r\nclass Segment(Detect):\r\n    # YOLOv5 Segment head for segmentation models\r\n    def __init__(self, nc=80, anchors=(), nm=32, npr=256, ch=(), inplace=True):\r\n        super().__init__(nc, anchors, ch, inplace)\r\n        self.nm = nm  # number of masks\r\n        self.npr = npr  # number of protos\r\n        self.no = 5 + nc + self.nm  # number of outputs per anchor\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\r\n        self.proto = Proto(ch[0], self.npr, self.nm)  # protos\r\n        self.detect = Detect.forward\r\n\r\n    def forward(self, x):\r\n        p = self.proto(x[0])\r\n        x = self.detect(self, x)\r\n        return (x, p) if self.training else (x[0], p) if self.export else (x[0], p, x[1])\r\n\r\n\r\nclass BaseModel(nn.Module):\r\n    # YOLOv5 base model\r\n    def forward(self, x, profile=False, visualize=False):\r\n        return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n\r\n    def _forward_once(self, x, profile=False, visualize=False):\r\n        y, dt = [], []  # outputs\r\n        for m in self.model:\r\n            if m.f != -1:  # if not from previous layer\r\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\r\n            if profile:\r\n                self._profile_one_layer(m, x, dt)\r\n            x = m(x)  # run\r\n            y.append(x if m.i in self.save else None)  # save output\r\n            if visualize:\r\n                feature_visualization(x, m.type, m.i, save_dir=visualize)\r\n        return x\r\n\r\n    def _profile_one_layer(self, m, x, dt):\r\n        c = m == self.model[-1]  # is final layer, copy input as inplace fix\r\n        o = thop.profile(m, inputs=(x.copy() if c else x, ), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs\r\n        t = time_sync()\r\n        for _ in range(10):\r\n            m(x.copy() if c else x)\r\n        dt.append((time_sync() - t) * 100)\r\n        if m == self.model[0]:\r\n            LOGGER.info(f\"{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module\")\r\n        LOGGER.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')\r\n        if c:\r\n            LOGGER.info(f\"{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total\")\r\n\r\n    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\r\n        LOGGER.info('Fusing layers... ')\r\n        for m in self.model.modules():\r\n            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\r\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\r\n                delattr(m, 'bn')  # remove batchnorm\r\n                m.forward = m.forward_fuse  # update forward\r\n        self.info()\r\n        return self\r\n\r\n    def info(self, verbose=False, img_size=640):  # print model information\r\n        model_info(self, verbose, img_size)\r\n\r\n    def _apply(self, fn):\r\n        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers\r\n        self = super()._apply(fn)\r\n        m = self.model[-1]  # Detect()\r\n        if isinstance(m, (Detect, Segment)):\r\n            m.stride = fn(m.stride)\r\n            m.grid = list(map(fn, m.grid))\r\n            if isinstance(m.anchor_grid, list):\r\n                m.anchor_grid = list(map(fn, m.anchor_grid))\r\n        return self\r\n\r\n\r\nclass DetectionModel(BaseModel):\r\n    # YOLOv5 detection model\r\n    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\r\n        super().__init__()\r\n        if isinstance(cfg, dict):\r\n            self.yaml = cfg  # model dict\r\n        else:  # is *.yaml\r\n            import yaml  # for torch hub\r\n            self.yaml_file = Path(cfg).name\r\n            with open(cfg, encoding='ascii', errors='ignore') as f:\r\n                self.yaml = yaml.safe_load(f)  # model dict\r\n\r\n        # Define model\r\n        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\r\n        if nc and nc != self.yaml['nc']:\r\n            LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\r\n            self.yaml['nc'] = nc  # override yaml value\r\n        if anchors:\r\n            LOGGER.info(f'Overriding model.yaml anchors with anchors={anchors}')\r\n            self.yaml['anchors'] = round(anchors)  # override yaml value\r\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\r\n        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\r\n        self.inplace = self.yaml.get('inplace', True)\r\n\r\n        # Build strides, anchors\r\n        m = self.model[-1]  # Detect()\r\n        if isinstance(m, (Detect, Segment)):\r\n            s = 256  # 2x min stride\r\n            m.inplace = self.inplace\r\n            forward = lambda x: self.forward(x)[0] if isinstance(m, Segment) else self.forward(x)\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases()  # only run once\r\n\r\n        # Init weights, biases\r\n        initialize_weights(self)\r\n        self.info()\r\n        LOGGER.info('')\r\n\r\n    def forward(self, x, augment=False, profile=False, visualize=False):\r\n        if augment:\r\n            return self._forward_augment(x)  # augmented inference, None\r\n        return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n\r\n    def _forward_augment(self, x):\r\n        img_size = x.shape[-2:]  # height, width\r\n        s = [1, 0.83, 0.67]  # scales\r\n        f = [None, 3, None]  # flips (2-ud, 3-lr)\r\n        y = []  # outputs\r\n        for si, fi in zip(s, f):\r\n            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\r\n            yi = self._forward_once(xi)[0]  # forward\r\n            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\r\n            yi = self._descale_pred(yi, fi, si, img_size)\r\n            y.append(yi)\r\n        y = self._clip_augmented(y)  # clip augmented tails\r\n        return torch.cat(y, 1), None  # augmented inference, train\r\n\r\n    def _descale_pred(self, p, flips, scale, img_size):\r\n        # de-scale predictions following augmented inference (inverse operation)\r\n        if self.inplace:\r\n            p[..., :4] /= scale  # de-scale\r\n            if flips == 2:\r\n                p[..., 1] = img_size[0] - p[..., 1]  # de-flip ud\r\n            elif flips == 3:\r\n                p[..., 0] = img_size[1] - p[..., 0]  # de-flip lr\r\n        else:\r\n            x, y, wh = p[..., 0:1] / scale, p[..., 1:2] / scale, p[..., 2:4] / scale  # de-scale\r\n            if flips == 2:\r\n                y = img_size[0] - y  # de-flip ud\r\n            elif flips == 3:\r\n                x = img_size[1] - x  # de-flip lr\r\n            p = torch.cat((x, y, wh, p[..., 4:]), -1)\r\n        return p\r\n\r\n    def _clip_augmented(self, y):\r\n        # Clip YOLOv5 augmented inference tails\r\n        nl = self.model[-1].nl  # number of detection layers (P3-P5)\r\n        g = sum(4 ** x for x in range(nl))  # grid points\r\n        e = 1  # exclude layer count\r\n        i = (y[0].shape[1] // g) * sum(4 ** x for x in range(e))  # indices\r\n        y[0] = y[0][:, :-i]  # large\r\n        i = (y[-1].shape[1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indices\r\n        y[-1] = y[-1][:, i:]  # small\r\n        return y\r\n\r\n    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\r\n        # https://arxiv.org/abs/1708.02002 section 3.3\r\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\r\n        m = self.model[-1]  # Detect() module\r\n        for mi, s in zip(m.m, m.stride):  # from\r\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b.data[:, 5:5 + m.nc] += math.log(0.6 / (m.nc - 0.99999)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\r\n\r\n\r\nModel = DetectionModel  # retain YOLOv5 'Model' class for backwards compatibility\r\n\r\n\r\nclass SegmentationModel(DetectionModel):\r\n    # YOLOv5 segmentation model\r\n    def __init__(self, cfg='yolov5s-seg.yaml', ch=3, nc=None, anchors=None):\r\n        super().__init__(cfg, ch, nc, anchors)\r\n\r\n\r\nclass ClassificationModel(BaseModel):\r\n    # YOLOv5 classification model\r\n    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):  # yaml, model, number of classes, cutoff index\r\n        super().__init__()\r\n        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)\r\n\r\n    def _from_detection_model(self, model, nc=1000, cutoff=10):\r\n        # Create a YOLOv5 classification model from a YOLOv5 detection model\r\n        if isinstance(model, DetectMultiBackend):\r\n            model = model.model  # unwrap DetectMultiBackend\r\n        model.model = model.model[:cutoff]  # backbone\r\n        m = model.model[-1]  # last layer\r\n        ch = m.conv.in_channels if hasattr(m, 'conv') else m.cv1.conv.in_channels  # ch into module\r\n        c = Classify(ch, nc)  # Classify()\r\n        c.i, c.f, c.type = m.i, m.f, 'models.common.Classify'  # index, from, type\r\n        model.model[-1] = c  # replace\r\n        self.model = model.model\r\n        self.stride = model.stride\r\n        self.save = []\r\n        self.nc = nc\r\n\r\n    def _from_yaml(self, cfg):\r\n        # Create a YOLOv5 classification model from a *.yaml file\r\n        self.model = None\r\n\r\n\r\ndef parse_model(d, ch):  # model_dict, input_channels(3)\r\n    # Parse a YOLOv5 model.yaml dictionary\r\n    LOGGER.info(f\"\\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}\")\r\n    anchors, nc, gd, gw, act = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple'], d.get('activation')\r\n    if act:\r\n        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()\r\n        LOGGER.info(f\"{colorstr('activation:')} {act}\")  # print\r\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\r\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\r\n\r\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\r\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\r\n        m = eval(m) if isinstance(m, str) else m  # eval strings\r\n        for j, a in enumerate(args):\r\n            with contextlib.suppress(NameError):\r\n                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\r\n\r\n        n = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain\r\n        if m in {\r\n                Conv, C3EMA, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv,\r\n                BottleneckCSP, C3, C3TR, C3SPP, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, ScConv,gnconv, ScConv,\r\n                h_sigmoid, h_swish, SELayer, conv_bn_hswish, MobileNetV3, RepConv}:\r\n            c1, c2 = ch[f], args[0]\r\n            if c2 != no:  # if not output\r\n                c2 = make_divisible(c2 * gw, 8)\r\n\r\n            args = [c1, c2, *args[1:]]\r\n            if m in {BottleneckCSP, C3,C3SC, C3TR, C3Ghost, C3x, C3EMA, SEMAConv,SCC3EMA}:\r\n                args.insert(2, n)  # number of repeats\r\n                n = 1\r\n        elif m is nn.BatchNorm2d:\r\n            args = [ch[f]]\r\n        elif m is Concat:\r\n            c2 = sum(ch[x] for x in f)\r\n        elif m is CAFMFusion:\r\n            c2 = args[0]\r\n        elif m is SSFF:\r\n            c2 = args[0]\r\n        elif m is SPDConv:\r\n            args = [ch[f], ch[f]]\r\n        # TODO: channel, gw, gd\r\n        elif m in {Detect, Segment}:\r\n            args.append([ch[x] for x in f])\r\n            if isinstance(args[1], int):  # number of anchors\r\n                args[1] = [list(range(args[1] * 2))] * len(f)\r\n            if m is Segment:\r\n                args[3] = make_divisible(args[3] * gw, 8)\r\n        elif m is Contract:\r\n            c2 = ch[f] * args[0] ** 2\r\n        elif m is Expand:\r\n            c2 = ch[f] // args[0] ** 2\r\n        # \u6dfb\u52a0bifpn_concat\u7ed3\u6784\r\n        elif m is BiFPN_Concat2:\r\n            c2 = sum(ch[x] for x in f)\r\n        # \u6dfb\u52a0bifpn_concat\u7ed3\u6784\r\n        elif m is BiFPN_Concat3:\r\n            c2 = sum(ch[x] for x in f)\r\n        else:\r\n            c2 = ch[f]\r\n\r\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\r\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\r\n        np = sum(x.numel() for x in m_.parameters())  # number params\r\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\r\n        LOGGER.info(f'{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}')  # print\r\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\r\n        layers.append(m_)\r\n        if i == 0:\r\n            ch = []\r\n        ch.append(c2)\r\n    return nn.Sequential(*layers), sorted(save)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--cfg', type=str, default='yolov5-SimRepCSP.yaml', help='model.yaml')\r\n    parser.add_argument('--batch-size', type=int, default=1, help='total batch size for all GPUs')\r\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\r\n    parser.add_argument('--profile', action='store_true', help='profile model speed')\r\n    parser.add_argument('--line-profile', action='store_true', help='profile model speed layer by layer')\r\n    parser.add_argument('--test', action='store_true', help='test all yolo*.yaml')\r\n    opt = parser.parse_args()\r\n    opt.cfg = check_yaml(opt.cfg)  # check YAML\r\n    print_args(vars(opt))\r\n    device = select_device(opt.device)\r\n\r\n    # Create model\r\n    im = torch.rand(opt.batch_size, 3, 640, 640).to(device)\r\n    model = Model(opt.cfg).to(device)\r\n\r\n    # Options\r\n    if opt.line_profile:  # profile layer by layer\r\n        model(im, profile=True)\r\n\r\n    elif opt.profile:  # profile forward-backward\r\n        results = profile(input=im, ops=[model], n=3)\r\n\r\n    elif opt.test:  # test all models\r\n        for cfg in Path(ROOT / 'models').rglob('yolo*.yaml'):\r\n            try:\r\n                _ = Model(cfg)\r\n            except Exception as e:\r\n                print(f'Error in {cfg}: {e}')\r\n\r\n    else:  # report fused model summary\r\n        model.fuse()\r\n\n"})}),"\n",(0,i.jsx)(r.h4,{id:"yv7",children:"yolov7"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import argparse\r\nimport logging\r\nimport sys\r\nfrom copy import deepcopy\r\n\r\nsys.path.append('./')  # to run '$ python *.py' files in subdirectories\r\nlogger = logging.getLogger(__name__)\r\nimport torch\r\nfrom models.common import *\r\nfrom models.experimental import *\r\nfrom utils.autoanchor import check_anchor_order\r\nfrom utils.general import make_divisible, check_file, set_logging\r\nfrom utils.torch_utils import time_synchronized, fuse_conv_and_bn, model_info, scale_img, initialize_weights, \\\r\n    select_device, copy_attr\r\nfrom utils.loss import SigmoidBin\r\n\r\ntry:\r\n    import thop  # for FLOPS computation\r\nexcept ImportError:\r\n    thop = None\r\n\r\n\r\nclass Detect(nn.Module):\r\n    stride = None  # strides computed during build\r\n    export = False  # onnx export\r\n    end2end = False\r\n    include_nms = False\r\n    concat = False\r\n\r\n    def __init__(self, nc=80, anchors=(), ch=()):  # detection layer\r\n        super(Detect, self).__init__()\r\n        self.nc = nc  # number of classes\r\n        self.no = nc + 5  # number of outputs per anchor\r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\r\n        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\r\n        self.register_buffer('anchors', a)  # shape(nl,na,2)\r\n        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\r\n\r\n    def forward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](x[i])  # conv\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n                y = x[i].sigmoid()\r\n                if not torch.onnx.is_in_onnx_export():\r\n                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                else:\r\n                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0\r\n                    xy = xy * (2. * self.stride[i]) + (self.stride[i] * (self.grid[i] - 0.5))  # new xy\r\n                    wh = wh ** 2 * (4 * self.anchor_grid[i].data)  # new wh\r\n                    y = torch.cat((xy, wh, conf), 4)\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        if self.training:\r\n            out = x\r\n        elif self.end2end:\r\n            out = torch.cat(z, 1)\r\n        elif self.include_nms:\r\n            z = self.convert(z)\r\n            out = (z, )\r\n        elif self.concat:\r\n            out = torch.cat(z, 1)\r\n        else:\r\n            out = (torch.cat(z, 1), x)\r\n\r\n        return out\r\n\r\n    @staticmethod\r\n    def _make_grid(nx=20, ny=20):\r\n        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\r\n        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\r\n\r\n    def convert(self, z):\r\n        z = torch.cat(z, 1)\r\n        box = z[:, :, :4]\r\n        conf = z[:, :, 4:5]\r\n        score = z[:, :, 5:]\r\n        score *= conf\r\n        convert_matrix = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 1], [-0.5, 0, 0.5, 0], [0, -0.5, 0, 0.5]],\r\n                                           dtype=torch.float32,\r\n                                           device=z.device)\r\n        box @= convert_matrix                          \r\n        return (box, score)\r\n\r\n\r\nclass IDetect(nn.Module):\r\n    stride = None  # strides computed during build\r\n    export = False  # onnx export\r\n    end2end = False\r\n    include_nms = False\r\n    concat = False\r\n\r\n    def __init__(self, nc=80, anchors=(), ch=()):  # detection layer\r\n        super(IDetect, self).__init__()\r\n        self.nc = nc  # number of classes\r\n        self.no = nc + 5  # number of outputs per anchor\r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\r\n        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\r\n        self.register_buffer('anchors', a)  # shape(nl,na,2)\r\n        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\r\n        \r\n        self.ia = nn.ModuleList(ImplicitA(x) for x in ch)\r\n        self.im = nn.ModuleList(ImplicitM(self.no * self.na) for _ in ch)\r\n\r\n    def forward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](self.ia[i](x[i]))  # conv\r\n            x[i] = self.im[i](x[i])\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n\r\n                y = x[i].sigmoid()\r\n                y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        return x if self.training else (torch.cat(z, 1), x)\r\n    \r\n    def fuseforward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](x[i])  # conv\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n\r\n                y = x[i].sigmoid()\r\n                if not torch.onnx.is_in_onnx_export():\r\n                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                else:\r\n                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0\r\n                    xy = xy * (2. * self.stride[i]) + (self.stride[i] * (self.grid[i] - 0.5))  # new xy\r\n                    wh = wh ** 2 * (4 * self.anchor_grid[i].data)  # new wh\r\n                    y = torch.cat((xy, wh, conf), 4)\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        if self.training:\r\n            out = x\r\n        elif self.end2end:\r\n            out = torch.cat(z, 1)\r\n        elif self.include_nms:\r\n            z = self.convert(z)\r\n            out = (z, )\r\n        elif self.concat:\r\n            out = torch.cat(z, 1)            \r\n        else:\r\n            out = (torch.cat(z, 1), x)\r\n\r\n        return out\r\n    \r\n    def fuse(self):\r\n        print(\"IDetect.fuse\")\r\n        # fuse ImplicitA and Convolution\r\n        for i in range(len(self.m)):\r\n            c1,c2,_,_ = self.m[i].weight.shape\r\n            c1_,c2_, _,_ = self.ia[i].implicit.shape\r\n            self.m[i].bias += torch.matmul(self.m[i].weight.reshape(c1,c2),self.ia[i].implicit.reshape(c2_,c1_)).squeeze(1)\r\n\r\n        # fuse ImplicitM and Convolution\r\n        for i in range(len(self.m)):\r\n            c1,c2, _,_ = self.im[i].implicit.shape\r\n            self.m[i].bias *= self.im[i].implicit.reshape(c2)\r\n            self.m[i].weight *= self.im[i].implicit.transpose(0,1)\r\n            \r\n    @staticmethod\r\n    def _make_grid(nx=20, ny=20):\r\n        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\r\n        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\r\n\r\n    def convert(self, z):\r\n        z = torch.cat(z, 1)\r\n        box = z[:, :, :4]\r\n        conf = z[:, :, 4:5]\r\n        score = z[:, :, 5:]\r\n        score *= conf\r\n        convert_matrix = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 1], [-0.5, 0, 0.5, 0], [0, -0.5, 0, 0.5]],\r\n                                           dtype=torch.float32,\r\n                                           device=z.device)\r\n        box @= convert_matrix                          \r\n        return (box, score)\r\n\r\n\r\nclass IKeypoint(nn.Module):\r\n    stride = None  # strides computed during build\r\n    export = False  # onnx export\r\n\r\n    def __init__(self, nc=80, anchors=(), nkpt=17, ch=(), inplace=True, dw_conv_kpt=False):  # detection layer\r\n        super(IKeypoint, self).__init__()\r\n        self.nc = nc  # number of classes\r\n        self.nkpt = nkpt\r\n        self.dw_conv_kpt = dw_conv_kpt\r\n        self.no_det=(nc + 5)  # number of outputs per anchor for box and class\r\n        self.no_kpt = 3*self.nkpt ## number of outputs per anchor for keypoints\r\n        self.no = self.no_det+self.no_kpt\r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\r\n        self.flip_test = False\r\n        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\r\n        self.register_buffer('anchors', a)  # shape(nl,na,2)\r\n        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no_det * self.na, 1) for x in ch)  # output conv\r\n        \r\n        self.ia = nn.ModuleList(ImplicitA(x) for x in ch)\r\n        self.im = nn.ModuleList(ImplicitM(self.no_det * self.na) for _ in ch)\r\n        \r\n        if self.nkpt is not None:\r\n            if self.dw_conv_kpt: #keypoint head is slightly more complex\r\n                self.m_kpt = nn.ModuleList(\r\n                            nn.Sequential(DWConv(x, x, k=3), Conv(x,x),\r\n                                          DWConv(x, x, k=3), Conv(x, x),\r\n                                          DWConv(x, x, k=3), Conv(x,x),\r\n                                          DWConv(x, x, k=3), Conv(x, x),\r\n                                          DWConv(x, x, k=3), Conv(x, x),\r\n                                          DWConv(x, x, k=3), nn.Conv2d(x, self.no_kpt * self.na, 1)) for x in ch)\r\n            else: #keypoint head is a single convolution\r\n                self.m_kpt = nn.ModuleList(nn.Conv2d(x, self.no_kpt * self.na, 1) for x in ch)\r\n\r\n        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\r\n\r\n    def forward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            if self.nkpt is None or self.nkpt==0:\r\n                x[i] = self.im[i](self.m[i](self.ia[i](x[i])))  # conv\r\n            else :\r\n                x[i] = torch.cat((self.im[i](self.m[i](self.ia[i](x[i]))), self.m_kpt[i](x[i])), axis=1)\r\n\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n            x_det = x[i][..., :6]\r\n            x_kpt = x[i][..., 6:]\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n                kpt_grid_x = self.grid[i][..., 0:1]\r\n                kpt_grid_y = self.grid[i][..., 1:2]\r\n\r\n                if self.nkpt == 0:\r\n                    y = x[i].sigmoid()\r\n                else:\r\n                    y = x_det.sigmoid()\r\n\r\n                if self.inplace:\r\n                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2) # wh\r\n                    if self.nkpt != 0:\r\n                        x_kpt[..., 0::3] = (x_kpt[..., ::3] * 2. - 0.5 + kpt_grid_x.repeat(1,1,1,1,17)) * self.stride[i]  # xy\r\n                        x_kpt[..., 1::3] = (x_kpt[..., 1::3] * 2. - 0.5 + kpt_grid_y.repeat(1,1,1,1,17)) * self.stride[i]  # xy\r\n                        #x_kpt[..., 0::3] = (x_kpt[..., ::3] + kpt_grid_x.repeat(1,1,1,1,17)) * self.stride[i]  # xy\r\n                        #x_kpt[..., 1::3] = (x_kpt[..., 1::3] + kpt_grid_y.repeat(1,1,1,1,17)) * self.stride[i]  # xy\r\n                        #print('=============')\r\n                        #print(self.anchor_grid[i].shape)\r\n                        #print(self.anchor_grid[i][...,0].unsqueeze(4).shape)\r\n                        #print(x_kpt[..., 0::3].shape)\r\n                        #x_kpt[..., 0::3] = ((x_kpt[..., 0::3].tanh() * 2.) ** 3 * self.anchor_grid[i][...,0].unsqueeze(4).repeat(1,1,1,1,self.nkpt)) + kpt_grid_x.repeat(1,1,1,1,17) * self.stride[i]  # xy\r\n                        #x_kpt[..., 1::3] = ((x_kpt[..., 1::3].tanh() * 2.) ** 3 * self.anchor_grid[i][...,1].unsqueeze(4).repeat(1,1,1,1,self.nkpt)) + kpt_grid_y.repeat(1,1,1,1,17) * self.stride[i]  # xy\r\n                        #x_kpt[..., 0::3] = (((x_kpt[..., 0::3].sigmoid() * 4.) ** 2 - 8.) * self.anchor_grid[i][...,0].unsqueeze(4).repeat(1,1,1,1,self.nkpt)) + kpt_grid_x.repeat(1,1,1,1,17) * self.stride[i]  # xy\r\n                        #x_kpt[..., 1::3] = (((x_kpt[..., 1::3].sigmoid() * 4.) ** 2 - 8.) * self.anchor_grid[i][...,1].unsqueeze(4).repeat(1,1,1,1,self.nkpt)) + kpt_grid_y.repeat(1,1,1,1,17) * self.stride[i]  # xy\r\n                        x_kpt[..., 2::3] = x_kpt[..., 2::3].sigmoid()\r\n\r\n                    y = torch.cat((xy, wh, y[..., 4:], x_kpt), dim = -1)\r\n\r\n                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\r\n                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                    if self.nkpt != 0:\r\n                        y[..., 6:] = (y[..., 6:] * 2. - 0.5 + self.grid[i].repeat((1,1,1,1,self.nkpt))) * self.stride[i]  # xy\r\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\r\n\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        return x if self.training else (torch.cat(z, 1), x)\r\n\r\n    @staticmethod\r\n    def _make_grid(nx=20, ny=20):\r\n        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\r\n        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\r\n\r\n\r\nclass IAuxDetect(nn.Module):\r\n    stride = None  # strides computed during build\r\n    export = False  # onnx export\r\n    end2end = False\r\n    include_nms = False\r\n    concat = False\r\n\r\n    def __init__(self, nc=80, anchors=(), ch=()):  # detection layer\r\n        super(IAuxDetect, self).__init__()\r\n        self.nc = nc  # number of classes\r\n        self.no = nc + 5  # number of outputs per anchor\r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\r\n        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\r\n        self.register_buffer('anchors', a)  # shape(nl,na,2)\r\n        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch[:self.nl])  # output conv\r\n        self.m2 = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch[self.nl:])  # output conv\r\n        \r\n        self.ia = nn.ModuleList(ImplicitA(x) for x in ch[:self.nl])\r\n        self.im = nn.ModuleList(ImplicitM(self.no * self.na) for _ in ch[:self.nl])\r\n\r\n    def forward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](self.ia[i](x[i]))  # conv\r\n            x[i] = self.im[i](x[i])\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n            \r\n            x[i+self.nl] = self.m2[i](x[i+self.nl])\r\n            x[i+self.nl] = x[i+self.nl].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n\r\n                y = x[i].sigmoid()\r\n                if not torch.onnx.is_in_onnx_export():\r\n                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                else:\r\n                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0\r\n                    xy = xy * (2. * self.stride[i]) + (self.stride[i] * (self.grid[i] - 0.5))  # new xy\r\n                    wh = wh ** 2 * (4 * self.anchor_grid[i].data)  # new wh\r\n                    y = torch.cat((xy, wh, conf), 4)\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        return x if self.training else (torch.cat(z, 1), x[:self.nl])\r\n\r\n    def fuseforward(self, x):\r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](x[i])  # conv\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n\r\n                y = x[i].sigmoid()\r\n                if not torch.onnx.is_in_onnx_export():\r\n                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                else:\r\n                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].data  # wh\r\n                    y = torch.cat((xy, wh, y[..., 4:]), -1)\r\n                z.append(y.view(bs, -1, self.no))\r\n\r\n        if self.training:\r\n            out = x\r\n        elif self.end2end:\r\n            out = torch.cat(z, 1)\r\n        elif self.include_nms:\r\n            z = self.convert(z)\r\n            out = (z, )\r\n        elif self.concat:\r\n            out = torch.cat(z, 1)            \r\n        else:\r\n            out = (torch.cat(z, 1), x)\r\n\r\n        return out\r\n    \r\n    def fuse(self):\r\n        print(\"IAuxDetect.fuse\")\r\n        # fuse ImplicitA and Convolution\r\n        for i in range(len(self.m)):\r\n            c1,c2,_,_ = self.m[i].weight.shape\r\n            c1_,c2_, _,_ = self.ia[i].implicit.shape\r\n            self.m[i].bias += torch.matmul(self.m[i].weight.reshape(c1,c2),self.ia[i].implicit.reshape(c2_,c1_)).squeeze(1)\r\n\r\n        # fuse ImplicitM and Convolution\r\n        for i in range(len(self.m)):\r\n            c1,c2, _,_ = self.im[i].implicit.shape\r\n            self.m[i].bias *= self.im[i].implicit.reshape(c2)\r\n            self.m[i].weight *= self.im[i].implicit.transpose(0,1)\r\n\r\n    @staticmethod\r\n    def _make_grid(nx=20, ny=20):\r\n        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\r\n        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\r\n\r\n    def convert(self, z):\r\n        z = torch.cat(z, 1)\r\n        box = z[:, :, :4]\r\n        conf = z[:, :, 4:5]\r\n        score = z[:, :, 5:]\r\n        score *= conf\r\n        convert_matrix = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 1], [-0.5, 0, 0.5, 0], [0, -0.5, 0, 0.5]],\r\n                                           dtype=torch.float32,\r\n                                           device=z.device)\r\n        box @= convert_matrix                          \r\n        return (box, score)\r\n\r\n\r\nclass IBin(nn.Module):\r\n    stride = None  # strides computed during build\r\n    export = False  # onnx export\r\n\r\n    def __init__(self, nc=80, anchors=(), ch=(), bin_count=21):  # detection layer\r\n        super(IBin, self).__init__()\r\n        self.nc = nc  # number of classes\r\n        self.bin_count = bin_count\r\n\r\n        self.w_bin_sigmoid = SigmoidBin(bin_count=self.bin_count, min=0.0, max=4.0)\r\n        self.h_bin_sigmoid = SigmoidBin(bin_count=self.bin_count, min=0.0, max=4.0)\r\n        # classes, x,y,obj\r\n        self.no = nc + 3 + \\\r\n            self.w_bin_sigmoid.get_length() + self.h_bin_sigmoid.get_length()   # w-bce, h-bce\r\n            # + self.x_bin_sigmoid.get_length() + self.y_bin_sigmoid.get_length()\r\n        \r\n        self.nl = len(anchors)  # number of detection layers\r\n        self.na = len(anchors[0]) // 2  # number of anchors\r\n        self.grid = [torch.zeros(1)] * self.nl  # init grid\r\n        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\r\n        self.register_buffer('anchors', a)  # shape(nl,na,2)\r\n        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\r\n        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\r\n        \r\n        self.ia = nn.ModuleList(ImplicitA(x) for x in ch)\r\n        self.im = nn.ModuleList(ImplicitM(self.no * self.na) for _ in ch)\r\n\r\n    def forward(self, x):\r\n\r\n        #self.x_bin_sigmoid.use_fw_regression = True\r\n        #self.y_bin_sigmoid.use_fw_regression = True\r\n        self.w_bin_sigmoid.use_fw_regression = True\r\n        self.h_bin_sigmoid.use_fw_regression = True\r\n        \r\n        # x = x.copy()  # for profiling\r\n        z = []  # inference output\r\n        self.training |= self.export\r\n        for i in range(self.nl):\r\n            x[i] = self.m[i](self.ia[i](x[i]))  # conv\r\n            x[i] = self.im[i](x[i])\r\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\r\n            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\r\n\r\n            if not self.training:  # inference\r\n                if self.grid[i].shape[2:4] != x[i].shape[2:4]:\r\n                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\r\n\r\n                y = x[i].sigmoid()\r\n                y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\r\n                #y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\r\n                \r\n\r\n                #px = (self.x_bin_sigmoid.forward(y[..., 0:12]) + self.grid[i][..., 0]) * self.stride[i]\r\n                #py = (self.y_bin_sigmoid.forward(y[..., 12:24]) + self.grid[i][..., 1]) * self.stride[i]\r\n\r\n                pw = self.w_bin_sigmoid.forward(y[..., 2:24]) * self.anchor_grid[i][..., 0]\r\n                ph = self.h_bin_sigmoid.forward(y[..., 24:46]) * self.anchor_grid[i][..., 1]\r\n\r\n                #y[..., 0] = px\r\n                #y[..., 1] = py\r\n                y[..., 2] = pw\r\n                y[..., 3] = ph\r\n                \r\n                y = torch.cat((y[..., 0:4], y[..., 46:]), dim=-1)\r\n                \r\n                z.append(y.view(bs, -1, y.shape[-1]))\r\n\r\n        return x if self.training else (torch.cat(z, 1), x)\r\n\r\n    @staticmethod\r\n    def _make_grid(nx=20, ny=20):\r\n        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\r\n        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, cfg='yolor-csp-c.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\r\n        super(Model, self).__init__()\r\n        self.traced = False\r\n        if isinstance(cfg, dict):\r\n            self.yaml = cfg  # model dict\r\n        else:  # is *.yaml\r\n            import yaml  # for torch hub\r\n            self.yaml_file = Path(cfg).name\r\n            with open(cfg) as f:\r\n                self.yaml = yaml.load(f, Loader=yaml.SafeLoader)  # model dict\r\n\r\n        # Define model\r\n        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\r\n        if nc and nc != self.yaml['nc']:\r\n            logger.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\r\n            self.yaml['nc'] = nc  # override yaml value\r\n        if anchors:\r\n            logger.info(f'Overriding model.yaml anchors with anchors={anchors}')\r\n            self.yaml['anchors'] = round(anchors)  # override yaml value\r\n        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\r\n        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\r\n        # print([x.shape for x in self.forward(torch.zeros(1, ch, 64, 64))])\r\n\r\n        # Build strides, anchors\r\n        m = self.model[-1]  # Detect()\r\n        if isinstance(m, Detect):\r\n            s = 256  # 2x min stride\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases()  # only run once\r\n            # print('Strides: %s' % m.stride.tolist())\r\n        if isinstance(m, IDetect):\r\n            s = 256  # 2x min stride\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases()  # only run once\r\n            # print('Strides: %s' % m.stride.tolist())\r\n        if isinstance(m, IAuxDetect):\r\n            s = 256  # 2x min stride\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))[:4]])  # forward\r\n            #print(m.stride)\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_aux_biases()  # only run once\r\n            # print('Strides: %s' % m.stride.tolist())\r\n        if isinstance(m, IBin):\r\n            s = 256  # 2x min stride\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases_bin()  # only run once\r\n            # print('Strides: %s' % m.stride.tolist())\r\n        if isinstance(m, IKeypoint):\r\n            s = 256  # 2x min stride\r\n            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\r\n            check_anchor_order(m)\r\n            m.anchors /= m.stride.view(-1, 1, 1)\r\n            self.stride = m.stride\r\n            self._initialize_biases_kpt()  # only run once\r\n            # print('Strides: %s' % m.stride.tolist())\r\n\r\n        # Init weights, biases\r\n        initialize_weights(self)\r\n        self.info()\r\n        logger.info('')\r\n\r\n    def forward(self, x, augment=False, profile=False):\r\n        if augment:\r\n            img_size = x.shape[-2:]  # height, width\r\n            s = [1, 0.83, 0.67]  # scales\r\n            f = [None, 3, None]  # flips (2-ud, 3-lr)\r\n            y = []  # outputs\r\n            for si, fi in zip(s, f):\r\n                xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\r\n                yi = self.forward_once(xi)[0]  # forward\r\n                # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\r\n                yi[..., :4] /= si  # de-scale\r\n                if fi == 2:\r\n                    yi[..., 1] = img_size[0] - yi[..., 1]  # de-flip ud\r\n                elif fi == 3:\r\n                    yi[..., 0] = img_size[1] - yi[..., 0]  # de-flip lr\r\n                y.append(yi)\r\n            return torch.cat(y, 1), None  # augmented inference, train\r\n        else:\r\n            return self.forward_once(x, profile)  # single-scale inference, train\r\n\r\n    def forward_once(self, x, profile=False):\r\n        y, dt = [], []  # outputs\r\n        for m in self.model:\r\n            if m.f != -1:  # if not from previous layer\r\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\r\n\r\n            if not hasattr(self, 'traced'):\r\n                self.traced=False\r\n\r\n            if self.traced:\r\n                if isinstance(m, Detect) or isinstance(m, IDetect) or isinstance(m, IAuxDetect) or isinstance(m, IKeypoint):\r\n                    break\r\n\r\n            if profile:\r\n                c = isinstance(m, (Detect, IDetect, IAuxDetect, IBin))\r\n                o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPS\r\n                for _ in range(10):\r\n                    m(x.copy() if c else x)\r\n                t = time_synchronized()\r\n                for _ in range(10):\r\n                    m(x.copy() if c else x)\r\n                dt.append((time_synchronized() - t) * 100)\r\n                print('%10.1f%10.0f%10.1fms %-40s' % (o, m.np, dt[-1], m.type))\r\n\r\n            x = m(x)  # run\r\n            \r\n            y.append(x if m.i in self.save else None)  # save output\r\n\r\n        if profile:\r\n            print('%.1fms total' % sum(dt))\r\n        return x\r\n\r\n    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\r\n        # https://arxiv.org/abs/1708.02002 section 3.3\r\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\r\n        m = self.model[-1]  # Detect() module\r\n        for mi, s in zip(m.m, m.stride):  # from\r\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\r\n\r\n    def _initialize_aux_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\r\n        # https://arxiv.org/abs/1708.02002 section 3.3\r\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\r\n        m = self.model[-1]  # Detect() module\r\n        for mi, mi2, s in zip(m.m, m.m2, m.stride):  # from\r\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\r\n            b2 = mi2.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            b2.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b2.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            mi2.bias = torch.nn.Parameter(b2.view(-1), requires_grad=True)\r\n\r\n    def _initialize_biases_bin(self, cf=None):  # initialize biases into Detect(), cf is class frequency\r\n        # https://arxiv.org/abs/1708.02002 section 3.3\r\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\r\n        m = self.model[-1]  # Bin() module\r\n        bc = m.bin_count\r\n        for mi, s in zip(m.m, m.stride):  # from\r\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            old = b[:, (0,1,2,bc+3)].data\r\n            obj_idx = 2*bc+4\r\n            b[:, :obj_idx].data += math.log(0.6 / (bc + 1 - 0.99))\r\n            b[:, obj_idx].data += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b[:, (obj_idx+1):].data += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            b[:, (0,1,2,bc+3)].data = old\r\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\r\n\r\n    def _initialize_biases_kpt(self, cf=None):  # initialize biases into Detect(), cf is class frequency\r\n        # https://arxiv.org/abs/1708.02002 section 3.3\r\n        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\r\n        m = self.model[-1]  # Detect() module\r\n        for mi, s in zip(m.m, m.stride):  # from\r\n            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\r\n            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\r\n            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\r\n            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\r\n\r\n    def _print_biases(self):\r\n        m = self.model[-1]  # Detect() module\r\n        for mi in m.m:  # from\r\n            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)\r\n            print(('%6g Conv2d.bias:' + '%10.3g' * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))\r\n\r\n    # def _print_weights(self):\r\n    #     for m in self.model.modules():\r\n    #         if type(m) is Bottleneck:\r\n    #             print('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights\r\n\r\n    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\r\n        print('Fusing layers... ')\r\n        for m in self.model.modules():\r\n            if isinstance(m, RepConv):\r\n                #print(f\" fuse_repvgg_block\")\r\n                m.fuse_repvgg_block()\r\n            elif isinstance(m, RepConv_OREPA):\r\n                #print(f\" switch_to_deploy\")\r\n                m.switch_to_deploy()\r\n            elif type(m) is Conv and hasattr(m, 'bn'):\r\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\r\n                delattr(m, 'bn')  # remove batchnorm\r\n                m.forward = m.fuseforward  # update forward\r\n            elif isinstance(m, (IDetect, IAuxDetect)):\r\n                m.fuse()\r\n                m.forward = m.fuseforward\r\n        self.info()\r\n        return self\r\n\r\n    def nms(self, mode=True):  # add or remove NMS module\r\n        present = type(self.model[-1]) is NMS  # last layer is NMS\r\n        if mode and not present:\r\n            print('Adding NMS... ')\r\n            m = NMS()  # module\r\n            m.f = -1  # from\r\n            m.i = self.model[-1].i + 1  # index\r\n            self.model.add_module(name='%s' % m.i, module=m)  # add\r\n            self.eval()\r\n        elif not mode and present:\r\n            print('Removing NMS... ')\r\n            self.model = self.model[:-1]  # remove\r\n        return self\r\n\r\n    def autoshape(self):  # add autoShape module\r\n        print('Adding autoShape... ')\r\n        m = autoShape(self)  # wrap model\r\n        copy_attr(m, self, include=('yaml', 'nc', 'hyp', 'names', 'stride'), exclude=())  # copy attributes\r\n        return m\r\n\r\n    def info(self, verbose=False, img_size=640):  # print model information\r\n        model_info(self, verbose, img_size)\r\n\r\n\r\ndef parse_model(d, ch):  # model_dict, input_channels(3)\r\n    logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\r\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\r\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\r\n    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\r\n\r\n    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\r\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\r\n        m = eval(m) if isinstance(m, str) else m  # eval strings\r\n        for j, a in enumerate(args):\r\n            try:\r\n                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\r\n            except:\r\n                pass\r\n\r\n        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\r\n        if m in [nn.Conv2d, Conv, RobustConv, RobustConv2, DWConv, GhostConv, RepConv, RepConv_OREPA, DownC, \r\n                 SPP, SPPF, SPPCSPC, GhostSPPCSPC, MixConv2d, Focus, Stem, GhostStem, CrossConv, \r\n                 Bottleneck, BottleneckCSPA, BottleneckCSPB, BottleneckCSPC, \r\n                 RepBottleneck, RepBottleneckCSPA, RepBottleneckCSPB, RepBottleneckCSPC,  \r\n                 Res, ResCSPA, ResCSPB, ResCSPC, \r\n                 RepRes, RepResCSPA, RepResCSPB, RepResCSPC, \r\n                 ResX, ResXCSPA, ResXCSPB, ResXCSPC, \r\n                 RepResX, RepResXCSPA, RepResXCSPB, RepResXCSPC, \r\n                 Ghost, GhostCSPA, GhostCSPB, GhostCSPC,\r\n                 SwinTransformerBlock, STCSPA, STCSPB, STCSPC,\r\n                 SwinTransformer2Block, ST2CSPA, ST2CSPB, ST2CSPC]:\r\n            c1, c2 = ch[f], args[0]\r\n            if c2 != no:  # if not output\r\n                c2 = make_divisible(c2 * gw, 8)\r\n\r\n            args = [c1, c2, *args[1:]]\r\n            if m in [DownC, SPPCSPC, GhostSPPCSPC, \r\n                     BottleneckCSPA, BottleneckCSPB, BottleneckCSPC, \r\n                     RepBottleneckCSPA, RepBottleneckCSPB, RepBottleneckCSPC, \r\n                     ResCSPA, ResCSPB, ResCSPC, \r\n                     RepResCSPA, RepResCSPB, RepResCSPC, \r\n                     ResXCSPA, ResXCSPB, ResXCSPC, \r\n                     RepResXCSPA, RepResXCSPB, RepResXCSPC,\r\n                     GhostCSPA, GhostCSPB, GhostCSPC,\r\n                     STCSPA, STCSPB, STCSPC,\r\n                     ST2CSPA, ST2CSPB, ST2CSPC]:\r\n                args.insert(2, n)  # number of repeats\r\n                n = 1\r\n        elif m is nn.BatchNorm2d:\r\n            args = [ch[f]]\r\n        elif m is Concat:\r\n            c2 = sum([ch[x] for x in f])\r\n        elif m is Chuncat:\r\n            c2 = sum([ch[x] for x in f])\r\n        elif m is Shortcut:\r\n            c2 = ch[f[0]]\r\n        elif m is Foldcut:\r\n            c2 = ch[f] // 2\r\n        elif m in [Detect, IDetect, IAuxDetect, IBin, IKeypoint]:\r\n            args.append([ch[x] for x in f])\r\n            if isinstance(args[1], int):  # number of anchors\r\n                args[1] = [list(range(args[1] * 2))] * len(f)\r\n        elif m is ReOrg:\r\n            c2 = ch[f] * 4\r\n        elif m is Contract:\r\n            c2 = ch[f] * args[0] ** 2\r\n        elif m is Expand:\r\n            c2 = ch[f] // args[0] ** 2\r\n        else:\r\n            c2 = ch[f]\r\n\r\n        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\r\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\r\n        np = sum([x.numel() for x in m_.parameters()])  # number params\r\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\r\n        logger.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\r\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\r\n        layers.append(m_)\r\n        if i == 0:\r\n            ch = []\r\n        ch.append(c2)\r\n    return nn.Sequential(*layers), sorted(save)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--cfg', type=str, default='yolor-csp-c.yaml', help='model.yaml')\r\n    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\r\n    parser.add_argument('--profile', action='store_true', help='profile model speed')\r\n    opt = parser.parse_args()\r\n    opt.cfg = check_file(opt.cfg)  # check file\r\n    set_logging()\r\n    device = select_device(opt.device)\r\n\r\n    # Create model\r\n    model = Model(opt.cfg).to(device)\r\n    model.train()\r\n    \r\n    if opt.profile:\r\n        img = torch.rand(1, 3, 640, 640).to(device)\r\n        y = model(img, profile=True)\r\n\r\n    # Profile\r\n    # img = torch.rand(8 if torch.cuda.is_available() else 1, 3, 640, 640).to(device)\r\n    # y = model(img, profile=True)\r\n\r\n    # Tensorboard\r\n    # from torch.utils.tensorboard import SummaryWriter\r\n    # tb_writer = SummaryWriter()\r\n    # print(\"Run 'tensorboard --logdir=models/runs' to view tensorboard at http://localhost:6006/\")\r\n    # tb_writer.add_graph(model.model, img)  # add model to tensorboard\r\n    # tb_writer.add_image('test', img[0], dataformats='CWH')  # add model to tensorboard\r\n\n"})}),"\n",(0,i.jsx)(r.h4,{id:"d2",children:"detectron2"}),"\n",(0,i.jsx)(r.p,{children:"\u5185\u5bb9\u8fc7\u591a\uff0c\u770b\u6e90\u4ee3\u7801"}),"\n",(0,i.jsx)(r.h4,{id:"fmrcm",children:"FastMaskRCNN-master"}),"\n",(0,i.jsx)(r.p,{children:"\u6570\u636e\u4e0b\u8f7d\u4e0e\u8f6c\u6362"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport tensorflow as tf\r\n\r\nfrom libs.datasets import download_and_convert_coco\r\nfrom libs.configs import config_v1\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# tf.app.flags.DEFINE_string(\r\n#     'dataset_name', 'coco',\r\n#     'The name of the dataset to convert, one of \"coco\", \"cifar10\", \"flowers\", \"mnist\".')\r\n\r\n# tf.app.flags.DEFINE_string(\r\n#     'dataset_dir', 'data/coco',\r\n#     'The directory where the output TFRecords and temporary files are saved.')\r\n\r\n\r\ndef main(_):\r\n  if not os.path.isdir('./output/mask_rcnn'):\r\n    os.makedirs('./output/mask_rcnn')\r\n  if not FLAGS.dataset_name:\r\n    raise ValueError('You must supply the dataset name with --dataset_name')\r\n  if not FLAGS.dataset_dir:\r\n    raise ValueError('You must supply the dataset directory with --dataset_dir')\r\n\r\n  elif FLAGS.dataset_name == 'coco':\r\n    download_and_convert_coco.run(FLAGS.dataset_dir, FLAGS.dataset_split_name)\r\n  else:\r\n    raise ValueError(\r\n        'dataset_name [%s] was not recognized.' % FLAGS.dataset_dir)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\n"})}),"\n",(0,i.jsx)(r.p,{children:"\u6a21\u578b\u8bad\u7ec3"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"#!/usr/bin/env python\r\n# coding=utf-8\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n \r\nimport functools\r\nimport os, sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nfrom time import gmtime, strftime\r\n\r\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\r\nimport libs.configs.config_v1 as cfg\r\nimport libs.datasets.dataset_factory as datasets\r\nimport libs.nets.nets_factory as network \r\n\r\nimport libs.preprocessings.coco_v1 as coco_preprocess\r\nimport libs.nets.pyramid_network as pyramid_network\r\nimport libs.nets.resnet_v1 as resnet_v1\r\n\r\nfrom train.train_utils import _configure_learning_rate, _configure_optimizer, \\\r\n  _get_variables_to_train, _get_init_fn, get_var_list_to_restore\r\n\r\nfrom PIL import Image, ImageFont, ImageDraw, ImageEnhance\r\nfrom libs.datasets import download_and_convert_coco\r\n#from libs.datasets.download_and_convert_coco import _cat_id_to_cls_name\r\nfrom libs.visualization.pil_utils import cat_id_to_cls_name, draw_img, draw_bbox\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\nresnet50 = resnet_v1.resnet_v1_50\r\n\r\ndef solve(global_step):\r\n    \"\"\"add solver to losses\"\"\"\r\n    # learning reate\r\n    lr = _configure_learning_rate(82783, global_step)\r\n    optimizer = _configure_optimizer(lr)\r\n    tf.summary.scalar('learning_rate', lr)\r\n\r\n    # compute and apply gradient\r\n    losses = tf.get_collection(tf.GraphKeys.LOSSES)\r\n    regular_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n    regular_loss = tf.add_n(regular_losses)\r\n    out_loss = tf.add_n(losses)\r\n    total_loss = tf.add_n(losses + regular_losses)\r\n\r\n    tf.summary.scalar('total_loss', total_loss)\r\n    tf.summary.scalar('out_loss', out_loss)\r\n    tf.summary.scalar('regular_loss', regular_loss)\r\n\r\n    update_ops = []\r\n    variables_to_train = _get_variables_to_train()\r\n    # update_op = optimizer.minimize(total_loss)\r\n    gradients = optimizer.compute_gradients(total_loss, var_list=variables_to_train)\r\n    grad_updates = optimizer.apply_gradients(gradients, \r\n            global_step=global_step)\r\n    update_ops.append(grad_updates)\r\n    \r\n    # update moving mean and variance\r\n    if FLAGS.update_bn:\r\n        update_bns = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        update_bn = tf.group(*update_bns)\r\n        update_ops.append(update_bn)\r\n\r\n    return tf.group(*update_ops)\r\n\r\ndef restore(sess):\r\n     \"\"\"choose which param to restore\"\"\"\r\n     if FLAGS.restore_previous_if_exists:\r\n        try:\r\n            checkpoint_path = tf.train.latest_checkpoint(FLAGS.train_dir)\r\n\r\n            restorer = tf.train.Saver()\r\n\r\n            restorer.restore(sess, checkpoint_path)\r\n            print ('restored previous model %s from %s'\\\r\n                    %(checkpoint_path, FLAGS.train_dir))\r\n            time.sleep(2)\r\n            return\r\n        except:\r\n            print ('--restore_previous_if_exists is set, but failed to restore in %s %s'\\\r\n                    % (FLAGS.train_dir, checkpoint_path))\r\n            time.sleep(2)\r\n\r\n     if FLAGS.pretrained_model:\r\n        if tf.gfile.IsDirectory(FLAGS.pretrained_model):\r\n            checkpoint_path = tf.train.latest_checkpoint(FLAGS.pretrained_model)\r\n        else:\r\n            checkpoint_path = FLAGS.pretrained_model\r\n\r\n        if FLAGS.checkpoint_exclude_scopes is None:\r\n            FLAGS.checkpoint_exclude_scopes='pyramid'\r\n        if FLAGS.checkpoint_include_scopes is None:\r\n            FLAGS.checkpoint_include_scopes='resnet_v1_50'\r\n\r\n        vars_to_restore = get_var_list_to_restore()\r\n        for var in vars_to_restore:\r\n            print ('restoring ', var.name)\r\n      \r\n        try:\r\n           restorer = tf.train.Saver(vars_to_restore)\r\n           restorer.restore(sess, checkpoint_path)\r\n           print ('Restored %d(%d) vars from %s' %(\r\n               len(vars_to_restore), len(tf.global_variables()),\r\n               checkpoint_path ))\r\n        except:\r\n           print ('Checking your params %s' %(checkpoint_path))\r\n           raise\r\n    \r\ndef train():\r\n    \"\"\"The main function that runs training\"\"\"\r\n\r\n    ## data\r\n    image, ih, iw, gt_boxes, gt_masks, num_instances, img_id = \\\r\n        datasets.get_dataset(FLAGS.dataset_name, \r\n                             FLAGS.dataset_split_name, \r\n                             FLAGS.dataset_dir, \r\n                             FLAGS.im_batch,\r\n                             is_training=True)\r\n\r\n    data_queue = tf.RandomShuffleQueue(capacity=32, min_after_dequeue=16,\r\n            dtypes=(\r\n                image.dtype, ih.dtype, iw.dtype, \r\n                gt_boxes.dtype, gt_masks.dtype, \r\n                num_instances.dtype, img_id.dtype)) \r\n    enqueue_op = data_queue.enqueue((image, ih, iw, gt_boxes, gt_masks, num_instances, img_id))\r\n    data_queue_runner = tf.train.QueueRunner(data_queue, [enqueue_op] * 4)\r\n    tf.add_to_collection(tf.GraphKeys.QUEUE_RUNNERS, data_queue_runner)\r\n    (image, ih, iw, gt_boxes, gt_masks, num_instances, img_id) =  data_queue.dequeue()\r\n    im_shape = tf.shape(image)\r\n    image = tf.reshape(image, (im_shape[0], im_shape[1], im_shape[2], 3))\r\n\r\n    ## network\r\n    logits, end_points, pyramid_map = network.get_network(FLAGS.network, image,\r\n            weight_decay=FLAGS.weight_decay, is_training=True)\r\n    outputs = pyramid_network.build(end_points, im_shape[1], im_shape[2], pyramid_map,\r\n            num_classes=81,\r\n            base_anchors=9,\r\n            is_training=True,\r\n            gt_boxes=gt_boxes, gt_masks=gt_masks,\r\n            loss_weights=[0.2, 0.2, 1.0, 0.2, 1.0])\r\n\r\n\r\n    total_loss = outputs['total_loss']\r\n    losses  = outputs['losses']\r\n    batch_info = outputs['batch_info']\r\n    regular_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n    \r\n    input_image = end_points['input']\r\n    final_box = outputs['final_boxes']['box']\r\n    final_cls = outputs['final_boxes']['cls']\r\n    final_prob = outputs['final_boxes']['prob']\r\n    final_gt_cls = outputs['final_boxes']['gt_cls']\r\n    gt = outputs['gt']\r\n\r\n    #############################\r\n    tmp_0 = outputs['losses']\r\n    tmp_1 = outputs['losses']\r\n    tmp_2 = outputs['losses']\r\n    tmp_3 = outputs['losses']\r\n    tmp_4 = outputs['losses']\r\n\r\n    # tmp_0 = outputs['tmp_0']\r\n    # tmp_1 = outputs['tmp_1']\r\n    # tmp_2 = outputs['tmp_2']\r\n    tmp_3 = outputs['tmp_3']\r\n    tmp_4 = outputs['tmp_4']\r\n    ############################\r\n\r\n\r\n    ## solvers\r\n    global_step = slim.create_global_step()\r\n    update_op = solve(global_step)\r\n\r\n    cropped_rois = tf.get_collection('__CROPPED__')[0]\r\n    transposed = tf.get_collection('__TRANSPOSED__')[0]\r\n    \r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\r\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n    init_op = tf.group(\r\n            tf.global_variables_initializer(),\r\n            tf.local_variables_initializer()\r\n            )\r\n    sess.run(init_op)\r\n\r\n    summary_op = tf.summary.merge_all()\r\n    logdir = os.path.join(FLAGS.train_dir, strftime('%Y%m%d%H%M%S', gmtime()))\r\n    if not os.path.exists(logdir):\r\n        os.makedirs(logdir)\r\n    summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)\r\n\r\n    ## restore\r\n    restore(sess)\r\n\r\n    ## main loop\r\n    coord = tf.train.Coordinator()\r\n    threads = []\r\n    # print (tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS))\r\n    for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\r\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\r\n                                         start=True))\r\n\r\n    tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    saver = tf.train.Saver(max_to_keep=20)\r\n\r\n    for step in range(FLAGS.max_iters):\r\n        \r\n        start_time = time.time()\r\n\r\n        s_, tot_loss, reg_lossnp, img_id_str, \\\r\n        rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss, \\\r\n        gt_boxesnp, \\\r\n        rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch, \\\r\n        input_imagenp, final_boxnp, final_clsnp, final_probnp, final_gt_clsnp, gtnp, tmp_0np, tmp_1np, tmp_2np, tmp_3np, tmp_4np= \\\r\n                     sess.run([update_op, total_loss, regular_loss, img_id] + \r\n                              losses + \r\n                              [gt_boxes] + \r\n                              batch_info + \r\n                              [input_image] + [final_box] + [final_cls] + [final_prob] + [final_gt_cls] + [gt] + [tmp_0] + [tmp_1] + [tmp_2] + [tmp_3] + [tmp_4])\r\n\r\n        duration_time = time.time() - start_time\r\n        if step % 1 == 0: \r\n            print ( \"\"\"iter %d: image-id:%07d, time:%.3f(sec), regular_loss: %.6f, \"\"\"\r\n                    \"\"\"total-loss %.4f(%.4f, %.4f, %.6f, %.4f, %.4f), \"\"\"\r\n                    \"\"\"instances: %d, \"\"\"\r\n                    \"\"\"batch:(%d|%d, %d|%d, %d|%d)\"\"\" \r\n                   % (step, img_id_str, duration_time, reg_lossnp, \r\n                      tot_loss, rpn_box_loss, rpn_cls_loss, refined_box_loss, refined_cls_loss, mask_loss,\r\n                      gt_boxesnp.shape[0], \r\n                      rpn_batch_pos, rpn_batch, refine_batch_pos, refine_batch, mask_batch_pos, mask_batch))\r\n\r\n            # draw_bbox(step, \r\n            #           np.uint8((np.array(input_imagenp[0])/2.0+0.5)*255.0), \r\n            #           name='est', \r\n            #           bbox=final_boxnp, \r\n            #           label=final_clsnp, \r\n            #           prob=final_probnp,\r\n            #           gt_label=np.argmax(np.asarray(final_gt_clsnp),axis=1),\r\n            #           )\r\n\r\n            # draw_bbox(step, \r\n            #           np.uint8((np.array(input_imagenp[0])/2.0+0.5)*255.0), \r\n            #           name='gt', \r\n            #           bbox=gtnp[:,0:4], \r\n            #           label=np.asarray(gtnp[:,4], dtype=np.uint8),\r\n            #           )\r\n            \r\n            print (\"labels\")\r\n            # print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(final_gt_clsnp),axis=1)))[1:])\r\n            # print (cat_id_to_cls_name(np.unique(np.asarray(gt_boxesnp, dtype=np.uint8)[:,4])))\r\n            print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(tmp_3np),axis=1)))[1:])\r\n            #print (cat_id_to_cls_name(np.unique(np.argmax(np.asarray(gt_boxesnp)[:,4],axis=1))))\r\n            print (\"classes\")\r\n            print (cat_id_to_cls_name(np.unique(np.argmax(np.array(tmp_4np),axis=1))))\r\n            # print (np.asanyarray(tmp_3np))\r\n\r\n            #print (\"ordered rois\")\r\n            #print (np.asarray(tmp_0np)[0])\r\n            #print (\"pyramid_feature\")\r\n            #print ()\r\n             #print(np.unique(np.argmax(np.array(final_probnp),axis=1)))\r\n            #for var, val in zip(tmp_2, tmp_2np):\r\n            #    print(var.name)  \r\n            #print(np.argmax(np.array(tmp_0np),axis=1))\r\n            \r\n            \r\n            if np.isnan(tot_loss) or np.isinf(tot_loss):\r\n                print (gt_boxesnp)\r\n                raise\r\n          \r\n        if step % 100 == 0:\r\n            summary_str = sess.run(summary_op)\r\n            summary_writer.add_summary(summary_str, step)\r\n            summary_writer.flush()\r\n\r\n        if (step % 10000 == 0 or step + 1 == FLAGS.max_iters) and step != 0:\r\n            checkpoint_path = os.path.join(FLAGS.train_dir, \r\n                                           FLAGS.dataset_name + '_' + FLAGS.network + '_model.ckpt')\r\n            saver.save(sess, checkpoint_path, global_step=step)\r\n\r\n        if coord.should_stop():\r\n            coord.request_stop()\r\n            coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n\n"})}),"\n",(0,i.jsx)(r.h4,{id:"en80",children:"efficientNet80"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.applications import EfficientNetB0\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\r\nfrom sklearn.metrics import classification_report, confusion_matrix\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n# \u6570\u636e\u96c6\u8def\u5f84\r\ntrain_dir = 'dataset/train'  # \u66ff\u6362\u4e3a\u8bad\u7ec3\u6570\u636e\u96c6\u8def\u5f84\r\nval_dir = 'dataset/test'      # \u66ff\u6362\u4e3a\u9a8c\u8bc1\u6570\u636e\u96c6\u8def\u5f84\r\n\r\n# \u53c2\u6570\u914d\u7f6e\r\nimg_height, img_width = 224, 224   # EfficientNetB0 \u9ed8\u8ba4\u8f93\u5165\u5c3a\u5bf8\r\nbatch_size = 32\r\nnum_classes = 3                    # \u7c7b\u522b\u6570\u91cf\uff0c\u5047\u8bbe3\u79cd\u5929\u6c14\u7c7b\u578b\r\nclass_labels = ['Rain', 'Fog', 'Snow']  # \u7c7b\u522b\u6807\u7b7e\r\n# 1. \u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1./255,\r\n    rotation_range=20,\r\n    width_shift_range=0.2,\r\n    height_shift_range=0.2,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True,\r\n    brightness_range=[0.8, 1.2],  # \u589e\u52a0\u4eae\u5ea6\u53d8\u5316\r\n    channel_shift_range=20.0     # \u589e\u52a0\u989c\u8272\u901a\u9053\u53d8\u5316\r\n)\r\n\r\nval_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical'\r\n)\r\n\r\nval_generator = val_datagen.flow_from_directory(\r\n    val_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    shuffle=False  # \u7981\u6b62\u6253\u4e71\uff0c\u786e\u4fdd\u8f93\u51fa\u987a\u5e8f\u4e00\u81f4\r\n)\r\n\r\n# 2. \u6a21\u578b\u6784\u5efa\r\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)   # \u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\r\nx = Dropout(0.5)(x)               # Dropout\u5c42\uff0c\u9632\u6b62\u8fc7\u62df\u5408\r\noutput = Dense(num_classes, activation='softmax')(x)\r\n\r\nmodel = Model(inputs=base_model.input, outputs=output)\r\n\r\n# \u89e3\u51bbEfficientNetB0\u7684\u6700\u540e\u51e0\u5c42\r\nfor layer in base_model.layers[-10:]:\r\n    layer.trainable = True\r\n\r\n# 3. \u7f16\u8bd1\u6a21\u578b\r\noptimizer = SGD(lr=0.001, momentum=0.9)  # \u4f7f\u7528SGD\u4f18\u5316\u5668\r\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n# 4. \u8bad\u7ec3\u6a21\u578b\r\nepochs = 20  # \u589e\u52a0\u8bad\u7ec3\u5468\u671f\r\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\r\n\r\nhistory = model.fit(\r\n    train_generator,\r\n    validation_data=val_generator,\r\n    epochs=epochs,\r\n    callbacks=[lr_scheduler]\r\n)\r\n\r\n# 5. \u6a21\u578b\u8bc4\u4f30\r\nval_loss, val_accuracy = model.evaluate(val_generator)\r\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\r\n\r\n# \u83b7\u53d6\u9884\u6d4b\u7ed3\u679c\r\ny_pred = model.predict(val_generator)\r\ny_pred_classes = np.argmax(y_pred, axis=1)\r\ny_true = val_generator.classes\r\n\r\n# \u6253\u5370\u5206\u7c7b\u62a5\u544a\r\nprint(\"Classification Report:\")\r\nprint(classification_report(y_true, y_pred_classes, target_names=class_labels))\r\n\r\n# \u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\r\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\r\nprint(\"Confusion Matrix:\")\r\nprint(conf_matrix)\r\n\r\n# 6. \u7ed8\u5236\u8bad\u7ec3\u66f2\u7ebf\r\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u66f2\u7ebf\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(history.history['accuracy'], label='Train Accuracy')\r\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Accuracy')\r\nplt.title('Training and Validation Accuracy')\r\nplt.legend()\r\nplt.show()\r\n\r\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(history.history['loss'], label='Train Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.title('Training and Validation Loss')\r\nplt.legend()\r\nplt.show()\n"})}),"\n",(0,i.jsx)(r.h4,{id:"cnn",children:"CNN"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import numpy as np\r\nimport pandas as pd\r\nimport os\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n# 1. \u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\r\n# \u5047\u8bbe\u6bcf\u79cd\u5929\u6c14\u7c7b\u578b\u7684\u56fe\u7247\u5b58\u653e\u5728\u4e0d\u540c\u7684\u6587\u4ef6\u5939\u91cc\uff0c\u6587\u4ef6\u5939\u540d\u79f0\u4e3a\u5929\u6c14\u7c7b\u578b\u7684\u540d\u79f0\r\ndata_dir = 'dataset/train'  # \u66ff\u6362\u4e3a\u4f60\u7684\u6570\u636e\u96c6\u8def\u5f84\r\nimg_height, img_width = 128, 128  # \u56fe\u7247\u5927\u5c0f\u8c03\u6574\u4e3a\u4e00\u81f4\r\nbatch_size = 32\r\n\r\n# \u4f7f\u7528ImageDataGenerator\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u548c\u52a0\u8f7d\r\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)  # \u5c06\u6570\u636e\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\r\n\r\ntrain_generator = datagen.flow_from_directory(\r\n    data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='training'\r\n)\r\n\r\nvalidation_generator = datagen.flow_from_directory(\r\n    data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='validation'\r\n)\r\n\r\n# 2. \u6784\u5efaCNN\u6a21\u578b\r\nmodel = Sequential([\r\n    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(64, (3, 3), activation='relu'),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(128, (3, 3), activation='relu'),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(),\r\n    Dense(128, activation='relu'),\r\n    Dropout(0.5),\r\n    Dense(4, activation='softmax')  # \u5047\u8bbe\u6709\u56db\u79cd\u5929\u6c14\u7c7b\u578b\r\n])\r\n\r\nmodel.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n# 3. \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(\r\n    train_generator,\r\n    validation_data=validation_generator,\r\n    epochs=10\r\n)\r\n\r\n# 4. \u6a21\u578b\u8bc4\u4f30\r\nvalidation_generator.reset()\r\ny_pred = model.predict(validation_generator)\r\ny_pred_classes = np.argmax(y_pred, axis=1)\r\ny_true = validation_generator.classes\r\n\r\n# \u6253\u5370\u5206\u7c7b\u62a5\u544a\r\nclass_labels = list(validation_generator.class_indices.keys())\r\nprint(\"CNN Classification Report:\\n\", classification_report(y_true, y_pred_classes, target_names=class_labels))\r\n\r\n# \u6253\u5370\u51c6\u786e\u7387\r\nprint(\"CNN Accuracy: \", accuracy_score(y_true, y_pred_classes))\r\n\r\n# 5. \u663e\u793a\u6bcf\u79cd\u5929\u6c14\u7684\u51c6\u786e\u7387\r\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\r\nconf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]  # \u8ba1\u7b97\u6bcf\u7c7b\u7684\u51c6\u786e\u7387\r\nprint(\"Per-class accuracy: \", np.diag(conf_matrix))\r\n\r\n# \u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\r\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\r\n\r\n# \u8f6c\u6362\u6df7\u6dc6\u77e9\u9635\u4e3a\u767e\u5206\u6bd4\u5f62\u5f0f\r\nconf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\r\n\r\n# \u6253\u5370\u6bcf\u7c7b\u7684\u51c6\u786e\u7387\r\nprint(\"Per-class accuracy: \", np.diag(conf_matrix_normalized))\r\n\r\n# \u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\r\nconf_matrix = confusion_matrix(y_true, y_pred_classes)\r\n\r\n# \u8f6c\u6362\u6df7\u6dc6\u77e9\u9635\u4e3a\u767e\u5206\u6bd4\u5f62\u5f0f\r\nconf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\r\n\r\n# \u6253\u5370\u6bcf\u7c7b\u7684\u51c6\u786e\u7387\r\nprint(\"Per-class accuracy: \", np.diag(conf_matrix_normalized))\r\n\r\n# 6. \u4eff\u771f\u56fe\r\n# \u6df7\u6dc6\u77e9\u9635\u53ef\u89c6\u5316\r\nplt.figure(figsize=(8, 6))\r\nsns.heatmap(conf_matrix_normalized, annot=True, cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, fmt=\".2%\")\r\nplt.title('Normalized Confusion Matrix')\r\nplt.xlabel('Predicted')\r\nplt.ylabel('True')\r\nplt.show()\r\n\r\n\r\n# \u663e\u793a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51c6\u786e\u7387\u7684\u53d8\u5316\u8d8b\u52bf\r\nplt.figure(figsize=(8, 6))\r\nplt.plot(history.history['accuracy'], label='Train Accuracy')\r\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\nplt.title('Training and Validation Accuracy')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Accuracy')\r\nplt.legend()\r\nplt.show()\r\n\r\n# \u663e\u793a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u7684\u53d8\u5316\u8d8b\u52bf\r\nplt.figure(figsize=(8, 6))\r\nplt.plot(history.history['loss'], label='Train Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\n"})}),"\n",(0,i.jsx)(r.h4,{id:"jcmx",children:"\u96c6\u6210\u6a21\u578b(lr, svm,rf,knn,dt,nb,mlp)"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import numpy as np\r\nimport pandas as pd\r\nimport os\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom sklearn.ensemble import VotingClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.neural_network import MLPClassifier\r\nfrom tensorflow.keras.applications import VGG16\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Flatten, Dense\r\nfrom tensorflow.keras.layers import Dropout\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\n\r\n# 1. \u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\r\ndata_dir = 'dataset/train'  # \u66ff\u6362\u4e3a\u4f60\u7684\u6570\u636e\u96c6\u8def\u5f84\r\nimg_height, img_width = 128, 128  # \u8c03\u6574\u56fe\u50cf\u5927\u5c0f\r\nbatch_size = 8\r\n\r\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\r\n\r\ntrain_generator = datagen.flow_from_directory(\r\n    data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='training'\r\n)\r\n\r\nvalidation_generator = datagen.flow_from_directory(\r\n    data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='validation'\r\n)\r\n\r\n# \u63d0\u53d6\u7279\u5f81\u5411\u91cf\r\ndef extract_features(generator, model):\r\n    features = []\r\n    labels = []\r\n    for i in range(len(generator)):\r\n        x, y = generator[i]\r\n        feature = model.predict(x)\r\n        features.append(feature)\r\n        labels.append(y)\r\n\r\n    # \u5c06\u6240\u6709\u6279\u6b21\u7684\u7279\u5f81\u548c\u6807\u7b7e\u5806\u53e0\u5728\u4e00\u8d77\r\n    features = np.vstack(features)\r\n    labels = np.vstack(labels)\r\n\r\n    return features, labels\r\n\r\n# \u4f7f\u7528\u9884\u8bad\u7ec3\u7684VGG16\u6a21\u578b\u63d0\u53d6\u7279\u5f81\r\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\r\nmodel = Model(inputs=base_model.input, outputs=Flatten()(base_model.output))\r\n\r\n# \u6dfb\u52a0\u5168\u8fde\u63a5\u5c42\u548c\u81ea\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\r\nx = Flatten()(base_model.output)\r\nx = Dense(128, activation='relu')(x)\r\nx = Dropout(0.5)(x)\r\noutput = Dense(len(train_generator.class_indices), activation='softmax')(x)\r\n\r\n# \u6784\u5efa\u5b8c\u6574\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\r\ncnn_model = Model(inputs=base_model.input, outputs=output)\r\n\r\n# \u51bb\u7ed3VGG16\u7684\u5377\u79ef\u5c42\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\n# \u7f16\u8bd1\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\r\ncnn_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n# \u8bbe\u7f6e\u56de\u8c03\u4fdd\u5b58\u6700\u597d\u7684\u6a21\u578b\r\ncheckpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\r\n\r\n# \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5e76\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\r\nhistory = cnn_model.fit(\r\n    train_generator,\r\n    validation_data=validation_generator,\r\n    epochs=10,  # \u4f60\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8c03\u6574\r\n    callbacks=[checkpoint]\r\n)\r\n\r\n# \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u5c06 accuracy \u548c loss \u53d8\u5316\u8d8b\u52bf\u53ef\u89c6\u5316\r\n# plt.figure(figsize=(10, 7))\r\n# plt.plot(history.history['accuracy'], label='Train Accuracy')\r\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\n# plt.title('Training and Validation Accuracy')\r\n# plt.xlabel('Epoch')\r\n# plt.ylabel('Accuracy')\r\n# plt.legend()\r\n# plt.show()\r\n#\r\n# plt.figure(figsize=(10, 7))\r\n# plt.plot(history.history['loss'], label='Train Loss')\r\n# plt.plot(history.history['val_loss'], label='Validation Loss')\r\n# plt.title('Training and Validation Loss')\r\n# plt.xlabel('Epoch')\r\n# plt.ylabel('Loss')\r\n# plt.legend()\r\n# plt.show()\r\n\r\n# \u63d0\u53d6\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7279\u5f81\r\nX_train, y_train = extract_features(train_generator, model)\r\nX_test, y_test = extract_features(validation_generator, model)\r\n\r\n# 2. \u6a21\u578b\u9009\u62e9\r\n# \u5b9a\u4e49\u591a\u4e2a\u6a21\u578b\r\nmodels = [\r\n    ('lr', LogisticRegression(max_iter=1000)),\r\n    ('svm', SVC(probability=True)),\r\n    ('rf', RandomForestClassifier()),\r\n    ('knn', KNeighborsClassifier()),\r\n    ('dt', DecisionTreeClassifier()),\r\n    ('nb', GaussianNB()),\r\n    ('mlp', MLPClassifier(max_iter=1000))\r\n]\r\n\r\n# 3. \u96c6\u6210\u6a21\u578b\r\n# \u4f7f\u7528VotingClassifier\u8fdb\u884c\u6a21\u578b\u96c6\u6210\r\nensemble_model = VotingClassifier(estimators=models, voting='soft')\r\nensemble_model.fit(X_train, np.argmax(y_train, axis=1))\r\n\r\n# \u83b7\u53d6\u7c7b\u6807\u7b7e\u7684\u540d\u79f0\uff08\u56db\u79cd\u5929\u6c14\u7c7b\u578b\uff09\r\nclass_labels = list(train_generator.class_indices.keys())\r\n\r\n# \u8bc4\u4f30\u96c6\u6210\u6a21\u578b\r\nensemble_model = VotingClassifier(estimators=models, voting='soft')\r\nensemble_model.fit(X_train, np.argmax(y_train, axis=1))\r\n\r\n# 4. \u6a21\u578b\u8bc4\u4f30\r\n# \u9884\u6d4b\u5e76\u8bc4\u4f30\u6027\u80fd\r\ny_pred = ensemble_model.predict(X_test)\r\ny_true = np.argmax(y_test, axis=1)\r\n\r\nprint(\"Classification report of integrated models:\\n\", classification_report(y_true, y_pred, target_names=class_labels))\r\nprint(\"Accuracy of integrated models: \", accuracy_score(y_true, y_pred))\r\n\r\n# 5. \u663e\u793a\u6bcf\u79cd\u5929\u6c14\u7684\u51c6\u786e\u7387\r\nconf_matrix = confusion_matrix(y_true, y_pred)\r\nconf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\r\nper_class_accuracy = np.diag(conf_matrix_normalized)\r\nprint(\"Accuracy for each type of weather: \", per_class_accuracy)\r\n\r\n# 6. \u4eff\u771f\u56fe\r\n# \u6df7\u6dc6\u77e9\u9635\u53ef\u89c6\u5316\r\nplt.figure(figsize=(10, 7))\r\nsns.heatmap(conf_matrix_normalized, annot=True, cmap='Blues', xticklabels=train_generator.class_indices.keys(), yticklabels=train_generator.class_indices.keys(), fmt=\".2f\")\r\nplt.title('Normalized Confusion Matrix')\r\nplt.xlabel('Predicted')\r\nplt.ylabel('True')\r\nplt.show()\r\n\r\n# \u663e\u793a\u6bcf\u79cd\u5929\u6c14\u7684\u51c6\u786e\u7387\r\nplt.figure(figsize=(10, 7))\r\nclass_labels = list(train_generator.class_indices.keys())\r\nplt.bar(class_labels, per_class_accuracy)\r\nplt.title('Per-class Accuracy')\r\nplt.xlabel('Weather Type')\r\nplt.ylabel('Accuracy')\r\nplt.show()\r\n\r\n# \u663e\u793a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51c6\u786e\u7387\u7684\u53d8\u5316\u8d8b\u52bf\r\nplt.figure(figsize=(10, 7))\r\nplt.plot(history.history['accuracy'], label='Train Accuracy')\r\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\r\nplt.title('Training and Validation Accuracy')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Accuracy')\r\nplt.legend()\r\nplt.show()\r\n\r\n# \u663e\u793a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u7684\u53d8\u5316\u8d8b\u52bf\r\nplt.figure(figsize=(10, 7))\r\nplt.plot(history.history['loss'], label='Train Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss')\r\nplt.xlabel('Epoch')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\n"})}),"\n",(0,i.jsx)(r.h4,{id:"vl16",children:"vit_L16"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'import torch\r\nimport torch.nn as nn\r\n\r\nclass ViT(nn.Module):\r\n    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072,\r\n                 dropout=0.1, channels=3):\r\n        super(ViT, self).__init__()\r\n\r\n        # \u521d\u59cb\u5316 patch_size\r\n        self.patch_size = patch_size  # \u4fdd\u5b58\u4e3a\u5b9e\u4f8b\u53d8\u91cf\r\n\r\n        assert image_size % patch_size == 0, "Image size must be divisible by patch size"\r\n        num_patches = (image_size // patch_size) ** 2\r\n        patch_dim = 3 * patch_size ** 2  # Assuming 3-channel images (RGB)\r\n\r\n        # Patch embedding: \u5c06\u56fe\u50cf\u5757\u5c55\u5e73\u5e76\u6620\u5c04\u5230dim\u7ef4\u5ea6\r\n        self.patch_embedding = nn.Linear(patch_dim, dim)\r\n\r\n        # \u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u7f16\u7801\r\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\r\n\r\n        # \u5206\u7c7b token\uff08\u5206\u7c7b\u6807\u8bc6\u7b26\uff09\r\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\r\n\r\n        # Dropout\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n        # Transformer \u6a21\u5757\r\n        self.transformer = nn.ModuleList([\r\n            TransformerBlock(dim, heads, mlp_dim, dropout) for _ in range(depth)\r\n        ])\r\n\r\n        # \u5206\u7c7b\u5934\r\n        self.mlp_head = nn.Sequential(\r\n            nn.LayerNorm(dim),\r\n            nn.Linear(dim, num_classes)\r\n        )\r\n\r\n    def forward(self, x):\r\n        batch_size, channels, height, width = x.shape\r\n\r\n        # \u5c06\u8f93\u5165\u56fe\u50cf\u5212\u5206\u4e3apatches\r\n        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\r\n        patches = patches.contiguous().view(batch_size, -1, self.patch_size ** 2 * channels)\r\n\r\n        # \u5bf9patches\u8fdb\u884c\u7ebf\u6027\u6620\u5c04\r\n        x = self.patch_embedding(patches)\r\n\r\n        # \u6dfb\u52a0\u5206\u7c7b token\r\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\r\n        x = torch.cat((cls_tokens, x), dim=1)\r\n\r\n        # \u6dfb\u52a0\u4f4d\u7f6e\u7f16\u7801\r\n        x += self.pos_embedding\r\n\r\n        # Dropout\r\n        x = self.dropout(x)\r\n\r\n        # \u901a\u8fc7 Transformer \u6a21\u5757\r\n        for block in self.transformer:\r\n            x = block(x)\r\n\r\n        # \u53d6\u51fa\u5206\u7c7b token \u8fdb\u884c\u5206\u7c7b\r\n        cls_token_final = x[:, 0]\r\n\r\n        # \u5206\u7c7b\u5934\r\n        x = self.mlp_head(cls_token_final)\r\n\r\n        return x\r\n\r\n\r\nclass TransformerBlock(nn.Module):\r\n    def __init__(self, dim, heads, mlp_dim, dropout):\r\n        super(TransformerBlock, self).__init__()\r\n        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout)\r\n        self.norm1 = nn.LayerNorm(dim)\r\n        self.norm2 = nn.LayerNorm(dim)\r\n        self.mlp = nn.Sequential(\r\n            nn.Linear(dim, mlp_dim),\r\n            nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(mlp_dim, dim),\r\n            nn.Dropout(dropout)\r\n        )\r\n\r\n    def forward(self, x):\r\n        # Multi-head Attention\r\n        attn_output, _ = self.attention(x, x, x)\r\n        x = attn_output + x\r\n        x = self.norm1(x)\r\n\r\n        # MLP\r\n        mlp_output = self.mlp(x)\r\n        x = mlp_output + x\r\n        x = self.norm2(x)\r\n\r\n        return x\r\n\r\n\r\n# \u6d4b\u8bd5\u6a21\u578b\r\nmodel = ViT(image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072)\r\n\r\n# \u968f\u673a\u751f\u6210\u4e00\u4e2a batch \u7684\u8f93\u5165\u56fe\u50cf\uff0c\u5f62\u72b6\u4e3a [batch_size, channels, height, width]\r\nx = torch.randn(8, 3, 224, 224)  # 8 \u5f20 RGB 224x224 \u56fe\u50cf\r\noutput = model(x)\r\n\r\n#print(output.shape)  # \u8f93\u51fa\u5f62\u72b6\u5e94\u4e3a (8, 1000)\uff0c\u5373 batch_size x num_classes\r\n#\u6a21\u578b\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5f62\u72b6\u4e3a [8, 1000] \u7684\u5f20\u91cf\uff0c\u8868\u793a\u5bf9\u4e8e 8 \u5f20\u8f93\u5165\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u90fd\u6709 1000 \u4e2a\u7c7b\u522b\u7684\u9884\u6d4b\u6982\u7387\uff0c\u901a\u5e38\u4f1a\u901a\u8fc7 softmax \u51fd\u6570\u6765\u786e\u5b9a\u6700\u7ec8\u7684\u5206\u7c7b\u7ed3\u679c\u3002\n'})}),"\n",(0,i.jsx)(r.h4,{id:"dn",children:"densenet"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"lass DenseLayer(nn.Module):\r\n    def __init__(self, in_channels, growth_rate):\r\n        super(DenseLayer, self).__init__()\r\n        self.bn1 = nn.BatchNorm2d(in_channels)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv1 = nn.Conv2d(in_channels, 4 * growth_rate, kernel_size=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\r\n        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\r\n\r\n    def forward(self, x):\r\n        out = self.bn1(x)\r\n        out = self.relu(out)\r\n        out = self.conv1(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n        out = self.conv2(out)\r\n        out = torch.cat((x, out), 1)  # concatenate input and output\r\n        return out\r\n\r\nclass DenseBlock(nn.Module):\r\n    def __init__(self, in_channels, num_layers, growth_rate):\r\n        super(DenseBlock, self).__init__()\r\n        layers = []\r\n        for _ in range(num_layers):\r\n            layers.append(DenseLayer(in_channels, growth_rate))\r\n            in_channels += growth_rate\r\n        self.dense_block = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.dense_block(x)\r\n\r\nclass TransitionLayer(nn.Module):\r\n    def __init__(self, in_channels):\r\n        super(TransitionLayer, self).__init__()\r\n        self.bn = nn.BatchNorm2d(in_channels)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, bias=False)\r\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\r\n\r\n    def forward(self, x):\r\n        out = self.bn(x)\r\n        out = self.relu(out)\r\n        out = self.conv(out)\r\n        out = self.pool(out)\r\n        return out\r\n\r\nclass DenseNet(nn.Module):\r\n    def __init__(self, growth_rate=32, num_layers=[6, 12, 24, 16], num_classes=10):\r\n        super(DenseNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        in_channels = 64\r\n        self.dense_blocks = []\r\n        for i in range(len(num_layers)):\r\n            self.dense_blocks.append(DenseBlock(in_channels, num_layers[i], growth_rate))\r\n            in_channels += num_layers[i] * growth_rate\r\n            if i != len(num_layers) - 1:\r\n                self.dense_blocks.append(TransitionLayer(in_channels))\r\n                in_channels //= 2  # halve the channels after transition\r\n\r\n        self.dense_blocks = nn.Sequential(*self.dense_blocks)\r\n        self.bn2 = nn.BatchNorm2d(in_channels)\r\n        self.relu2 = nn.ReLU(inplace=True)\r\n        self.fc = nn.Linear(in_channels, num_classes)\r\n\r\n    def forward(self, x):\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n        out = self.pool(out)\r\n\r\n        out = self.dense_blocks(out)\r\n        out = self.bn2(out)\r\n        out = self.relu2(out)\r\n        out = F.adaptive_avg_pool2d(out, (1, 1))\r\n        out = out.view(out.size(0), -1)\r\n        out = self.fc(out)\r\n        return out\n"})}),"\n",(0,i.jsx)(r.h4,{id:"gn",children:"googlenet"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"class Inception(nn.Module):  #\u5b9a\u4e49 \u5757\u7c7b\r\n    def __init__(self,in_channels,c1,c2,c3,c4):   #c1\u662f\u5377\u79ef\u6838\u6570\u91cf\r\n        super(Inception,self).__init__()\r\n        self.ReLu = nn.ReLU()\r\n        #\u8def\u7ebf1 \uff0c1*1\u5377\u79ef\u5c42\r\n        self.p1_1 = nn.Conv2d(in_channels=in_channels,out_channels=c1,kernel_size = 1)\r\n\r\n        # \u8def\u7ebf2 \uff0c1*1\u5377\u79ef\u5c42  3*3\u5377\u79ef\u5c42\r\n        self.p2_1 = nn.Conv2d(in_channels=in_channels,out_channels=c2[0],kernel_size = 1)\r\n        self.p2_2 = nn.Conv2d(in_channels=c2[0],out_channels=c2[1],kernel_size = 3, padding=1)\r\n\r\n        # \u8def\u7ebf3 \uff0c1*1\u5377\u79ef\u5c42  5*5\u5377\u79ef\u5c42\r\n        self.p3_1 = nn.Conv2d(in_channels=in_channels,out_channels=c3[0],kernel_size = 1)\r\n        self.p3_2 = nn.Conv2d(in_channels=c3[0],out_channels=c3[1],kernel_size = 5, padding=2)\r\n\r\n        # \u8def\u7ebf4 \uff0c3*3\u6700\u5927\u6c60\u5316\uff0c1*1\u5377\u79ef\u5c42\r\n        self.p4_1 = nn.MaxPool2d(kernel_size = 3,padding=1,stride=1)\r\n        self.p4_2 = nn.Conv2d(in_channels=in_channels,out_channels=c4,kernel_size = 1)\r\n\r\n\r\n    def forward(self,x):\r\n        p1 = self.ReLu(self.p1_1(x))\r\n        p2 = self.ReLu(self.p2_2(self.ReLu(self.p2_1(x))))   #\u8def\u7ebf2\r\n        p3 = self.ReLu(self.p3_2(self.ReLu(self.p3_1(x))))   #\u8def\u7ebf3\r\n        p4 = self.ReLu(self.p4_2(self.p4_1(x)))   #\u8def\u7ebf4\r\n\r\n        #print(p1.shape,p2.shape,p3.shape,p4.shape)\r\n        return torch.cat((p1,p2,p3,p4),1)  ###\u7279\u5f81\u878d\u5408\r\n\r\n\r\nclass GoogLeNet(nn.Module):\r\n    def __init__(self,Inception):\r\n        super(GoogLeNet,self).__init__()\r\n        self.b1 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=3,stride=2),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\r\n        )\r\n        self.b2 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        )\r\n        #\u4e00\u51719\u4e2aInception\u5757\r\n        self.b3 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            Inception(192,64,(96,128),(16,32),32),  #192\u662f\u8f93\u5165\uff0c64\u8def\u5f841\u8f93\u51fa\uff0c96128\u8def\u5f842\u7684\u4e24\u6b21\u8f93\u51fa\u3002\u3002\u3002\u3002\r\n            Inception(256,128,(128,192),(32,96),64),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        )\r\n        self.b4 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            Inception(480, 192, (96, 208), (16, 48), 64),  # 192\u662f\u8f93\u5165\uff0c64\u8def\u5f841\u8f93\u51fa\uff0c96128\u8def\u5f842\u7684\u4e24\u6b21\u8f93\u51fa\u3002\u3002\u3002\u3002\r\n            Inception(512, 160, (112, 224), (24, 64), 64),\r\n            Inception(512, 128, (128, 256), (24, 64), 64),\r\n            Inception(512, 112, (128, 288), (32, 64), 64),\r\n            Inception(528, 256, (160, 320), (32, 128), 128),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        )\r\n        self.b5 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            Inception(832, 256, (160, 320), (32, 128), 128),  # 192\u662f\u8f93\u5165\uff0c64\u8def\u5f841\u8f93\u51fa\uff0c96128\u8def\u5f842\u7684\u4e24\u6b21\u8f93\u51fa\u3002\u3002\u3002\u3002\r\n            Inception(832, 384, (192, 384), (48, 128), 128),\r\n            nn.AdaptiveAvgPool2d((1,1)),  #\u5e73\u5747\u6c60\u5316\r\n            nn.Flatten(),\r\n            nn.Linear(1024,5)\r\n        )\r\n\r\n        #\u521d\u59cb\u5316\u6743\u91cd\r\n        for m in self.modules():  #\u4ece\u6a21\u578b\u4e2d\u8c03\u7528\u6bcf\u5c42\u7684\u53c2\u6570\r\n            if isinstance(m,nn.Conv2d):\r\n                nn.init.kaiming_normal_(m.weight,mode=\"fan_out\",nonlinearity='relu')  ###\r\n                if m.bias is not None:\r\n                    nn.init.constant_(m.bias,0)\r\n                elif isinstance(m,nn.Linear):\r\n                    nn.init.normal_(m.weight, 0, 0.01)  ###\r\n                    if m.bias is not None:\r\n                        nn.init.constant_(m.bias, 0)\r\n\r\n    def forward(self,x):   #def\u5b9a\u4e49\u51fd\u6570\r\n        x = self.b1(x)\r\n        x = self.b2(x)\r\n        x = self.b3(x)\r\n        x = self.b4(x)\r\n        x = self.b5(x)\r\n        return x\n"})}),"\n",(0,i.jsx)(r.h4,{id:"mnv3",children:"MoblieNetV3"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"class hswish(nn.Module):\r\n    def forward(self, x):\r\n        out = x * F.relu6(x + 3, inplace=True) / 6\r\n        return out\r\n\r\n\r\nclass hsigmoid(nn.Module):\r\n    def forward(self, x):\r\n        out = F.relu6(x + 3, inplace=True) / 6\r\n        return out\r\n\r\n\r\nclass SeModule(nn.Module):\r\n    def __init__(self, in_size, reduction=4):\r\n        super(SeModule, self).__init__()\r\n        expand_size = max(in_size // reduction, 8)\r\n        self.se = nn.Sequential(\r\n            nn.AdaptiveAvgPool2d(1),\r\n            nn.Conv2d(in_size, expand_size, kernel_size=1, bias=False),\r\n            nn.BatchNorm2d(expand_size),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(expand_size, in_size, kernel_size=1, bias=False),\r\n            nn.Hardsigmoid()\r\n        )\r\n\r\n    def forward(self, x):\r\n        return x * self.se(x)\r\n\r\n\r\nclass Block(nn.Module):\r\n    '''expand + depthwise + pointwise'''\r\n\r\n    def __init__(self, kernel_size, in_size, expand_size, out_size, act, se, stride):\r\n        super(Block, self).__init__()\r\n        self.stride = stride\r\n\r\n        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(expand_size)\r\n        self.act1 = act(inplace=True)\r\n\r\n        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride,\r\n                               padding=kernel_size // 2, groups=expand_size, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(expand_size)\r\n        self.act2 = act(inplace=True)\r\n        self.se = SeModule(expand_size) if se else nn.Identity()\r\n\r\n        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(out_size)\r\n        self.act3 = act(inplace=True)\r\n\r\n        self.skip = None\r\n        if stride == 1 and in_size != out_size:\r\n            self.skip = nn.Sequential(\r\n                nn.Conv2d(in_size, out_size, kernel_size=1, bias=False),\r\n                nn.BatchNorm2d(out_size)\r\n            )\r\n\r\n        if stride == 2 and in_size != out_size:\r\n            self.skip = nn.Sequential(\r\n                nn.Conv2d(in_channels=in_size, out_channels=in_size, kernel_size=3, groups=in_size, stride=2, padding=1,\r\n                          bias=False),\r\n                nn.BatchNorm2d(in_size),\r\n                nn.Conv2d(in_size, out_size, kernel_size=1, bias=True),\r\n                nn.BatchNorm2d(out_size)\r\n            )\r\n\r\n        if stride == 2 and in_size == out_size:\r\n            self.skip = nn.Sequential(\r\n                nn.Conv2d(in_channels=in_size, out_channels=out_size, kernel_size=3, groups=in_size, stride=2,\r\n                          padding=1, bias=False),\r\n                nn.BatchNorm2d(out_size)\r\n            )\r\n\r\n    def forward(self, x):\r\n        skip = x\r\n\r\n        out = self.act1(self.bn1(self.conv1(x)))\r\n        out = self.act2(self.bn2(self.conv2(out)))\r\n        out = self.se(out)\r\n        out = self.bn3(self.conv3(out))\r\n\r\n        if self.skip is not None:\r\n            skip = self.skip(skip)\r\n        return self.act3(out + skip)\r\n\r\n\r\nclass MobileNetV3_Small(nn.Module):\r\n    def __init__(self, num_classes=1000, act=nn.Hardswish):\r\n        super(MobileNetV3_Small, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(16)\r\n        self.hs1 = act(inplace=True)\r\n\r\n        self.bneck = nn.Sequential(\r\n            Block(3, 16, 16, 16, nn.ReLU, True, 2),  #small\u4e0elarge\u7684\u53c2\u6570\u4e0d\u4e00\u6837\r\n            Block(3, 16, 72, 24, nn.ReLU, False, 2),\r\n            Block(3, 24, 88, 24, nn.ReLU, False, 1),\r\n            Block(5, 24, 96, 40, act, True, 2),\r\n            Block(5, 40, 240, 40, act, True, 1),\r\n            Block(5, 40, 240, 40, act, True, 1),\r\n            Block(5, 40, 120, 48, act, True, 1),\r\n            Block(5, 48, 144, 48, act, True, 1),\r\n            Block(5, 48, 288, 96, act, True, 2),\r\n            Block(5, 96, 576, 96, act, True, 1),\r\n            Block(5, 96, 576, 96, act, True, 1),\r\n        )\r\n\r\n        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(576)\r\n        self.hs2 = act(inplace=True)\r\n        self.gap = nn.AdaptiveAvgPool2d(1)\r\n\r\n        self.linear3 = nn.Linear(576, 1280, bias=False)\r\n        self.bn3 = nn.BatchNorm1d(1280)\r\n        self.hs3 = act(inplace=True)\r\n        self.drop = nn.Dropout(0.2)\r\n        self.linear4 = nn.Linear(1280, num_classes)\r\n        self.init_params()\r\n\r\n    def init_params(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                init.kaiming_normal_(m.weight, mode='fan_out')\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                init.constant_(m.weight, 1)\r\n                init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.Linear):\r\n                init.normal_(m.weight, std=0.001)\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        out = self.hs1(self.bn1(self.conv1(x)))\r\n        out = self.bneck(out)\r\n\r\n        out = self.hs2(self.bn2(self.conv2(out)))\r\n        out = self.gap(out).flatten(1)\r\n        out = self.drop(self.hs3(self.bn3(self.linear3(out))))\r\n\r\n        return self.linear4(out)\r\n\r\n\r\nclass MobileNetV3_Large(nn.Module):\r\n    def __init__(self, num_classes=1000, act=nn.Hardswish):\r\n        super(MobileNetV3_Large, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(16)\r\n        self.hs1 = act(inplace=True)\r\n\r\n        self.bneck = nn.Sequential(\r\n            Block(3, 16, 16, 16, nn.ReLU, False, 1),\r\n            Block(3, 16, 64, 24, nn.ReLU, False, 2),\r\n            Block(3, 24, 72, 24, nn.ReLU, False, 1),\r\n            Block(5, 24, 72, 40, nn.ReLU, True, 2),\r\n            Block(5, 40, 120, 40, nn.ReLU, True, 1),\r\n            Block(5, 40, 120, 40, nn.ReLU, True, 1),\r\n            Block(3, 40, 240, 80, act, False, 2),\r\n            Block(3, 80, 200, 80, act, False, 1),\r\n            Block(3, 80, 184, 80, act, False, 1),\r\n            Block(3, 80, 184, 80, act, False, 1),\r\n            Block(3, 80, 480, 112, act, True, 1),\r\n            Block(3, 112, 672, 112, act, True, 1),\r\n            Block(5, 112, 672, 160, act, True, 2),\r\n            Block(5, 160, 672, 160, act, True, 1),\r\n            Block(5, 160, 960, 160, act, True, 1),\r\n        )\r\n\r\n        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(960)\r\n        self.hs2 = act(inplace=True)\r\n        self.gap = nn.AdaptiveAvgPool2d(1)\r\n\r\n        self.linear3 = nn.Linear(960, 1280, bias=False)\r\n        self.bn3 = nn.BatchNorm1d(1280)\r\n        self.hs3 = act(inplace=True)\r\n        self.drop = nn.Dropout(0.2)\r\n\r\n        self.linear4 = nn.Linear(1280, num_classes)\r\n        self.init_params()\r\n\r\n    def init_params(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                init.kaiming_normal_(m.weight, mode='fan_out')\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                init.constant_(m.weight, 1)\r\n                init.constant_(m.bias, 0)\r\n            elif isinstance(m, nn.Linear):\r\n                init.normal_(m.weight, std=0.001)\r\n                if m.bias is not None:\r\n                    init.constant_(m.bias, 0)\r\n\r\n    def forward(self, x):\r\n        out = self.hs1(self.bn1(self.conv1(x)))\r\n        out = self.bneck(out)\r\n\r\n        out = self.hs2(self.bn2(self.conv2(out)))\r\n        out = self.gap(out).flatten(1)\r\n        out = self.drop(self.hs3(self.bn3(self.linear3(out))))\r\n\r\n        return self.linear4(out)\n"})}),"\n",(0,i.jsx)(r.h4,{id:"rn18",children:"ResNet18"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"class Residual(nn.Module):\r\n    def __init__(self, input_channels, num_channels, use_1conv = False,strides = 1):  # c1\u662f\u5377\u79ef\u6838\u6570\u91cf\r\n        super(Residual, self).__init__()\r\n        self.ReLu = nn.ReLU()\r\n        self.conv1 = nn.Conv2d(in_channels=input_channels,out_channels=num_channels,kernel_size=3,padding=1,stride=strides)\r\n        self.conv2 = nn.Conv2d(in_channels=num_channels,out_channels=num_channels,kernel_size=3,padding=1)\r\n        self.bn1 = nn.BatchNorm2d(num_channels)\r\n        self.bn2 = nn.BatchNorm2d(num_channels)\r\n        if use_1conv:   #\u5982\u679c\u67091*1\u5377\u79ef\r\n            self.conv3 = nn.Conv2d(in_channels=input_channels,out_channels=num_channels,kernel_size=1,stride=strides)\r\n        else:\r\n            self.conv3 = None\r\n\r\n    def forward(self,x):  #\u524d\u5411\u4f20\u64ad\r\n        y = self.ReLu(self.bn1(self.conv1(x)))\r\n        y = self.bn2(self.conv2(y))\r\n\r\n        if self.conv3:\r\n            x = self.conv3(x)\r\n        y = self.ReLu(x + y)\r\n        return y\r\n\r\n\r\n\r\nclass ResNet18(nn.Module):\r\n    def __init__(self,Residual):\r\n        super(ResNet18,self).__init__()\r\n        self.b1 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=2),\r\n            nn.ReLU(),\r\n            nn.BatchNorm2d(64),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        )\r\n        self.b2 = nn.Sequential(  #\u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n             Residual(64, 64, use_1conv = False,strides = 1),\r\n             Residual(64, 64, use_1conv = False,strides = 1),\r\n        )\r\n        self.b3 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(64,  128, use_1conv=True, strides=2),\r\n            Residual(128, 128, use_1conv=False, strides=1),\r\n        )\r\n        self.b4 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(128, 256, use_1conv=True, strides=2),\r\n            Residual(256, 256, use_1conv=False, strides=1),\r\n        )\r\n        self.b5 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(256, 512, use_1conv=True, strides=2),\r\n            Residual(512, 512, use_1conv=False, strides=1),\r\n        )\r\n        self.b6 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            nn.AdaptiveAvgPool2d((1,1)),\r\n            nn.Flatten(),\r\n            nn.Linear(512,5)\r\n        )\r\n    def forward(self,x):\r\n        x = self.b1(x)\r\n        x = self.b2(x)\r\n        x = self.b3(x)\r\n        x = self.b4(x)\r\n        x = self.b5(x)\r\n        x = self.b6(x)\r\n        return x\n"})}),"\n",(0,i.jsx)(r.h4,{id:"rn50",children:"ResNet50"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:"import torch\r\nimport torch.nn as nn\r\n\r\n# --------------------------------#\r\n# \u4ecetorch\u5b98\u65b9\u53ef\u4ee5\u4e0b\u8f7dresnet50\u7684\u6743\u91cd\r\n# --------------------------------#\r\nmodel_urls = {\r\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\r\n}\r\n\r\n\r\n# -----------------------------------------------#\r\n# \u6b64\u5904\u4e3a\u5b9a\u4e493*3\u7684\u5377\u79ef\uff0c\u5373\u4e3a\u6307\u6b64\u6b21\u5377\u79ef\u7684\u5377\u79ef\u6838\u7684\u5927\u5c0f\u4e3a3*3\r\n# -----------------------------------------------#\r\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\r\n\r\n\r\n# -----------------------------------------------#\r\n# \u6b64\u5904\u4e3a\u5b9a\u4e491*1\u7684\u5377\u79ef\uff0c\u5373\u4e3a\u6307\u6b64\u6b21\u5377\u79ef\u7684\u5377\u79ef\u6838\u7684\u5927\u5c0f\u4e3a1*1\r\n# -----------------------------------------------#\r\ndef conv1x1(in_planes, out_planes, stride=1):\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n\r\n\r\n# ----------------------------------#\r\n# \u6b64\u4e3aresnet50\u4e2d\u6807\u51c6\u6b8b\u5dee\u7ed3\u6784\u7684\u5b9a\u4e49\r\n# conv3x3\u4ee5\u53caconv1x1\u5747\u5728\u8be5\u7ed3\u6784\u4e2d\u88ab\u5b9a\u4e49\r\n# ----------------------------------#\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1,\r\n                 norm_layer=None):\r\n        super(Bottleneck, self).__init__()\r\n        # --------------------------------------------#\r\n        # \u5f53\u4e0d\u6307\u5b9a\u6b63\u5219\u5316\u64cd\u4f5c\u65f6\u5c06\u4f1a\u9ed8\u8ba4\u8fdb\u884c\u4e8c\u7ef4\u7684\u6570\u636e\u5f52\u4e00\u5316\u64cd\u4f5c\r\n        # --------------------------------------------#\r\n        if norm_layer is None:\r\n            norm_layer = nn.BatchNorm2d\r\n        # ---------------------------------------------------#\r\n        # \u6839\u636einput\u7684planes\u786e\u5b9awidth,width\u7684\u503c\u4e3a\r\n        # \u5377\u79ef\u8f93\u51fa\u901a\u9053\u4ee5\u53caBatchNorm2d\u7684\u6570\u503c\r\n        # \u56e0\u4e3a\u5728\u63a5\u4e0b\u6765resnet\u7ed3\u6784\u6784\u5efa\u7684\u8fc7\u7a0b\u4e2d\u7ed9\u5230\u7684planes\u7684\u6570\u503c\u4e0d\u76f8\u540c\r\n        # ---------------------------------------------------#\r\n        width = int(planes * (base_width / 64.)) * groups\r\n        # -----------------------------------------------#\r\n        # \u5f53\u6b65\u957f\u7684\u503c\u4e0d\u4e3a1\u65f6,self.conv2 and self.downsample\r\n        # \u7684\u4f5c\u7528\u5747\u4e3a\u5bf9\u8f93\u5165\u8fdb\u884c\u4e0b\u91c7\u6837\u64cd\u4f5c\r\n        # \u4e0b\u9762\u4e3a\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u64cd\u4f5c,\u5305\u62ec\u5377\u79ef\uff0c\u6570\u636e\u5f52\u4e00\u5316\u4ee5\u53carelu\u7b49\r\n        # -----------------------------------------------#\r\n        self.conv1 = conv1x1(inplanes, width)\r\n        self.bn1 = norm_layer(width)\r\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\r\n        self.bn2 = norm_layer(width)\r\n        self.conv3 = conv1x1(width, planes * self.expansion)\r\n        self.bn3 = norm_layer(planes * self.expansion)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    # --------------------------------------#\r\n    # \u5b9a\u4e49resnet50\u4e2d\u7684\u6807\u51c6\u6b8b\u5dee\u7ed3\u6784\u7684\u524d\u5411\u4f20\u64ad\u51fd\u6570\r\n    # --------------------------------------#\r\n    def forward(self, x):\r\n        identity = x\r\n        # -------------------------------------------------------------------------#\r\n        # conv1*1->bn1->relu \u5148\u8fdb\u884c\u4e00\u6b211*1\u7684\u5377\u79ef\u4e4b\u540e\u8fdb\u884c\u6570\u636e\u5f52\u4e00\u5316\u64cd\u4f5c\u6700\u540e\u8fc7relu\u589e\u52a0\u975e\u7ebf\u6027\u56e0\u7d20\r\n        # conv3*3->bn2->relu \u5148\u8fdb\u884c\u4e00\u6b213*3\u7684\u5377\u79ef\u4e4b\u540e\u8fdb\u884c\u6570\u636e\u5f52\u4e00\u5316\u64cd\u4f5c\u6700\u540e\u8fc7relu\u589e\u52a0\u975e\u7ebf\u6027\u56e0\u7d20\r\n        # conv1*1->bn3 \u5148\u8fdb\u884c\u4e00\u6b211*1\u7684\u5377\u79ef\u4e4b\u540e\u8fdb\u884c\u6570\u636e\u5f52\u4e00\u5316\u64cd\u4f5c\r\n        # -------------------------------------------------------------------------#\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n        # -----------------------------#\r\n        # \u82e5\u6709\u4e0b\u91c7\u6837\u64cd\u4f5c\u5219\u8fdb\u884c\u4e00\u6b21\u4e0b\u91c7\u6837\u64cd\u4f5c\r\n        # -----------------------------#\r\n        if self.downsample is not None:\r\n            identity = self.downsample(identity)\r\n        # ---------------------------------------------#\r\n        # \u9996\u5148\u662f\u5c06\u4e24\u90e8\u5206\u8fdb\u884cadd\u64cd\u4f5c,\u6700\u540e\u8fc7relu\u6765\u589e\u52a0\u975e\u7ebf\u6027\u56e0\u7d20\r\n        # concat\uff08\u5806\u53e0\uff09\u53ef\u4ee5\u770b\u4f5c\u662f\u901a\u9053\u6570\u7684\u589e\u52a0\r\n        # add\uff08\u76f8\u52a0\uff09\u53ef\u4ee5\u770b\u4f5c\u662f\u7279\u5f81\u56fe\u76f8\u52a0\uff0c\u901a\u9053\u6570\u4e0d\u53d8\r\n        # add\u53ef\u4ee5\u770b\u4f5c\u7279\u6b8a\u7684concat,\u5e76\u4e14\u5176\u8ba1\u7b97\u91cf\u76f8\u5bf9\u8f83\u5c0f\r\n        # ---------------------------------------------#\r\n        out += identity\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\n# --------------------------------#\r\n# \u6b64\u4e3aresnet50\u7f51\u7edc\u7684\u5b9a\u4e49\r\n# input\u7684\u5927\u5c0f\u4e3a224*224\r\n# \u521d\u59cb\u5316\u51fd\u6570\u4e2d\u7684block\u5373\u4e3a\u4e0a\u9762\u5b9a\u4e49\u7684\r\n# \u6807\u51c6\u6b8b\u5dee\u7ed3\u6784--Bottleneck\r\n# --------------------------------#\r\nclass ResNet(nn.Module):\r\n\r\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\r\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\r\n                 norm_layer=None):\r\n\r\n        super(ResNet, self).__init__()\r\n        if norm_layer is None:\r\n            norm_layer = nn.BatchNorm2d\r\n        self._norm_layer = norm_layer\r\n        self.inplanes = 64\r\n        self.dilation = 1\r\n        # ---------------------------------------------------------#\r\n        # \u4f7f\u7528\u81a8\u80c0\u7387\u6765\u66ff\u4ee3stride,\u82e5replace_stride_with_dilation\u4e3anone\r\n        # \u5219\u8fd9\u4e2a\u5217\u8868\u4e2d\u7684\u4e09\u4e2a\u503c\u5747\u4e3aFalse\r\n        # ---------------------------------------------------------#\r\n        if replace_stride_with_dilation is None:\r\n            replace_stride_with_dilation = [False, False, False]\r\n        # ----------------------------------------------#\r\n        # \u82e5replace_stride_with_dilation\u8fd9\u4e2a\u5217\u8868\u7684\u957f\u5ea6\u4e0d\u4e3a3\r\n        # \u5219\u4f1a\u6709ValueError\r\n        # ----------------------------------------------#\r\n        if len(replace_stride_with_dilation) != 3:\r\n            raise ValueError(\"replace_stride_with_dilation should be None \"\r\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\r\n\r\n        self.block = block\r\n        self.groups = groups\r\n        self.base_width = width_per_group\r\n        # -----------------------------------#\r\n        # conv1*1->bn1->relu\r\n        # 224,224,3 -> 112,112,64\r\n        # -----------------------------------#\r\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\r\n        self.bn1 = norm_layer(self.inplanes)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        # ------------------------------------#\r\n        # \u6700\u5927\u6c60\u5316\u53ea\u4f1a\u6539\u53d8\u7279\u5f81\u56fe\u50cf\u7684\u9ad8\u5ea6\u4ee5\u53ca\r\n        # \u5bbd\u5ea6,\u5176\u901a\u9053\u6570\u5e76\u4e0d\u4f1a\u53d1\u751f\u6539\u53d8\r\n        # 112,112,64 -> 56,56,64\r\n        # ------------------------------------#\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        # 56,56,64   -> 56,56,256\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n\r\n        # 56,56,256  -> 28,28,512\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\r\n\r\n        # 28,28,512  -> 14,14,1024\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\r\n\r\n        # 14,14,1024 -> 7,7,2048\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\r\n        # --------------------------------------------#\r\n        # \u81ea\u9002\u5e94\u7684\u4e8c\u7ef4\u5e73\u5747\u6c60\u5316\u64cd\u4f5c,\u7279\u5f81\u56fe\u50cf\u7684\u9ad8\u548c\u5bbd\u7684\u503c\u5747\u53d8\u4e3a1\r\n        # \u5e76\u4e14\u7279\u5f81\u56fe\u50cf\u7684\u901a\u9053\u6570\u5c06\u4e0d\u4f1a\u53d1\u751f\u6539\u53d8\r\n        # 7,7,2048 -> 1,1,2048\r\n        # --------------------------------------------#\r\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n        # ----------------------------------------#\r\n        # \u5c06\u76ee\u524d\u7684\u7279\u5f81\u901a\u9053\u6570\u53d8\u6210\u6240\u8981\u6c42\u7684\u7279\u5f81\u901a\u9053\u6570\uff081000\uff09\r\n        # 2048 -> num_classes\r\n        # ----------------------------------------#\r\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n\r\n        # -------------------------------#\r\n        # \u90e8\u5206\u6743\u91cd\u7684\u521d\u59cb\u5316\u64cd\u4f5c\r\n        # -------------------------------#\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n                nn.init.constant_(m.weight, 1)\r\n                nn.init.constant_(m.bias, 0)\r\n        # -------------------------------#\r\n        # \u90e8\u5206\u6743\u91cd\u7684\u521d\u59cb\u5316\u64cd\u4f5c\r\n        # -------------------------------#\r\n        if zero_init_residual:\r\n            for m in self.modules():\r\n                if isinstance(m, Bottleneck):\r\n                    nn.init.constant_(m.bn3.weight, 0)\r\n\r\n    # --------------------------------------#\r\n    # _make_layer\u8fd9\u4e2a\u51fd\u6570\u7684\u5b9a\u4e49\u5176\u53ef\u4ee5\u5728\u7c7b\u7684\r\n    # \u521d\u59cb\u5316\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\r\n    # block\u5373\u4e3a\u4e0a\u9762\u5b9a\u4e49\u7684\u6807\u51c6\u6b8b\u5dee\u7ed3\u6784--Bottleneck\r\n    # --------------------------------------#\r\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\r\n        norm_layer = self._norm_layer\r\n        downsample = None\r\n        previous_dilation = self.dilation\r\n        # -----------------------------------#\r\n        # \u5728\u51fd\u6570\u7684\u5b9a\u4e49\u4e2ddilate\u7684\u503c\u4e3aFalse\r\n        # \u6240\u4ee5\u8bf4\u4e0b\u9762\u7684\u8bed\u53e5\u5c06\u76f4\u63a5\u8df3\u8fc7\r\n        # -----------------------------------#\r\n        if dilate:\r\n            self.dilation *= stride\r\n            stride = 1\r\n        # -----------------------------------------------------------#\r\n        # \u5982\u679cstride\uff01=1\u6216\u8005self.inplanes != planes * block.expansion\r\n        # \u5219downsample\u5c06\u6709\u4e00\u6b211*1\u7684conv\u4ee5\u53ca\u4e00\u6b21BatchNorm2d\r\n        # -----------------------------------------------------------#\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                conv1x1(self.inplanes, planes * block.expansion, stride),\r\n                norm_layer(planes * block.expansion),\r\n            )\r\n        # -----------------------------------------------#\r\n        # \u9996\u5148\u5b9a\u4e49\u4e00\u4e2alayers,\u5176\u4e3a\u4e00\u4e2a\u5217\u8868\r\n        # \u5377\u79ef\u5757\u7684\u5b9a\u4e49,\u6bcf\u4e00\u4e2a\u5377\u79ef\u5757\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2aBottleneck\u7684\u4f7f\u7528\r\n        # -----------------------------------------------#\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\r\n                            self.base_width, previous_dilation, norm_layer))\r\n        self.inplanes = planes * block.expansion\r\n        for _ in range(1, blocks):\r\n            # identity_block\r\n            layers.append(block(self.inplanes, planes, groups=self.groups,\r\n                                base_width=self.base_width, dilation=self.dilation,\r\n                                norm_layer=norm_layer))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    # ------------------------------#\r\n    # resnet50\u7684\u524d\u5411\u4f20\u64ad\u51fd\u6570\r\n    # ------------------------------#\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        x = self.avgpool(x)\r\n        # --------------------------------------#\r\n        # \u6309\u7167x\u7684\u7b2c1\u4e2a\u7ef4\u5ea6\u62fc\u63a5\uff08\u6309\u7167\u5217\u6765\u62fc\u63a5\uff0c\u6a2a\u5411\u62fc\u63a5\uff09\r\n        # \u62fc\u63a5\u4e4b\u540e,\u5f20\u91cf\u7684shape\u4e3a(batch_size,2048)\r\n        # --------------------------------------#\r\n        x = torch.flatten(x, 1)\r\n        # --------------------------------------#\r\n        # \u8fc7\u5168\u8fde\u63a5\u5c42\u6765\u8c03\u6574\u7279\u5f81\u901a\u9053\u6570\r\n        # (batch_size,2048)->(batch_size,1000)\r\n        # --------------------------------------#\r\n        x = self.fc(x)\r\n        return x\r\n\r\nblock =Bottleneck\r\nF = torch.randn(16, 3, 224, 224)\r\nprint(\"As begin,shape:\", format(F.shape))\r\nresnet = ResNet(block, [3, 4, 6, 3])\r\nF = resnet(F)\r\nprint(F.shape)\n"})}),"\n",(0,i.jsx)(r.h4,{id:"v16",children:"VGG16"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-py",children:'import  torch\r\nfrom torch import nn\r\nfrom torch.cuda import device\r\nfrom torchsummary import summary\r\n\r\nclass VGG16(nn.Module):\r\n    def __init__(self):\r\n        super(VGG16,self).__init__()\r\n        self.block1 = nn.Sequential(  #\u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2,stride=2)\r\n        )\r\n        self.block2 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        ) #\u5b9a\u4e49\u5757\r\n        self.block3 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block4 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block5 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block6 = nn.Sequential(    #\u5168\u8fde\u63a5\u5c42\u4e0d\u7528relu\u6fc0\u6d3b\r\n            nn.Flatten(),\r\n            nn.Linear(7*7*512,128),#\u8981\u5199\u8f93\u5165\u8f93\u51fa\u5927\u5c0f    \u663e\u5b58\u5c0f\uff0c\u8bbe\u7f6e\u7684\u5c0f\u70b9\u513f    \u539f\u67654096\r\n            nn.ReLU(),\r\n            nn.Linear(128,64),  # 4096 \uff0c 4096\r\n            nn.ReLU(),\r\n            nn.Linear(64,10)    # 4096 \uff0c 10\r\n        )\r\n\r\n        #\u521d\u59cb\u5316w\uff0cb \uff0c\u9632\u6b62\u8bad\u7ec3\u540e\u4e0d\u6536\u655b\r\n        for m in self.modules():\r\n            #print(m)  #\u6253\u5370\u6240\u6709\u7f51\u7edc\u5c42\r\n            if isinstance(m, nn.Conv2d):     #\u5377\u79ef\u521d\u59cb\u5316\u6743\u91cdw\u65f6\uff0c\u7528\u51ef\u660e\u521d\u59cb\u5316\r\n                nn.init.kaiming_normal_(m.weight,nonlinearity=\'relu\')    #kaiming\u9488\u5bf9\u4e8e\u6fc0\u6d3b\u51fd\u6570\r\n                if m.bias is not None:   # b\u662f\u504f\u79fb\u503c\uff0c\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u521d\u59cb\u5316\u4e3a0\r\n                    nn.init.constant_(m.bias,0)\r\n            elif isinstance(m, nn.Linear):  #\u5168\u8fde\u63a5\u5c42\u521d\u59cb\u5316\r\n                nn.init.normal_(m.weight,0,0.01)  #\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a0.01\r\n                if m.bias is not None:   # b\u662f\u504f\u79fb\u503c\uff0c\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u521d\u59cb\u5316\u4e3a0\r\n                    nn.init.constant_(m.bias,0)\r\n\r\n\r\n    def forward(self,x):\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        x = self.block3(x)\r\n        x = self.block4(x)\r\n        x = self.block5(x)\r\n        x = self.block6(x)\r\n        return x\r\n\r\nif __name__ == "__main__":\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n    model = VGG16().to(device)\r\n    print(summary(model,(3,224,224)))\n'})})]})}function f(n={}){const{wrapper:r}={...(0,t.R)(),...n.components};return r?(0,i.jsx)(r,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,r,e)=>{e.d(r,{R:()=>a,x:()=>o});var s=e(6540);const i={},t=s.createContext(i);function a(n){const r=s.useContext(t);return s.useMemo((function(){return"function"==typeof n?n(r):{...r,...n}}),[r,n])}function o(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),s.createElement(t.Provider,{value:r},n.children)}}}]);