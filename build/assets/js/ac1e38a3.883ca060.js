"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1880],{2398:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>_,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"time-forcasting/precipitation","title":"precipitation","description":"\u964d\u96e8\u91cf\u9884\u6d4b","source":"@site/docs/time-forcasting/precipitation.md","sourceDirName":"time-forcasting","slug":"/time-forcasting/precipitation","permalink":"/docs/time-forcasting/precipitation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"others","permalink":"/docs/time-forcasting/others"}}');var a=n(4848),s=n(8453);const i={},l=void 0,o={},d=[{value:"\u964d\u96e8\u91cf\u9884\u6d4b",id:"\u964d\u96e8\u91cf\u9884\u6d4b",level:2},{value:"\u5218\u65b0\u5b87",id:"lxy",level:4},{value:"\u5f90\u534e",id:"XH",level:4},{value:"LSTM\u6a21\u578b",id:"LSTM",level:4},{value:"CEEMDAN",id:"CEEMDAN",level:4}];function p(r){const e={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...r.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h2,{id:"\u964d\u96e8\u91cf\u9884\u6d4b",children:"\u964d\u96e8\u91cf\u9884\u6d4b"}),"\n",(0,a.jsx)(e.p,{children:"[\u6e90\u4ee3\u7801\u94fe\u63a5](../../precipitation.rar\uff09"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.a,{href:"#lxy",children:"\u5218\u65b0\u5b87"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.a,{href:"#XH",children:"\u5f90\u534e"})}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#LSTM",children:"LSTM"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#CEEMDAN",children:"CEEMDAN(\u6539\u8fdb\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3)"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h4,{id:"lxy",children:"\u5218\u65b0\u5b87"}),"\n",(0,a.jsx)(e.p,{children:"\u7a0b\u5e8f\u5165\u53e3"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",metastring:"title='run'",children:"\r\n#RNN\u65f6\u95f4\u5e8f\u5217\r\ndef main():\r\n    #\u8bfb\u53d6\u6240\u9700\u53c2\u6570\r\n    configs = json.load(open('config.json', 'r'))\r\n    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\r\n    #\u8bfb\u53d6\u6570\u636e\r\n    data = DataLoader(\r\n        os.path.join('data', configs['data']['filename']),\r\n        configs['data']['train_test_split'],\r\n        configs['data']['columns']\r\n    )\r\n    #\u521b\u5efaRNN\u6a21\u578b\r\n    model = Model()\r\n    mymodel = model.build_model(configs)\r\n    \r\n    plot_model(mymodel, to_file='model.png',show_shapes=True)\r\n    \r\n    #\u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\r\n    x, y = data.get_train_data(\r\n        seq_len=configs['data']['sequence_length'],\r\n        normalise=configs['data']['normalise']\r\n    )\r\n    print (x.shape)\r\n    print (y.shape)\r\n    \r\n\t#\u8bad\u7ec3\u6a21\u578b\r\n    model.train(\r\n\t\tx,\r\n\t\ty,\r\n\t\tepochs = configs['training']['epochs'],\r\n\t\tbatch_size = configs['training']['batch_size'],\r\n\t\tsave_dir = configs['model']['save_dir']\r\n\t)\r\n\t\r\n   #\u6d4b\u8bd5\u7ed3\u679c\r\n    x_test, y_test = data.get_test_data(\r\n        seq_len=configs['data']['sequence_length'],\r\n        normalise=configs['data']['normalise']\r\n    )\r\n    \r\n    #\u5c55\u793a\u6d4b\u8bd5\u6548\u679c\r\n    predictions = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'],debug=False)\r\n    print (np.array(predictions).shape)\r\n\r\n    plot_results_multiple(predictions, y_test, configs['data']['sequence_length'])\r\n    \r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u6570\u636e\u9884\u5904\u7406\u6a21\u5757"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import math\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nclass DataLoader():\r\n    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\r\n\r\n    def __init__(self, filename, split, cols):\r\n        dataframe = pd.read_csv(filename)\r\n        i_split = int(len(dataframe) * split)\r\n        self.data_train = dataframe.get(cols).values[:i_split]\r\n        self.data_test  = dataframe.get(cols).values[i_split:]\r\n        self.len_train  = len(self.data_train)\r\n        self.len_test   = len(self.data_test)\r\n        self.len_train_windows = None\r\n\r\n    def get_test_data(self, seq_len, normalise):\r\n        '''\r\n        Create x, y test data windows\r\n        Warning: batch method, not generative, make sure you have enough memory to\r\n        load data, otherwise reduce size of the training split.\r\n        '''\r\n        data_windows = []\r\n        for i in range(self.len_test - seq_len):\r\n            data_windows.append(self.data_test[i:i+seq_len])\r\n\r\n        data_windows = np.array(data_windows).astype(float)\r\n        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\r\n\r\n        x = data_windows[:, :-1]\r\n        y = data_windows[:, -1, [0]]\r\n        return x,y\r\n\r\n    def get_train_data(self, seq_len, normalise):\r\n        '''\r\n        Create x, y train data windows\r\n        Warning: batch method, not generative, make sure you have enough memory to\r\n        load data, otherwise use generate_training_window() method.\r\n        '''\r\n        data_x = []\r\n        data_y = []\r\n        for i in range(self.len_train - seq_len):\r\n            x, y = self._next_window(i, seq_len, normalise)\r\n            data_x.append(x)\r\n            data_y.append(y)\r\n        return np.array(data_x), np.array(data_y)\r\n\r\n    def generate_train_batch(self, seq_len, batch_size, normalise):\r\n        '''Yield a generator of training data from filename on given list of cols split for train/test'''\r\n        i = 0\r\n        while i < (self.len_train - seq_len):\r\n            x_batch = []\r\n            y_batch = []\r\n            for b in range(batch_size):\r\n                if i >= (self.len_train - seq_len):\r\n                    # stop-condition for a smaller final batch if data doesn't divide evenly\r\n                    yield np.array(x_batch), np.array(y_batch)\r\n                    i = 0\r\n                x, y = self._next_window(i, seq_len, normalise)\r\n                x_batch.append(x)\r\n                y_batch.append(y)\r\n                i += 1\r\n            yield np.array(x_batch), np.array(y_batch)\r\n\r\n    def _next_window(self, i, seq_len, normalise):\r\n        '''Generates the next data window from the given index location i'''\r\n        window = self.data_train[i:i+seq_len]\r\n        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\r\n        x = window[:-1]\r\n        y = window[-1, [0]]\r\n        return x, y\r\n\r\n    def normalise_windows(self, window_data, single_window=False):\r\n        '''Normalise window with a base value of zero'''\r\n        normalised_data = []\r\n        window_data = [window_data] if single_window else window_data\r\n        for window in window_data:\r\n            normalised_window = []\r\n            for col_i in range(window.shape[1]):\r\n                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\r\n                normalised_window.append(normalised_col)\r\n            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\r\n            normalised_data.append(normalised_window)\r\n        return np.array(normalised_data)\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"\u6a21\u578b"})}),"\n",(0,a.jsxs)(e.p,{children:["\u8fd9\u6bb5\u4ee3\u7801\u5b9a\u4e49\u4e86\u4e00\u4e2a\u540d\u4e3a ",(0,a.jsx)(e.code,{children:"Model"})," \u7684\u7c7b\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u52a0\u8f7d\u57fa\u4e8e\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002"]}),"\n",(0,a.jsx)(e.p,{children:"\u6784\u5efa\u6a21\u578b\u7684\u6b65\u9aa4\uff1a"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"\u521d\u59cb\u5316\u8ba1\u65f6\u5668"}),"\uff1a\u4f7f\u7528 ",(0,a.jsx)(e.code,{children:"Timer"})," \u5f00\u59cb\u8ba1\u65f6\u3002"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"\u904d\u5386\u914d\u7f6e"}),"\uff1a\u904d\u5386 ",(0,a.jsx)(e.code,{children:"configs"})," \u5b57\u5178\u4e2d\u7684 ",(0,a.jsx)(e.code,{children:"layers"})," \u5217\u8868\uff0c\u6bcf\u4e2a\u5217\u8868\u9879\u4ee3\u8868\u4e00\u5c42\u3002"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"\u6dfb\u52a0\u5c42\uff1a"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["\u5982\u679c\u5c42\u7c7b\u578b\u4e3a ",(0,a.jsx)(e.code,{children:"'dense'"}),"\uff0c\u5219\u6dfb\u52a0\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff08",(0,a.jsx)(e.code,{children:"Dense"}),"\uff09\u3002"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["\u5982\u679c\u5c42\u7c7b\u578b\u4e3a ",(0,a.jsx)(e.code,{children:"'lstm'"}),"\uff0c\u5219\u6dfb\u52a0\u4e00\u4e2a LSTM \u5c42\uff0c\u5e76\u8bbe\u7f6e\u8f93\u5165\u65f6\u95f4\u6b65\u548c\u7ef4\u5ea6\u3002"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["\u5982\u679c\u5c42\u7c7b\u578b\u4e3a ",(0,a.jsx)(e.code,{children:"'dropout'"}),"\uff0c\u5219\u6dfb\u52a0\u4e00\u4e2a Dropout \u5c42\u3002"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.code,{children:"load_model"})," \u65b9\u6cd5\uff1a"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"\u8fd9\u4e2a\u65b9\u6cd5\u7528\u4e8e\u4ece\u6587\u4ef6\u8def\u5f84\u52a0\u8f7d\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002"}),"\n",(0,a.jsxs)(e.li,{children:["\u5b83\u63a5\u6536\u4e00\u4e2a\u53c2\u6570 ",(0,a.jsx)(e.code,{children:"filepath"}),"\uff0c\u8868\u793a\u6a21\u578b\u6587\u4ef6\u7684\u8def\u5f84\u3002"]}),"\n",(0,a.jsxs)(e.li,{children:["\u4f7f\u7528 ",(0,a.jsx)(e.code,{children:"load_model"})," \u51fd\u6570\uff08\u9700\u8981\u4ece ",(0,a.jsx)(e.code,{children:"tensorflow.keras.models"})," \u5bfc\u5165\uff09\u6765\u52a0\u8f7d\u6a21\u578b\u3002"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.code,{children:"build_model"})," \u65b9\u6cd5\uff1a"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"\u8fd9\u4e2a\u65b9\u6cd5\u7528\u4e8e\u6839\u636e\u914d\u7f6e\u6784\u5efa\u6a21\u578b\u3002"}),"\n",(0,a.jsxs)(e.li,{children:["\u5b83\u63a5\u6536\u4e00\u4e2a\u53c2\u6570 ",(0,a.jsx)(e.code,{children:"configs"}),"\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u6a21\u578b\u914d\u7f6e\u7684\u5b57\u5178\u3002"]}),"\n",(0,a.jsxs)(e.li,{children:["\u65b9\u6cd5\u5185\u90e8\u4f7f\u7528\u4e86\u4e00\u4e2a ",(0,a.jsx)(e.code,{children:"Timer"})," \u7c7b\uff08\u9700\u8981\u4ece ",(0,a.jsx)(e.code,{children:"time"})," \u6a21\u5757\u5bfc\u5165\uff09\u6765\u6d4b\u91cf\u6a21\u578b\u6784\u5efa\u7684\u65f6\u95f4\u3002"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import os\r\nimport math\r\nimport numpy as np\r\nimport datetime as dt\r\nfrom numpy import newaxis\r\nfrom core.utils import Timer\r\nfrom keras.layers import Dense, Activation, Dropout, LSTM\r\nfrom keras.models import Sequential, load_model\r\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\r\n\r\nclass Model():\r\n\t\"\"\"LSTM \u6a21\u578b\"\"\"\r\n\r\n\tdef __init__(self):\r\n\t\tself.model = Sequential()\r\n\r\n\tdef load_model(self, filepath):\r\n\t\tprint('[Model] Loading model from file %s' % filepath)\r\n\t\tself.model = load_model(filepath)\r\n\r\n\tdef build_model(self, configs):\r\n\t\ttimer = Timer()\r\n\t\ttimer.start()\r\n\r\n\t\tfor layer in configs['model']['layers']:\r\n\t\t\tneurons = layer['neurons'] if 'neurons' in layer else None\r\n\t\t\tdropout_rate = layer['rate'] if 'rate' in layer else None\r\n\t\t\tactivation = layer['activation'] if 'activation' in layer else None\r\n\t\t\treturn_seq = layer['return_seq'] if 'return_seq' in layer else None\r\n\t\t\tinput_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\r\n\t\t\tinput_dim = layer['input_dim'] if 'input_dim' in layer else None\r\n\r\n\t\t\tif layer['type'] == 'dense':\r\n\t\t\t\tself.model.add(Dense(neurons, activation=activation))\r\n\t\t\tif layer['type'] == 'lstm':\r\n\t\t\t\tself.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\r\n\t\t\tif layer['type'] == 'dropout':\r\n\t\t\t\tself.model.add(Dropout(dropout_rate))\r\n\r\n\t\tself.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\r\n\r\n\t\tprint('[Model] Model Compiled')\r\n\t\ttimer.stop()\r\n\t\t\r\n\t\treturn self.model\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u8bad\u7ec3\u8fc7\u7a0b"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"\tdef train(self, x, y, epochs, batch_size, save_dir):\r\n\t\ttimer = Timer()\r\n\t\ttimer.start()\r\n\t\tprint('[Model] Training Started')\r\n\t\tprint('[Model] %s epochs, %s batch size' % (epochs, batch_size))\r\n\t\t\r\n\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\r\n\t\tcallbacks = [\r\n\t\t\tEarlyStopping(monitor='val_loss', patience=2),\r\n\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\r\n\t\t]\r\n\t\tself.model.fit(\r\n\t\t\tx,\r\n\t\t\ty,\r\n\t\t\tepochs=epochs,\r\n\t\t\tbatch_size=batch_size,\r\n\t\t\tcallbacks=callbacks\r\n\t\t)\r\n\t\tself.model.save(save_fname)\r\n\r\n\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\r\n\t\ttimer.stop()\r\n\r\n\tdef train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir):\r\n\t\ttimer = Timer()\r\n\t\ttimer.start()\r\n\t\tprint('[Model] Training Started')\r\n\t\tprint('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\r\n\t\t\r\n\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\r\n\t\tcallbacks = [\r\n\t\t\tModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\r\n\t\t]\r\n\t\tself.model.fit_generator(\r\n\t\t\tdata_gen,\r\n\t\t\tsteps_per_epoch=steps_per_epoch,\r\n\t\t\tepochs=epochs,\r\n\t\t\tcallbacks=callbacks,\r\n\t\t\tworkers=1\r\n\t\t)\r\n\t\t\r\n\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\r\n\t\ttimer.stop()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u9884\u6d4b\u8fc7\u7a0b\uff0c\u70b9\u9884\u6d4b\u548c\u5168\u5e8f\u5217\u9884\u6d4b"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"def predict_point_by_point(self, data):\r\n\t\tprint('[Model] Predicting Point-by-Point...')\r\n\t\tpredicted = self.model.predict(data)\r\n\t\tpredicted = np.reshape(predicted, (predicted.size,))\r\n\t\treturn predicted\r\n\r\ndef predict_sequences_multiple(self, data, window_size, prediction_len,debug=False):\r\n    if debug == False:\r\n        print('[Model] Predicting Sequences Multiple...')\r\n        prediction_seqs = []\r\n        for i in range(int(len(data)/prediction_len)):\r\n            curr_frame = data[i*prediction_len]\r\n            predicted = []\r\n            for j in range(prediction_len):\r\n                predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\r\n                curr_frame = curr_frame[1:]\r\n                curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\r\n            prediction_seqs.append(predicted)\r\n        return prediction_seqs\r\n    else :\r\n        print('[Model] Predicting Sequences Multiple...')\r\n        prediction_seqs = []\r\n        for i in range(int(len(data)/prediction_len)):\r\n            print (data.shape)\r\n            curr_frame = data[i*prediction_len]\r\n            print (curr_frame)\r\n            predicted = []\r\n            for j in range(prediction_len):\r\n                predict_result = self.model.predict(curr_frame[newaxis,:,:])\r\n                print (predict_result)\r\n                final_result = predict_result[0,0]\r\n                predicted.append(final_result)\r\n                curr_frame = curr_frame[1:]\r\n                print (curr_frame)\r\n                curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\r\n                print (curr_frame)\r\n            prediction_seqs.append(predicted)\r\n\r\n\r\ndef predict_sequence_full(self, data, window_size):\r\n    print('[Model] Predicting Sequences Full...')\r\n    curr_frame = data[0]\r\n    predicted = []\r\n    for i in range(len(data)):\r\n        predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\r\n        curr_frame = curr_frame[1:]\r\n        curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\r\n    return predicted\r\n\n"})}),"\n",(0,a.jsx)(e.p,{children:"class Timer"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import datetime as dt\r\n\r\nclass Timer():\r\n\r\n\tdef __init__(self):\r\n\t\tself.start_dt = None\r\n\r\n\tdef start(self):\r\n\t\tself.start_dt = dt.datetime.now()\r\n\r\n\tdef stop(self):\r\n\t\tend_dt = dt.datetime.now()\r\n\t\tprint('Time taken: %s' % (end_dt - self.start_dt))\n"})}),"\n",(0,a.jsx)(e.h4,{id:"XH",children:"\u5f90\u534e"}),"\n",(0,a.jsx)(e.p,{children:"\u9884\u5904\u7406\u6b65\u9aa4"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import pandas as pd\r\ndf = pd.read_csv('/private/\u6570\u636e/2019-2020sydatahb.csv')\r\ndf\r\n\r\ndata = df\r\n\r\nimport matplotlib.pyplot as plt\r\nseries = data.set_index(['date'], drop=True)\r\nplt.figure(figsize=(10, 6))\r\nseries['\u6c88\u9633\u8fc7\u53bb1\u5c0f\u65f6\u964d\u6c34\u91cf(\u6beb\u7c73)'].plot()\r\nplt.show()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u6570\u636e\u5dee\u5206\u8f6c\u6362\u4e0e\u5f52\u4e00\u5316"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dense\r\nfrom keras.models import Sequential\r\nfrom sklearn.metrics import mean_squared_error\r\nimport numpy as np\r\nimport math\r\n \r\n# \u6570\u636e\u7684\u5dee\u5206\u8f6c\u6362\r\ndef difference(data_set,interval=1):\r\n    diff=list()\r\n    for i in range(interval,len(data_set)):\r\n        value=data_set[i]-data_set[i-interval]\r\n        diff.append(value)\r\n    return pd.Series(diff)\r\n \r\n# \u5bf9\u9884\u6d4b\u7684\u6570\u636e\u8fdb\u884c\u9006\u5dee\u5206\u8f6c\u6362\r\ndef invert_difference(history,yhat,interval=1):\r\n    return yhat+history[-interval]\r\n \r\n# \u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u76d1\u7763\u5b66\u4e60\u96c6\uff0c\u79fb\u4f4d\u540e\u4ea7\u751f\u7684NaN\u503c\u88650\r\ndef timeseries_to_supervised(data,lag=1):\r\n    df=pd.DataFrame(data)\r\n    columns=[df.shift(i) for i in range(1,lag+1)]\r\n    columns.append(df)\r\n    df=pd.concat(columns,axis=1)\r\n    df.fillna(0,inplace=True)\r\n    return df\r\n \r\n# \u5c06\u6570\u636e\u7f29\u653e\u5230[-1,1]\u4e4b\u95f4\r\ndef scale(train,test):\r\n    # \u521b\u5efa\u4e00\u4e2a\u7f29\u653e\u5668\uff0c\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u7f29\u653e\u5230[-1,1]\u7684\u53d6\u503c\u8303\u56f4\u4e2d\r\n    scaler=MinMaxScaler(feature_range=(-1,1))\r\n    # \u4f7f\u7528\u6570\u636e\u6765\u8bad\u7ec3\u7f29\u653e\u5668\r\n    scaler=scaler.fit(train)\r\n    # \u4f7f\u7528\u7f29\u653e\u5668\u6765\u5c06\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u8fdb\u884c\u7f29\u653e\r\n    train_scaled=scaler.transform(train)\r\n    test_scaled=scaler.transform(test)\r\n    return scaler,train_scaled,test_scaled\r\n \r\n# \u5c06\u9884\u6d4b\u503c\u8fdb\u884c\u9006\u7f29\u653e\uff0c\u4f7f\u7528\u4e4b\u524d\u8bad\u7ec3\u597d\u7684\u7f29\u653e\u5668\uff0cx\u4e3a\u4e00\u7ef4\u6570\u7ec4\uff0cy\u4e3a\u5b9e\u6570\r\ndef invert_scale(scaler,X,y):\r\n    # \u5c06X,y\u8f6c\u6362\u4e3a\u4e00\u4e2alist\u5217\u8868\r\n    new_row=[x for x in X]+[y]\r\n    # \u5c06\u5217\u8868\u8f6c\u6362\u4e3a\u6570\u7ec4\r\n    array=np.array(new_row)\r\n    # \u5c06\u6570\u7ec4\u91cd\u6784\u6210\u4e00\u4e2a\u5f62\u72b6\u4e3a[1,2]\u7684\u4e8c\u7ef4\u6570\u7ec4->[[10,12]]\r\n    array=array.reshape(1,len(array))\r\n    # \u9006\u7f29\u653e\u8f93\u5165\u7684\u5f62\u72b6\u4e3a[1,2]\uff0c\u8f93\u51fa\u5f62\u72b6\u4e5f\u662f\u5982\u6b64\r\n    invert=scaler.inverse_transform(array)\r\n    # \u53ea\u9700\u8981\u8fd4\u56dey\u503c\u5373\u53ef\r\n    return invert[0,-1]\n"})}),"\n",(0,a.jsx)(e.h4,{id:"LSTM",children:"LSTM\u6a21\u578b"}),"\n",(0,a.jsx)(e.p,{children:"\u521b\u5efa\u4e0e\u8bad\u7ec3"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import numpy as np\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import LSTM, Dense\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import mean_squared_error\r\nimport matplotlib.pyplot as plt\r\n\r\n# \u52a0\u8f7d\u6570\u636e\r\ndf = pd.read_csv('/private/\u6570\u636e/2019-2020sydatahb.csv')  # \u8bf7\u66ff\u6362\u4e3a\u4f60\u7684\u6587\u4ef6\u8def\u5f84\r\ndata = df['\u6c88\u9633\u8fc7\u53bb1\u5c0f\u65f6\u964d\u6c34\u91cf(\u6beb\u7c73)'].values.reshape(-1, 1)\r\n\r\n# \u6570\u636e\u7f29\u653e\u5230[-1, 1]\r\nscaler = MinMaxScaler(feature_range=(-1, 1))\r\ndata_scaled = scaler.fit_transform(data)\r\n\r\n# \u5c06\u5e8f\u5217\u8f6c\u6362\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\r\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\r\n    n_vars = 1 if type(data) is list else data.shape[1]\r\n    df = pd.DataFrame(data)\r\n    cols, names = list(), list()\r\n    # \u8f93\u5165\u5e8f\u5217 (t-n, ... t-1)\r\n    for i in range(n_in, 0, -1):\r\n        cols.append(df.shift(i))\r\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\r\n    # \u9884\u6d4b\u5e8f\u5217 (t, t+1, ... t+n)\r\n    for i in range(0, n_out):\r\n        cols.append(df.shift(-i))\r\n        if i == 0:\r\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\r\n        else:\r\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\r\n    # \u5c06\u6240\u6709\u7684\u62fc\u63a5\u5728\u4e00\u8d77\r\n    agg = pd.concat(cols, axis=1)\r\n    agg.columns = names\r\n    # \u4e22\u5f03\u542b\u6709NaN\u503c\u7684\u884c\r\n    if dropnan:\r\n        agg.dropna(inplace=True)\r\n    return agg\r\n\r\nn_hours = 64  # \u4f7f\u75283\u5c0f\u65f6\u7684\u6570\u636e\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5c0f\u65f6\r\nn_features = 1  # \u7279\u5f81\u6570\u91cf\uff0c\u8fd9\u91cc\u53ea\u6709\u964d\u6c34\u91cf\u4e00\u4e2a\u7279\u5f81\r\nreframed = series_to_supervised(data_scaled, n_hours, 1)\r\n\r\n# \u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\r\nvalues = reframed.values\r\nn_train_hours = int(len(values) * 0.8)\r\ntrain = values[:n_train_hours, :]\r\ntest = values[n_train_hours:, :]\r\n# \u5206\u5272\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\r\ntrain_X, train_y = train[:, :-1], train[:, -1]\r\ntest_X, test_y = test[:, :-1], test[:, -1]\r\n# \u91cd\u5851\u62103D\u5f62\u72b6 [\u6837\u672c, \u65f6\u95f4\u6b65, \u7279\u5f81]\r\ntrain_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\r\ntest_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\r\n\r\n# \u6784\u5efaLSTM\u6a21\u578b\r\nmodel = Sequential()\r\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mae', optimizer='adam')\r\n\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"# \u6784\u5efa\u4e00\u4e2aLSTM\u6a21\u578b\r\ndef fit_lstm(train,batch_size,nb_epoch,neurons):\r\n    # \u5c06\u6570\u636e\u5bf9\u4e2d\u7684x\u548cy\u5206\u5f00\r\n    X,y=train[:,0:-1],train[:,-1]\r\n    # \u5c062D\u6570\u636e\u62fc\u63a5\u62103D\u6570\u636e\uff0c\u5f62\u72b6\u4e3a[N*1*1]\r\n    X=X.reshape(X.shape[0],1,X.shape[1])\r\n \r\n    model=Sequential()\r\n    model.add(LSTM(neurons,batch_input_shape=(batch_size,X.shape[1],X.shape[2]),stateful=True))\r\n    model.add(Dense(1))\r\n \r\n    model.compile(loss='mean_squared_error',optimizer='adam')\r\n    for i in range(nb_epoch):\r\n        # shuffle\u662f\u4e0d\u6df7\u6dc6\u6570\u636e\u987a\u5e8f\r\n        his=model.fit(X,y,batch_size=batch_size,verbose=1,shuffle=False)\r\n        # \u6bcf\u8bad\u7ec3\u5b8c\u4e00\u6b21\u5c31\u91cd\u7f6e\u4e00\u6b21\u7f51\u7edc\u72b6\u6001\uff0c\u7f51\u7edc\u72b6\u6001\u4e0e\u7f51\u7edc\u6743\u91cd\u4e0d\u540c\r\n        model.reset_states()\r\n    return model\r\n \r\n# \u5f00\u59cb\u5355\u6b65\u9884\u6d4b\r\nimport tensorflow as tf  # \u786e\u4fdd\u5bfc\u5165tensorflow\r\n\r\ndef forecast_lstm(model, batch_size, X):\r\n    # \u786e\u4fddX\u662f\u4e00\u4e2aTensor\r\n    X = tf.convert_to_tensor(X, dtype=tf.float32)  # \u6dfb\u52a0\u8fd9\u884c\u4ee3\u7801\u6765\u8f6c\u6362X\r\n    X = tf.reshape(1, 1, len(X))\r\n    yhat = model.predict(X, batch_size=batch_size)\r\n    return yhat[0,0]\r\n\r\n \r\n# \u8bfb\u53d6\u6570\u636e\uff0c\u5c06\u65e5\u671f\u548c\u65f6\u95f4\u5217\u5408\u5e76\uff0c\u5176\u4ed6\u5217\u5220\u9664\uff0c\u5408\u5e76\u540e\u7684\u5217\u8f6c\u6362\u4e3a\u65f6\u95f4\u683c\u5f0f\uff0c\u8bbe\u4e3a\u7d22\u5f15\r\ndata['date']=pd.to_datetime(data['date'])\r\nseries=data.set_index(['date'],drop=True)\r\n \r\n# \u5c06\u539f\u6570\u636e\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u6570\u7ec4\u5f62\u5f0f\uff0c\u4f8b\u5982\uff1a\r\n# [[4.6838],[4.6882],[4.7048]]\r\nraw_value=series.values\r\n# \u5c06\u6570\u636e\u8fdb\u884c\u5dee\u5206\u8f6c\u6362\uff0c\u4f8b\u5982[[4.6838],[4.6882],[4.7048]]\u8f6c\u6362\u4e3a[[4.6882-4.6838],[4.7048-4.6882]]\r\ndiff_value=difference(raw_value,1)\r\n#\r\n# \u5c06\u5e8f\u5217\u5f62\u5f0f\u7684\u6570\u636e\u8f6c\u6362\u4e3a\u76d1\u7763\u5b66\u4e60\u96c6\u5f62\u5f0f\uff0c\u4f8b\u5982[[10],[11],[12],[13]]\r\n# \u5728\u6b64\u5c06\u5176\u8f6c\u6362\u4e3a\u76d1\u7763\u5b66\u4e60\u96c6\u5f62\u5f0f\uff1a[[0,10],[10,11],[11,12],[12,13]]\uff0c\r\n# \u5373\u524d\u4e00\u4e2a\u6570\u4f5c\u4e3a\u8f93\u5165\uff0c\u540e\u4e00\u4e2a\u6570\u4f5c\u4e3a\u5bf9\u5e94\u7684\u8f93\u51fa\r\nsupervised=timeseries_to_supervised(diff_value,1)\r\nsupervised_value=supervised.values\r\n \r\n# \u5c06\u6570\u636e\u96c6\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bbe\u7f6e\u540e1000\u4e2a\u6570\u636e\u4e3a\u6d4b\u8bd5\u96c6\r\ntestNum=3000\r\ntrain,test=supervised_value[:-testNum],supervised_value[-testNum:]\r\n \r\n# \u5c06\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u90fd\u7f29\u653e\u5230[-1,1]\u4e4b\u95f4\r\nscaler,train_scaled,test_scaled=scale(train,test)\r\n \r\n# \u6784\u5efa\u4e00\u4e2aLSTM\u6a21\u578b\u5e76\u8bad\u7ec3\uff0c\u6837\u672c\u6570\u4e3a1\uff0c\u8bad\u7ec3\u6b21\u6570\u4e3a5\uff0cLSTM\u5c42\u795e\u7ecf\u5143\u4e2a\u6570\u4e3a4\r\nlstm_model=fit_lstm(train_scaled,1,1,4)\r\n# \u904d\u5386\u6d4b\u8bd5\u96c6\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u5355\u6b65\u9884\u6d4b\r\npredictions=list()\r\nfor i in range(len(test_scaled)):\r\n    # \u5c06\u6d4b\u8bd5\u96c6\u62c6\u5206\u4e3aX\u548cy\r\n    X,y=test[i,0:-1],test[i,-1]\r\n    # \u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3001\u6d4b\u8bd5\u6570\u636e\u4f20\u5165\u9884\u6d4b\u51fd\u6570\u4e2d\r\n    yhat=forecast_lstm(lstm_model,1,X)\r\n    # \u5c06\u9884\u6d4b\u503c\u8fdb\u884c\u9006\u7f29\u653e\r\n    yhat=invert_scale(scaler,X,yhat)\r\n    # \u5bf9\u9884\u6d4b\u7684y\u503c\u8fdb\u884c\u9006\u5dee\u5206\r\n    yhat=invert_difference(raw_value,yhat,len(test_scaled)+1-i)\r\n    # \u5b58\u50a8\u6b63\u5728\u9884\u6d4b\u7684y\u503c\r\n    predictions.append(yhat)\r\n \r\n# \u8ba1\u7b97\u65b9\u5dee\r\nrmse=mean_squared_error(raw_value[-testNum:],predictions)\r\nprint(\"Test RMSE:\",rmse)\r\nplt.plot(raw_value[-testNum:])\r\nplt.plot(predictions)\r\nplt.legend(['true','pred'])\r\nplt.show()\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"# \u6a21\u578b\u8bad\u7ec3\r\nhistory = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\r\n\r\n# \u7ed8\u5236\u5386\u53f2\u6570\u636e\r\nplt.plot(history.history['loss'], label='train')\r\nplt.plot(history.history['val_loss'], label='test')\r\nplt.legend()\r\nplt.show()\r\n\r\n# \u505a\u51fa\u9884\u6d4b\r\nyhat = model.predict(test_X)\r\ntest_X_reshaped = test_X.reshape((test_X.shape[0], n_hours*n_features))\r\n\r\n# \u9006\u7f29\u653e\u9884\u6d4b\u503c\r\ninv_yhat = np.concatenate((yhat, test_X_reshaped[:, -(n_features-1):]), axis=1)\r\ninv_yhat = scaler.inverse_transform(inv_yhat)\r\ninv_yhat = inv_yhat[:,0]\r\n\r\n# \u9006\u7f29\u653e\u771f\u5b9e\u503c\r\ntest_y_reshaped = test_y.reshape((len(test_y), 1))\r\ninv_y = np.concatenate((test_y_reshaped, test_X_reshaped[:, -(n_features-1):]), axis=1)\r\ninv_y = scaler.inverse_transform(inv_y)\r\ninv_y = inv_y[:,0]\r\n\r\n# \u8ba1\u7b97 RMSE\r\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\r\nprint('Test RMSE: %.3f' % rmse)\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u7ed8\u5236\u7ed3\u679c"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import matplotlib.pyplot as plt\r\n\r\n# \u7ed8\u5236\u5b9e\u9645\u964d\u6c34\u91cf\u4e0e\u9884\u6d4b\u964d\u6c34\u91cf\u7684\u5bf9\u6bd4\u56fe\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(inv_y, label='Actual', color='blue')  # \u5b9e\u9645\u964d\u6c34\u91cf\r\nplt.plot(inv_yhat, label='Predicted', color='red', linestyle='--')  # \u9884\u6d4b\u964d\u6c34\u91cf\r\nplt.title('LSTM Model Precipitation Prediction vs Actual')\r\nplt.xlabel('Time')\r\nplt.ylabel('Precipitation Amount (mm)')\r\nplt.legend()\r\nplt.show()\n"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"from math import sqrt\r\nfrom numpy import concatenate\r\nfrom matplotlib import pyplot\r\nfrom pandas import read_csv\r\nfrom pandas import DataFrame\r\nfrom pandas import concat\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nimport numpy as np\r\n\r\n\r\n#\u8f6c\u6210\u6709\u76d1\u7763\u6570\u636e\r\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\r\n    n_vars = 1 if type(data) is list else data.shape[1]\r\n    df = DataFrame(data)\r\n    cols, names = list(), list()\r\n    #\u6570\u636e\u5e8f\u5217(\u4e5f\u5c06\u5c31\u662finput) input sequence (t-n, ... t-1)\r\n    for i in range(n_in, 0, -1):\r\n        cols.append(df.shift(i))\r\n        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\r\n        #\u9884\u6d4b\u6570\u636e\uff08input\u5bf9\u5e94\u7684\u8f93\u51fa\u503c\uff09 forecast sequence (t, t+1, ... t+n)\r\n    for i in range(0, n_out):\r\n        cols.append(df.shift(-i))\r\n        if i == 0:\r\n            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\r\n        else:\r\n            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\r\n    #\u62fc\u63a5 put it all together\r\n    agg = concat(cols, axis=1)\r\n    agg.columns = names\r\n    # \u5220\u9664\u503c\u4e3aNAN\u7684\u884c drop rows with NaN values\r\n    if dropnan:\r\n        agg.dropna(inplace=True)\r\n    return agg\r\n\r\n\r\n##\u6570\u636e\u9884\u5904\u7406 load dataset\r\ndataset = read_csv('data_set/pollution.csv', header=0, index_col=0)\r\nvalues = dataset.values\r\n#\u6807\u7b7e\u7f16\u7801 integer encode direction\r\nencoder = LabelEncoder()\r\nvalues[:, 4] = encoder.fit_transform(values[:, 4])\r\n#\u4fdd\u8bc1\u4e3afloat ensure all data is float\r\nvalues = values.astype('float32')\r\n#\u5f52\u4e00\u5316 normalize features\r\nscaler = MinMaxScaler(feature_range=(0, 1))\r\nscaled = scaler.fit_transform(values)\r\n#\u8f6c\u6210\u6709\u76d1\u7763\u6570\u636e frame as supervised learning\r\nreframed = series_to_supervised(scaled, 1, 1)\r\n#\u5220\u9664\u4e0d\u9884\u6d4b\u7684\u5217 drop columns we don't want to predict\r\nreframed.drop(reframed.columns[[9, 10, 11, 12, 13, 14, 15]], axis=1, inplace=True)\r\nprint(reframed.head())\r\n\r\n#\u6570\u636e\u51c6\u5907\r\n#\u628a\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e split into train and test sets\r\nvalues = reframed.values\r\n#\u62ff\u4e00\u5e74\u7684\u65f6\u95f4\u957f\u5ea6\u8bad\u7ec3\r\nn_train_hours = 365 * 24\r\n#\u5212\u5206\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\r\ntrain = values[:n_train_hours, :]\r\ntest = values[n_train_hours:, :]\r\n#\u62c6\u5206\u8f93\u5165\u8f93\u51fa split into input and outputs\r\ntrain_X, train_y = train[:, :-1], train[:, -1]\r\ntest_X, test_y = test[:, :-1], test[:, -1]\r\n#reshape\u8f93\u5165\u4e3aLSTM\u7684\u8f93\u5165\u683c\u5f0f reshape input to be 3D [samples, timesteps, features]\r\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\r\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\r\nprint ('train_x.shape, train_y.shape, test_x.shape, test_y.shape')\r\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\r\n\r\n##\u6a21\u578b\u5b9a\u4e49 design network\r\nmodel = Sequential()\r\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mae', optimizer='adam')\r\n#\u6a21\u578b\u8bad\u7ec3 fit network\r\nhistory = model.fit(train_X, train_y, epochs=50, batch_size=36, validation_data=(test_X, test_y), verbose=2,\r\n                    shuffle=False)\r\n#\u8f93\u51fa plot history\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n#\u8fdb\u884c\u9884\u6d4b make a prediction\r\nyhat = model.predict(test_X)\r\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\r\n#\u9884\u6d4b\u6570\u636e\u9006\u7f29\u653e invert scaling for forecast\r\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\r\ninv_yhat = scaler.inverse_transform(inv_yhat)\r\ninv_yhat = inv_yhat[:, 0]\r\ninv_yhat = np.array(inv_yhat)\r\n#\u771f\u5b9e\u6570\u636e\u9006\u7f29\u653e invert scaling for actual\r\ntest_y = test_y.reshape((len(test_y), 1))\r\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\r\ninv_y = scaler.inverse_transform(inv_y)\r\ninv_y = inv_y[:, 0]\r\n\r\n#\u753b\u51fa\u771f\u5b9e\u6570\u636e\u548c\u9884\u6d4b\u6570\u636e\r\npyplot.plot(inv_yhat,label='prediction')\r\npyplot.plot(inv_y,label='true')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n# calculate RMSE\r\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\r\nprint('Test RMSE: %.3f' % rmse)\n"})}),"\n",(0,a.jsx)(e.h4,{id:"CEEMDAN",children:"CEEMDAN"}),"\n",(0,a.jsx)(e.p,{children:"\u9884\u5904\u7406\u9636\u6bb5"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.neighbors import KNeighborsRegressor\r\nfrom sklearn.ensemble import AdaBoostRegressor\r\nfrom sklearn.tree import DecisionTreeRegressor\r\n\r\nfrom tensorflow.keras import Sequential,layers,losses,utils,Input\r\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout,concatenate,Flatten, Conv1D, MaxPooling1D,Activation,RepeatVector,TimeDistributed\r\nimport tensorflow as tf\r\n# from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\r\nfrom tensorflow.keras.models import Model\r\n# from tensorflow.keras.layers import Bidirectional,GRU,Lambda,Dot,Concatenate\r\nfrom sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error,mean_absolute_percentage_error\r\nfrom keras.layers import Bidirectional,LSTM,GRU,Lambda,Dot,Concatenate\r\n\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib\r\n\r\nfrom sklearn.decomposition import PCA\r\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\r\nfrom keras import optimizers\r\nfrom keras.models import load_model\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\ndata=pd.read_csv('/private/\u6570\u636e/2019-2022sydatag2.csv')\r\n\r\ncol=[ 'date', 'Precipitation', 'Temperature', 'Humidity',\r\n       'Barometric pressure', 'wind speed', 'visibility']\r\ndatacol=data[col]\r\ndatacol.isnull().sum()\r\n# \u76f4\u63a5\u5220\u9664\u7f3a\u5931\u503c\r\ndatashan=datacol.dropna()\r\nfeatures = ['Precipitation']\r\ntarget = ['Precipitation']\r\nX = data[features].values\r\ny = data[target].values\r\ntest_split=round(len(X)*0.20)\r\ndf_for_training=X[:-7008]\r\ndf_for_testing=X[-7008:]\r\n\r\ndef createXY(dataset,n_past):\r\n  dataX = []\r\n  dataY = []\r\n  for i in range(n_past, len(dataset)):\r\n          dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\r\n          dataY.append(dataset[i,0])\r\n  return np.array(dataX),np.array(dataY)\r\n\r\ntrainX,trainY=createXY(df_for_training_scaled,48)\r\ntestX,testY=createXY(df_for_testing_scaled,48)\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u4fee\u6539\u540e\u7684LSTM"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"from keras.layers import LSTM, Dense, Dropout\r\n\r\nfrom keras.layers import LSTM, Dense, Dropout\r\n\r\n# \u5b9a\u4e49\u4fee\u6539\u540e\u7684LSTM\u6a21\u578b\r\nmodel = Sequential()\r\nmodel.add(LSTM(100, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(100))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mse', optimizer='adam')\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(trainX, trainY, epochs=70, batch_size=16, validation_data=(testX, testY), verbose=2)\n"})}),"\n",(0,a.jsx)(e.p,{children:"CEEMDAN\u5206\u89e3"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"# \u5bfc\u5165\u76f8\u5e94\u7684\u5e93\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom datetime import datetime\r\nimport matplotlib.pyplot as plt\r\nimport datetime\r\nimport warnings\r\nimport scipy\r\nfrom scipy import stats\r\nimport statsmodels\r\nfrom PyEMD import CEEMDAN, Visualisation\r\nX1 = data['Precipitation']\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u5206\u89e3\u7ed3\u679c"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"#Decomposition results of RV for stock indices.\r\n\r\nceemdan = CEEMDAN()\r\nceemdan.ceemdan(X1.values.reshape(-1))\r\nimfs_close, res_close = ceemdan.get_imfs_and_residue()\r\n\r\nt = np.arange(0, len(X1), 1)\r\nvis = Visualisation()\r\nvis.plot_imfs(imfs=imfs_close, residue=res_close, t=t, include_residue=True)\r\n# vis.plot_instant_freq(t, imfs=imfs)\r\nvis.show()\r\ndecompose_data = pd.DataFrame(np.vstack((imfs_close, res_close)).T,columns = ['IMF%d'%(i+1) for i in range(len(imfs_close))] + ['Res'])\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u4f7f\u7528\u5206\u89e3\u7ed3\u679c\u8bad\u7ec3"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras import layers\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\nfrom keras.callbacks import EarlyStopping\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom sklearn.model_selection import train_test_split\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nimport numpy as np\r\nimport pandas as pd\r\nimport math\r\n# import tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, LSTM, GRU\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\r\n\r\n# \u5b9a\u4e49\u65f6\u95f4\u5e8f\u5217\u5212\u5206\u51fd\u6570\r\ndef TimeSeries(dataset, start_index, history_size, end_index, step,\r\n               target_size, point_time, true):\r\n    data = []  # \u4fdd\u5b58\u7279\u5f81\u6570\u636e\r\n    labels = []  # \u4fdd\u5b58\u7279\u5f81\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\u503c\r\n\r\n    start_index = start_index + history_size  # \u7b2c\u4e00\u6b21\u7684\u53d6\u503c\u8303\u56f4[0:start_index]\r\n\r\n    # \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u6ed1\u52a8\u7a97\u53e3\u53d6\u5230\u54ea\u4e2a\u7ed3\u675f\uff0c\u90a3\u5c31\u53d6\u5230\u6700\u540e\r\n    if end_index is None:\r\n        # \u6570\u636e\u96c6\u6700\u540e\u4e00\u5757\u662f\u7528\u6765\u4f5c\u4e3a\u6807\u7b7e\u503c\u7684\uff0c\u7279\u5f81\u4e0d\u80fd\u53d6\u5230\u5e95\r\n        end_index = len(dataset) - target_size\r\n\r\n    # \u6ed1\u52a8\u7a97\u53e3\u7684\u8d77\u59cb\u4f4d\u7f6e\u5230\u7ec8\u6b62\u4f4d\u7f6e\u6bcf\u6b21\u79fb\u52a8\u4e00\u6b65\r\n    for i in range(start_index, end_index):\r\n\r\n        index = range(i - history_size, i, step)  # \u7b2c\u4e00\u6b21\u76f8\u5f53\u4e8erange(0, start_index, 6)\r\n\r\n        # \u6839\u636e\u7d22\u5f15\u53d6\u51fa\u6240\u6709\u7684\u7279\u5f81\u6570\u636e\u7684\u6307\u5b9a\u884c\r\n        data.append(dataset.iloc[index])\r\n        # \u7528\u8fd9\u4e9b\u7279\u5f81\u6765\u9884\u6d4b\u67d0\u4e00\u4e2a\u65f6\u95f4\u70b9\u7684\u503c\u8fd8\u662f\u672a\u6765\u67d0\u4e00\u65f6\u95f4\u6bb5\u7684\u503c\r\n        if point_time is True:  # \u9884\u6d4b\u67d0\u4e00\u4e2a\u65f6\u95f4\u70b9\r\n            # \u9884\u6d4b\u672a\u6765\u54ea\u4e2a\u65f6\u95f4\u70b9\u7684\u6570\u636e\uff0c\u4f8b\u5982[0:20]\u7684\u7279\u5f81\u6570\u636e\uff0820\u53d6\u4e0d\u5230\uff09\uff0c\u6765\u9884\u6d4b\u7b2c20\u4e2a\u7684\u6807\u7b7e\u503c\r\n            labels.append(true[i + target_size])\r\n\r\n        else:  # \u9884\u6d4b\u672a\u6765\u67d0\u4e00\u65f6\u95f4\u533a\u95f4\r\n            # \u4f8b\u5982[0:20]\u7684\u7279\u5f81\u6570\u636e\uff0820\u53d6\u4e0d\u5230\uff09\uff0c\u6765\u9884\u6d4b[20,20+target_size]\u6570\u636e\u533a\u95f4\u7684\u6807\u7b7e\u503c\r\n            labels.append(true[i:i + target_size])\r\n\r\n    # \u8fd4\u56de\u5212\u5206\u597d\u4e86\u7684\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e\u503c\r\n    return np.array(data), np.array(labels)\r\n\r\n\r\n# \u6309\u71677:2:1\u5212\u5206\u8bad\u7ec3\u9a8c\u8bc1\u6d4b\u8bd5\u96c6\r\ndef get_tain_val_test(serie_data, window_size):\r\n    train_num = int(len(serie_data) * 0.7)\r\n    val_num = int(len(serie_data) * 0.9)  # \u53d62w-2.3w\u7684\u6570\u636e\u7528\u4e8e\u9a8c\u8bc1\r\n    history_size = window_size  # \u6bcf\u4e2a\u6ed1\u7a97\u53d65-26-272\u5929\u7684\u6570\u636e\u91cf(\u8868\u793a\u77ed\u671f\u4e2d\u671f\u957f\u671f\u9884\u6d4b)\r\n    target_size = 0  # \u9884\u6d4b\u672a\u6765\u4e0b\u4e00\u4e2a\u65f6\u95f4\u70b9\u7684\u6c14\u6e29\u503c\r\n    step = 1  # \u6b65\u957f\u4e3a1\u53d6\u6240\u6709\u7684\u884c\r\n\r\n    # \u6c42\u8bad\u7ec3\u96c6\u7684\u6bcf\u4e2a\u7279\u5f81\u5217\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\r\n    feat_mean = serie_data.mean(axis=0)\r\n    feat_std = serie_data.std(axis=0)\r\n\r\n    # \u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u8ba1\u7b97\u6807\u51c6\u5dee\r\n    feat = (serie_data - feat_mean) / feat_std\r\n\r\n    # \u6784\u9020\u8bad\u7ec3\u96c6\r\n    x_train, y_train = TimeSeries(dataset=serie_data, start_index=0, history_size=history_size, end_index=train_num,\r\n                                  step=step, target_size=target_size, point_time=True, true=serie_data)\r\n\r\n    # \u6784\u9020\u9a8c\u8bc1\u96c6\r\n    x_val, y_val = TimeSeries(dataset=serie_data, start_index=train_num, history_size=history_size, end_index=val_num,\r\n                              step=step, target_size=target_size, point_time=True, true=serie_data)\r\n\r\n    # \u6784\u9020\u6d4b\u8bd5\u96c6\r\n    x_test, y_test = TimeSeries(dataset=serie_data, start_index=val_num, history_size=history_size,\r\n                                end_index=len(serie_data),\r\n                                step=step, target_size=target_size, point_time=True, true=serie_data)\r\n    return x_train, y_train, x_val, y_val, x_test, y_test\r\n\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u5f00\u59cb\u8bad\u7ec3"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"from tensorflow.keras.optimizers import Adam\r\n\r\ny_pre_list = []\r\ny_test_list = []\r\nwindow_size = 12\r\n\r\nfor column in decompose_data.columns:\r\n    serie_data = decompose_data[column]\r\n    x_train, y_train, x_val, y_val, x_test, y_test = get_tain_val_test(serie_data, window_size)\r\n    \r\n    # \u5728\u6bcf\u6b21\u5faa\u73af\u5f00\u59cb\u65f6\u91cd\u65b0\u5b9e\u4f8b\u5316\u6a21\u578b\r\n    model = Sequential([\r\n        GRU(50, input_shape=(window_size, 1), return_sequences=False),\r\n        Dense(1)\r\n    ])\r\n    \r\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\r\n    \r\n    model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), batch_size=16, verbose=1)\r\n    \r\n    y_pre = model.predict(x_test)\r\n    y_pre_list.append(y_pre)\r\n    y_test_list.append(y_test)\r\n\r\n# \u8fd9\u91cc\u53ef\u4ee5\u6dfb\u52a0\u4ee3\u7801\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u6bd4\u5982\u8ba1\u7b97\u6bcf\u4e00\u5217\u7684MSE\u6216MAE\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u8bad\u7ec3\u7ed3\u679c"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"fig = plt.figure(figsize=(10,20))\r\nfor i,column in enumerate(decompose_data.columns):\r\n    \r\n    axes = fig.add_subplot(len(decompose_data.columns),1,i+1)\r\n    axes.plot(y_test_list[i], 'b-',linewidth = '1', label='actual')\r\n    axes.plot(y_pre_list[i], 'r-', linewidth = '1', label='predict')\r\n\r\n    plt.legend()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u9884\u6d4b\u6c42\u548c"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"#\u6b64\u65f6\u7684\u9884\u6d4b\u662f\u5bf9\u5168\u90e8\u5206\u89e3\u7ed3\u679c\u7684\u9884\u6d4b\u6c42\u548c\r\ny_pre_total = np.sum(np.array(y_pre_list),axis = 0).reshape(-1)\r\nx_train_all,y_train_all,x_val_all, y_val_all,x_test_all, y_test_all = get_tain_val_test(X1,window_size)\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u7ed8\u5236\u52a0\u548c\u7ed3\u679c"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"fig = plt.figure(figsize=(10,5))\r\naxes = fig.add_subplot(111)\r\naxes.plot(y_test_all, 'b-',linewidth = 1, label='actual')\r\naxes.plot(y_pre_total, 'r-', linewidth = 1, label='predict')\r\n\r\nplt.legend()\r\n#plt.grid()\r\nplt.show()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u4e00\u4e9b\u7ed3\u679c\u8bef\u5dee"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"def mean_absolute_error(y_test,y_pre):\r\n    mae = np.sum(np.absolute(y_pre-y_test))/len(y_test)\r\n    return mae\r\ndef mean_squared_error(y_test,y_pre):\r\n    mse = np.sum((y_pre-y_test)**2)/len(y_test)\r\n    return mse\r\ndef h_mean_absolute_error(y_test,y_pre):\r\n    hmae = mean_absolute_error(y_test,y_pre) / np.mean(y_pre)\r\n    return hmae\r\ndef h_mean_squared_error(y_test,y_pre):\r\n    hmse = mean_squared_error(y_test,y_pre) / np.mean(y_pre) ** 2\r\n    return hmse\r\nfrom sklearn.metrics import r2_score\r\nr2_train = r2_score(y_test_all, y_pre_total)\r\n\r\nprint(\"MAE:\", mean_absolute_error(y_test_all, y_pre_total))\r\nprint(\"MSE:\", mean_squared_error(y_test_all, y_pre_total))\r\nprint(\"HMAE:\", h_mean_absolute_error(y_test_all, y_pre_total))\r\nprint(\"HMSE:\", h_mean_squared_error(y_test_all, y_pre_total))\r\nprint(f'R^2: {r2_train}')\r\n# \u5c06\u9884\u6d4b\u7ed3\u679c\u4e2d\u5c0f\u4e8e0\u7684\u503c\u8bbe\u7f6e\u4e3a0\r\ny_pre_total[y_pre_total < 0] = 0\r\nfig = plt.figure(figsize=(10,5))\r\naxes = fig.add_subplot(111)\r\naxes.plot(y_test_all, 'b-',linewidth = 1, label='actual')\r\naxes.plot(y_pre_total, 'r-', linewidth = 1, label='predict')\r\n\r\nplt.legend()\r\n#plt.grid()\r\nplt.show()\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u76f8\u5173\u6027\u5206\u6790\u4ee3\u7801"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-py",children:"import pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\n# \u52a0\u8f7d\u6570\u636e\u96c6\r\ndata = pd.read_csv('/private/\u6570\u636e/2019-2020sydatahbg_E.csv')  # \u66ff\u6362\u4e3a\u4f60\u7684\u6570\u636e\u96c6\u8def\u5f84\r\n\r\n# \u8ba1\u7b97\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\r\ncorrelation_matrix = data.corr()\r\n\r\n# \u53ef\u89c6\u5316\u76f8\u5173\u6027\u77e9\u9635\r\nplt.figure(figsize=(10, 8))\r\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\r\nplt.title('Correlation Matrix of Weather Features')\r\nplt.show()\r\ndata = pd.read_csv('/private/\u6570\u636e/2019-2020sydatahbg.csv')  # \u66ff\u6362\u4e3a\u4f60\u7684\u6570\u636e\u96c6\u8def\u5f84\r\n\r\ncol=['\u6c88\u9633\u8fc7\u53bb1\u5c0f\u65f6\u964d\u6c34\u91cf(\u6beb\u7c73)', '\u6e29\u5ea6/\u6c14\u6e29(\u6444\u6c0f\u5ea6(\u2103))', '\u76f8\u5bf9\u6e7f\u5ea6(\u767e\u5206\u7387)', '\u6c14\u538b(\u767e\u5e15)',\r\n       '10\u5206\u949f\u5e73\u5747\u98ce\u901f(\u7c73/\u79d2)', '10\u5206\u949f\u5e73\u5747\u6c34\u5e73\u80fd\u89c1\u5ea6(m)']\r\ndatacol=data[col]\r\n# Since the data is already loaded and 'day' is set as index in the previous step, \r\n# we directly calculate the correlation matrix here.\r\n\r\n# Calculate the correlation matrix using Pearson's method\r\ncorrelation_matrix_updated = data.corr()\r\n\r\n# Display the correlation matrix\r\ncorrelation_matrix_updated\r\ncorrelation_matrix = data.corr()\r\ncorrelation_matrix\r\n\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport warnings\r\nimport numpy\r\nwarnings.filterwarnings(\"ignore\")\r\n# \u652f\u6301\u4e2d\u6587\r\nplt.rcParams['font.sans-serif'] = ['SimHei']  # \u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e\r\nplt.rcParams['axes.unicode_minus'] = False  # \u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7\r\nplt.rcParams['font.sans-serif'] = ['SimHei']\r\n\r\n# \u8bbe\u7f6e\u70ed\u56fe\u7684\u5c3a\u5bf8\r\nplt.figure(figsize=(10, 6))\r\n\r\n# \u7ed8\u5236\u70ed\u56fe\r\nsns.heatmap(correlation_matrix_updated, annot=True, cmap='coolwarm', fmt=\".2f\")\r\n\r\n# \u6dfb\u52a0\u6807\u9898\r\nplt.title('Correlation Heatmap')\r\n\r\n# \u663e\u793a\u56fe\u5f62\r\nplt.show()\n"})})]})}function _(r={}){const{wrapper:e}={...(0,s.R)(),...r.components};return e?(0,a.jsx)(e,{...r,children:(0,a.jsx)(p,{...r})}):p(r)}},8453:(r,e,n)=>{n.d(e,{R:()=>i,x:()=>l});var t=n(6540);const a={},s=t.createContext(a);function i(r){const e=t.useContext(s);return t.useMemo((function(){return"function"==typeof r?r(e):{...e,...r}}),[e,r])}function l(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(a):r.components||a:i(r.components),t.createElement(s.Provider,{value:e},r.children)}}}]);