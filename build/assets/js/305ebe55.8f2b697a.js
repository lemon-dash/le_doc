"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[109],{9042:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>c,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"comparison_classic","title":"comparison_classic","description":"\u7ecf\u5178\u7b97\u6cd5(\u6bd4\u8f83)","source":"@site/docs/comparison_classic.md","sourceDirName":".","slug":"/comparison_classic","permalink":"/docs/comparison_classic","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"atmosphere","permalink":"/docs/graphics/atmosphere"}}');var a=r(4848),i=r(8453);const t={},l=void 0,d={},o=[{value:"\u7ecf\u5178\u7b97\u6cd5(\u6bd4\u8f83)",id:"\u7ecf\u5178\u7b97\u6cd5\u6bd4\u8f83",level:2},{value:"\u7ebf\u6027\u56de\u5f52",id:"1",level:4},{value:"\u51b3\u7b56\u6811",id:"2",level:4},{value:"\u968f\u673a\u68ee\u6797",id:"3",level:4},{value:"Adaboost",id:"4",level:4},{value:"GBRT",id:"5",level:4},{value:"XGBoost",id:"6",level:4},{value:"\u652f\u6301\u5411\u91cf\u673a",id:"7",level:4},{value:"lasso\u56de\u5f52",id:"8",level:4},{value:"LSTM",id:"9",level:4},{value:"GRU",id:"10",level:4},{value:"BP",id:"11",level:4},{value:"CNN",id:"12",level:4},{value:"ResNet18",id:"13",level:4},{value:"VGG16",id:"14",level:4},{value:"densenet",id:"15",level:4}];function _(n){const e={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",pre:"pre",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h2,{id:"\u7ecf\u5178\u7b97\u6cd5\u6bd4\u8f83",children:"\u7ecf\u5178\u7b97\u6cd5(\u6bd4\u8f83)"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#1",children:"\u7ebf\u6027\u56de\u5f52"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#2",children:"\u51b3\u7b56\u6811"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#3",children:"\u968f\u673a\u68ee\u6797"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#4",children:"Adaboost"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#6",children:"GBRT"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#6",children:"XGBoost"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#7",children:"\u652f\u6301\u5411\u91cf\u673a"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#8",children:"lasso\u56de\u5f52"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#9",children:"LSTM"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#10",children:"GRU"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#11",children:"BP"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#12",children:"CNN"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#13",children:"ResNet18"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#14",children:"VGG16"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"#15",children:"DenseNet"})}),"\n"]}),"\n",(0,a.jsx)(e.h4,{id:"1",children:"\u7ebf\u6027\u56de\u5f52"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.linear_model import LinearRegression,RidgeCV,SGDRegressor\r\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score   #\u5747\u65b9\u8bef\u5dee\r\n# # # # 1.\u7ebf\u6027\u56de\u5f52----\u6b64\u5904\u4f7f\u7528\u57fa\u6a21\u578b\uff0c\u53c2\u6570\u5747\u4f7f\u7528\u9ed8\u8ba4\u7684\uff0c\u7528\u4e8e\u540e\u9762\u8c03\u53c2\u5bf9\u6bd4\r\n# Lr=LinearRegression(normalize=False)\r\n# Lr.fit(dujiqix_train,dujiqiy_train)\r\n# xy_predict=Lr.predict(dujiqix_test)\r\n# xy_modify = xy_predict*(xy_predict>=0)\r\n# r2=r2_score(dujiqiy_test,xy_modify)\r\n# # Mae=mean_squared_error(y_test,xy_modify)\r\n# r2\r\n # \u6570\u636e\u9006\u5f52\u4e00\u5316\r\nmaxmin = [datal['PM2.5'].max(), datal['PM2.5'].min()]  # \u539f\u6765\u7684\u6700\u5927\u6700\u5c0f\u503c\uff0c\u53cd\u653e\u7f29\r\nprint(maxmin)\r\n# xpreds = np.array(xy_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# xlabels = np.array(dujiqiy_test)\r\n# xpreds= xpreds.reshape((-1,1))\r\n# xfuture_len= xpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(xfuture_len):\r\n#     xlabels[:, k] = xlabels[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\n# for k in range(xfuture_len):\r\n#     xpreds[:, k] = xpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"2",children:"\u51b3\u7b56\u6811"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# tree_model=DecisionTreeRegressor()\r\n# tree_model.fit(dux_train,duy_train)\r\n# treey_pre=tree_model.predict(dux_test)\r\n# ty_modify = treey_pre*(treey_pre>=0)\r\n# jr=r2_score(duy_test,ty_modify)\r\n# # treemae2=mean_squared_error(y_test,ty_modify2)\r\n# # print(jr2,treemae2)\r\n# jr\r\n#  # \u6570\u636e\u9006\u5f52\u4e00\u5316\r\n# maxmin = [datal['PM2.5'].max(), datal['PM2.5'].min()]  # \u539f\u6765\u7684\u6700\u5927\u6700\u5c0f\u503c\uff0c\u53cd\u653e\u7f29\r\n# print(maxmin)\r\n# preds = np.array(ty_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(duy_test)\r\n# preds= preds.reshape((-1,1))\r\n# future_len= preds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(future_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\n# for k in range(future_len):\r\n#     preds[:, k] = preds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"3",children:"\u968f\u673a\u68ee\u6797"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.ensemble import RandomForestRegressor\r\nrd=RandomForestRegressor(max_depth=11,max_features=6,min_samples_split=35,n_estimators=96,min_samples_leaf=9)     #\u57fa\u6a21\u578b\r\nrd.fit(dujiqix_train,dujiqiy_train)\r\nsuiy_pres=rd.predict(dujiqix_test)\r\nyrand_modify = suiy_pres*(suiy_pres>=0)\r\nsuijir=r2_score(dujiqiy_test,yrand_modify)\r\nlabel = np.array(dujiqiy_test)\r\nfor k in range(1):\r\n#     print(k)\r\n    label[:, k] = label[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nnp.array(dujiqiy_test)\r\nspreds = np.array(yrand_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(dujiqiy_test)\r\nspreds= spreds.reshape((-1,1))\r\nsfuture_len= spreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(sfuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(sfuture_len):\r\n    spreds[:, k] = spreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"4",children:"Adaboost"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.ensemble import AdaBoostRegressor\r\n# \u4e5f\u53ef\u4ee5\u5bf9\u9009\u62e9\u7684\u5f31\u56de\u5f52\u5668\u8fdb\u884c\u53c2\u6570\u9009\u62e9\uff0c\u65b9\u5f0f\u4e3abase_estimator__\u82e5\u56de\u5f52\u5668\u53c2\u6570\u540d\r\nAda_model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=11, min_samples_split=35, min_samples_leaf=35),\r\n                            n_estimators=50,learning_rate=0.1,loss='linear')\r\nAda_model.fit(dujiqix_train,dujiqiy_train)\r\naday_pres=Ada_model.predict(dujiqix_test)\r\naday_modify = aday_pres*(aday_pres>=0)\r\nadar=r2_score(dujiqiy_test,aday_modify)\r\napreds = np.array(aday_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(dujiqiy_test)\r\napreds= apreds.reshape((-1,1))\r\nafuture_len= apreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(afuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(afuture_len):\r\n    apreds[:, k] = apreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"5",children:"GBRT"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.ensemble import GradientBoostingRegressor\r\nGbrt_model= GradientBoostingRegressor()#\u8fd9\u91cc\u4f7f\u752850\u4e2a\u51b3\u7b56\u6811\r\nGbrt_model.fit(dujiqix_train,dujiqiy_train)\r\ngbrty_pres=Gbrt_model.predict(dujiqix_test)\r\ngbrty_modify = gbrty_pres*(gbrty_pres>=0)\r\ngbrtr=r2_score(dujiqiy_test,gbrty_modify)\r\ngpreds = np.array(gbrty_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels2 = np.array(dujiqiy_test)\r\ngpreds= gpreds.reshape((-1,1))\r\ngfuture_len= gpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(gfuture_len):\r\n#     labels2[:, k] = labels2[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(gfuture_len):\r\n    gpreds[:, k] = gpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"6",children:"XGBoost"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from xgboost import XGBRegressor\r\n\r\nother_params = {'learning_rate': 0.1, 'n_estimators': 75, 'max_depth': 4, 'min_child_weight': 5, \r\n                     'colsample_bytree': 0.7, 'gamma': 0.01,}\r\nxgb_model1=XGBRegressor(**other_params)\r\nxgb_model1.fit(dujiqix_train,dujiqiy_train)\r\nxgby1_pred=xgb_model1.predict(dujiqix_test)\r\nxgby1_modify = xgby1_pred*(xgby1_pred>=0)\r\nxgbr1=r2_score(dujiqiy_test,xgby1_modify)\r\n\r\nother_params1 = {'learning_rate': 0.1, 'n_estimators': 70, 'max_depth': 4, 'min_child_weight': 5, \r\n                     'colsample_bytree': 0.7, 'gamma': 0.01,\r\n                 }\r\nxgb_model=XGBRegressor(**other_params1)\r\nxgb_model.fit(dujiqix_train,dujiqiy_train)\r\nxgby_pred=xgb_model.predict(dujiqix_test)\r\nxgby_modify = xgby_pred*(xgby_pred>=0)\r\nxgbr=r2_score(dujiqiy_test,xgby_modify)\r\nxgbpreds = np.array(xgby1_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels3 = np.array(dujiqiy_test)\r\nxgbpreds= xgbpreds.reshape((-1,1))\r\nxfuture_len= xgbpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(xfuture_len):\r\n#     labels3[:, k] = labels3[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor j in range(xfuture_len):\r\n    xgbpreds[:, j] = xgbpreds[:, j] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"7",children:"\u652f\u6301\u5411\u91cf\u673a"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.svm import SVR\r\nsvr_rbf = SVR(kernel='linear', C=10, gamma=1)\r\nsvr_rbf.fit(dujiqix_train,dujiqiy_train)\r\nsvry_pred= svr_rbf.predict(dujiqix_test)\r\nsvry_modify= svry_pred*(svry_pred>=0)\r\n\r\nsvrr=r2_score(dujiqiy_test,svry_modify)\r\nsvr_rbf2 = SVR(kernel='rbf', C=10, gamma=1)\r\nsvr_rbf2.fit(dujiqix_train,dujiqiy_train)\r\nsvry_pred2= svr_rbf2.predict(dujiqix_test)\r\nsvry_modify2 = svry_pred2*(svry_pred2>=0)\r\nsvrr2=r2_score(dujiqiy_test,svry_modify2)\r\nsvrpreds = np.array(svry_modify2)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(y_test)\r\nsvrpreds= svrpreds.reshape((-1,1))\r\nsvrfuture_len= svrpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(sfuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(svrfuture_len):\r\n    svrpreds[:, k] = svrpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"8",children:"lasso\u56de\u5f52"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sklearn.linear_model import LassoCV \r\nLa=LassoCV()\r\nLa.fit(dujiqix_train,dujiqiy_train)\r\nlay_predict=La.predict(dujiqix_test)\r\nlay_modify = lay_predict*(lay_predict>=0)\r\nrla=r2_score(dujiqiy_test,lay_modify)\r\nlapreds = np.array(lay_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(y_test)\r\nlapreds= lapreds.reshape((-1,1))\r\nlafuture_len= lapreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(lafuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(lafuture_len):\r\n    lapreds[:, k] = lapreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,a.jsx)(e.h4,{id:"9",children:"LSTM"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# \u8bfb\u53d6\u8bad\u7ec3\u96c6\r\ndushendux_train=pd.read_csv('shendux_train.csv')\r\ndushendux_train1=np.array(dushendux_train)\r\ndushendux_train2=np.reshape(dushendux_train1,(-1,120,7))\r\n\r\ndushenduy_train=pd.read_csv('shenduy_train.csv')\r\ndushenduy_train2=np.array(dushenduy_train)\r\n# \u8bfb\u53d6\u6d4b\u8bd5\u96c6\r\ndushendux_test=pd.read_csv('shendux_test.csv')\r\ndushendux_test1=np.array(dushendux_test)\r\ndushendux_test2=np.reshape(dushendux_test1,(-1,120,7))\r\n\r\ndushenduy_test=pd.read_csv('shenduy_test.csv')\r\ndushenduy_test2=np.array(dushenduy_test)\r\n\r\n# # # \u6784\u9020\u6279\u6570\u636e\r\ndef create_batch_dataset(x,y,train=True,buffer_size=1000,batch_size=64):#buffer_size=1000\u8868\u793a\u53ef\u4ee5\u6253\u4e71\u7a97\u53e3\u91cc\u9762\u7684\u6570\u636e\r\n    batch_data=tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))#\u6570\u636e\u5c01\u88c5\uff0ctensor\u7c7b\u578b\r\n    if train:\r\n        return batch_data.cache().shuffle(buffer_size).batch(batch_size)\r\n    else:\r\n        return batch_data.batch(batch_size)\r\ntrain_data_single=create_batch_dataset(dushendux_train2, dushenduy_train2,train=True)\r\nval_data_single=create_batch_dataset(dushendux_test2,dushenduy_test2,train=False)\r\n\r\n# \u6dfb\u52a0\u6ce8\u610f\u529b\u673a\u5236\r\ndef attention_bilstm(inputs):\r\n    \"\"\"\r\n        Many-to-one attention mechanism for Keras.\r\n        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\r\n        @return: 2D tensor with shape (batch_size, 128)\r\n        @author: felixhao28, philipperemy.\r\n        \"\"\"\r\n    hidden_states = inputs\r\n    hidden_size = int(hidden_states.shape[2])\r\n        # Inside dense layer\r\n        #              hidden_states            dot               W            =>           score_first_part\r\n        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\r\n        # W is the trainable weight matrix of attention Luong's multiplicative style score\r\n    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\r\n        #            score_first_part           dot        last_hidden_state     => attention_weights\r\n        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\r\n    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\r\n    score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\r\n    attention_weights = Activation('softmax', name='attention_weight')(score)\r\n        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\r\n    context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\r\n    pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\r\n    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\r\n    return attention_vector\r\n\r\n# \u6784\u9020BI_GRU\r\n# attbilstm_model = Sequential()\r\ntime_steps=120\r\ninput_dim=7\r\nmodel_input=Input(shape=(time_steps,input_dim))\r\nbi_gru1=Bidirectional(LSTM(128,return_sequences=True))(model_input)\r\nbi_gru2=Dropout(0.2)(bi_gru1)\r\nbi_gru3=Bidirectional(LSTM(64,return_sequences=True))(bi_gru2)\r\n# attbilstm_model.add(Bidirectional(GRU(64), input_shape=trainx.shape[-2:],return_sequences=True))\r\nattbigru=attention_bilstm(bi_gru3)\r\n# a=Dropout(0.2)(attbigru)\r\nzhong=Dense(1)(attbigru)\r\nattbilstm_model=Model(model_input,zhong)\r\nattbilstm_model.compile(optimizer='adam', loss='mae')# metrics=['accuracy']\r\nattbilstm_history = attbilstm_model.fit(train_data_single, validation_data=val_data_single,epochs=800, verbose=1)\r\n\r\n# \u9884\u6d4b\r\nattbilstm_pres=attbilstm_model.predict(dushendux_test2,verbose=1)\r\nattbilstm_modify = attbilstm_pres*(attbilstm_pres>=0)\r\nlstmr1=r2_score(dushenduy_test2,attbilstm_modify)\n"})}),"\n",(0,a.jsx)(e.h4,{id:"10",children:"GRU"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from keras.layers import GRU\r\n\r\ndef trainModel(train_X, train_Y, test_X, test_Y):\r\n    model = Sequential()\r\n    model.add(GRU(108, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\r\n    # model.add(Dropout(0.3))\r\n    model.add(Dense(train_Y.shape[1]))\r\n    model.add(Activation(\"relu\"))\r\n    adam = adam_v2.Adam(learning_rate=0.01)\r\n    model.compile(loss='mse', optimizer=adam, metrics=['acc'])\r\n    # \u4fdd\u5b58\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u635f\u5931\u51fd\u6570\u548c\u7cbe\u786e\u5ea6\u7684\u53d8\u5316\r\n    log = CSVLogger(f\"./log50\u70bc\u4e393.csv\", separator=\",\", append=True)\r\n    # \u7528\u6765\u81ea\u52a8\u964d\u4f4e\u5b66\u4e60\u7387\r\n    reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=1, verbose=1,\r\n                               mode='auto', min_delta=0.001, cooldown=0, min_lr=0.001)\r\n    # \u6a21\u578b\u8bad\u7ec3\r\n    model.fit(train_X, train_Y, epochs=50, batch_size=32, verbose=1, validation_split=0.1,\r\n                  callbacks=[log, reduce])\r\n    # \u7528\u6d4b\u8bd5\u96c6\u8bc4\u4f30\r\n    loss, acc = model.evaluate(test_X, test_Y, verbose=1)\r\n    print('Loss : {}, Accuracy: {}'.format(loss, acc * 100))\r\n    # \u4fdd\u5b58\u6a21\u578b\r\n    model.save(f\"./GRU_50_model\u70bc\u4e393_.h5\")\r\n    # \u6253\u5370\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u7edf\u8ba1\u53c2\u6570\u6570\u76ee\r\n    model.summary()\r\n    return model\n"})}),"\n",(0,a.jsx)(e.h4,{id:"11",children:"BP"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from keras.models import Sequential\r\nfrom keras.layers import Dense, Flatten\r\nfrom keras.optimizers import Adam\r\nfrom keras.callbacks import CSVLogger, ReduceLROnPlateau\r\n\r\ndef trainModel(train_X, train_Y, test_X, test_Y):\r\n    model = Sequential()\r\n    model.add(Flatten(input_shape=(train_X.shape[1], train_X.shape[2])))  # \u5c06\u6570\u636e\u5c55\u5e73\u4ee5\u9002\u5e94\u8f93\u5165\u5c42\r\n    model.add(Dense(64, activation='relu'))  # \u8f93\u5165\u5c42\r\n    model.add(Dense(32, activation='relu'))  # \u9690\u85cf\u5c42\r\n    model.add(Dense(train_Y.shape[1]))  # \u8f93\u51fa\u5c42\r\n\r\n    adam = Adam(learning_rate=0.01)\r\n    model.compile(loss='mse', optimizer=adam, metrics=['acc'])\r\n    \r\n    log = CSVLogger(f\"./bp_50\u70bc\u4e39.csv\", separator=\",\", append=True)\r\n    reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=1, verbose=1, mode='auto', min_delta=0.001, cooldown=0, min_lr=0.001)\r\n    model.fit(train_X, train_Y, epochs=50, batch_size=32, verbose=1, validation_split=0.1, callbacks=[log, reduce])\r\n    \r\n    loss, acc = model.evaluate(test_X, test_Y, verbose=1)\r\n    print('Loss : {}, Accuracy: {}'.format(loss, acc * 100))\r\n    \r\n    model.save(f\"./BP_50_model\u70bc\u4e39.h5\")\r\n    model.summary()\r\n    return model\n"})}),"\n",(0,a.jsx)(e.h4,{id:"12",children:"CNN"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nclass Conv2d(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\r\n                            nn.BatchNorm2d(cout)\r\n                            )\r\n        self.act = nn.ReLU()\r\n        self.residual = residual\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        if self.residual:\r\n            out += x\r\n        return self.act(out)\r\n\r\nclass nonorm_Conv2d(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\r\n                            )\r\n        self.act = nn.LeakyReLU(0.01, inplace=True)\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        return self.act(out)\r\n\r\nclass Conv2dTranspose(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\r\n                            nn.BatchNorm2d(cout)\r\n                            )\r\n        self.act = nn.ReLU()\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        return self.act(out)\n"})}),"\n",(0,a.jsx)(e.h4,{id:"13",children:"ResNet18"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class Residual(nn.Module):\r\n    def __init__(self, input_channels, num_channels, use_1conv = False,strides = 1):  # c1\u662f\u5377\u79ef\u6838\u6570\u91cf\r\n        super(Residual, self).__init__()\r\n        self.ReLu = nn.ReLU()\r\n        self.conv1 = nn.Conv2d(in_channels=input_channels,out_channels=num_channels,kernel_size=3,padding=1,stride=strides)\r\n        self.conv2 = nn.Conv2d(in_channels=num_channels,out_channels=num_channels,kernel_size=3,padding=1)\r\n        self.bn1 = nn.BatchNorm2d(num_channels)\r\n        self.bn2 = nn.BatchNorm2d(num_channels)\r\n        if use_1conv:   #\u5982\u679c\u67091*1\u5377\u79ef\r\n            self.conv3 = nn.Conv2d(in_channels=input_channels,out_channels=num_channels,kernel_size=1,stride=strides)\r\n        else:\r\n            self.conv3 = None\r\n\r\n    def forward(self,x):  #\u524d\u5411\u4f20\u64ad\r\n        y = self.ReLu(self.bn1(self.conv1(x)))\r\n        y = self.bn2(self.conv2(y))\r\n\r\n        if self.conv3:\r\n            x = self.conv3(x)\r\n        y = self.ReLu(x + y)\r\n        return y\r\n\r\n\r\n\r\nclass ResNet18(nn.Module):\r\n    def __init__(self,Residual):\r\n        super(ResNet18,self).__init__()\r\n        self.b1 = nn.Sequential(  # \u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=3, stride=2),\r\n            nn.ReLU(),\r\n            nn.BatchNorm2d(64),\r\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        )\r\n        self.b2 = nn.Sequential(  #\u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n             Residual(64, 64, use_1conv = False,strides = 1),\r\n             Residual(64, 64, use_1conv = False,strides = 1),\r\n        )\r\n        self.b3 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(64,  128, use_1conv=True, strides=2),\r\n            Residual(128, 128, use_1conv=False, strides=1),\r\n        )\r\n        self.b4 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(128, 256, use_1conv=True, strides=2),\r\n            Residual(256, 256, use_1conv=False, strides=1),\r\n        )\r\n        self.b5 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            Residual(256, 512, use_1conv=True, strides=2),\r\n            Residual(512, 512, use_1conv=False, strides=1),\r\n        )\r\n        self.b6 = nn.Sequential(  # \u524d\u4fe9\u6b8b\u5dee\u5757\u4e3a\u4e00\u5305\u88c5\u5757\r\n            nn.AdaptiveAvgPool2d((1,1)),\r\n            nn.Flatten(),\r\n            nn.Linear(512,5)\r\n        )\r\n    def forward(self,x):\r\n        x = self.b1(x)\r\n        x = self.b2(x)\r\n        x = self.b3(x)\r\n        x = self.b4(x)\r\n        x = self.b5(x)\r\n        x = self.b6(x)\r\n        return x\n"})}),"\n",(0,a.jsx)(e.h4,{id:"14",children:"VGG16"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import  torch\r\nfrom torch import nn\r\nfrom torch.cuda import device\r\nfrom torchsummary import summary\r\n\r\nclass VGG16(nn.Module):\r\n    def __init__(self):\r\n        super(VGG16,self).__init__()\r\n        self.block1 = nn.Sequential(  #\u7528\u5e8f\u5217\u6765\u5305\u88c5\u5757\r\n            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2,stride=2)\r\n        )\r\n        self.block2 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        ) #\u5b9a\u4e49\u5757\r\n        self.block3 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block4 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block5 = nn.Sequential(  # \u5e8f\u5217\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )  # \u5b9a\u4e49\u5757\r\n        self.block6 = nn.Sequential(    #\u5168\u8fde\u63a5\u5c42\u4e0d\u7528relu\u6fc0\u6d3b\r\n            nn.Flatten(),\r\n            nn.Linear(7*7*512,128),#\u8981\u5199\u8f93\u5165\u8f93\u51fa\u5927\u5c0f    \u663e\u5b58\u5c0f\uff0c\u8bbe\u7f6e\u7684\u5c0f\u70b9\u513f    \u539f\u67654096\r\n            nn.ReLU(),\r\n            nn.Linear(128,64),  # 4096 \uff0c 4096\r\n            nn.ReLU(),\r\n            nn.Linear(64,10)    # 4096 \uff0c 10\r\n        )\r\n\r\n        #\u521d\u59cb\u5316w\uff0cb \uff0c\u9632\u6b62\u8bad\u7ec3\u540e\u4e0d\u6536\u655b\r\n        for m in self.modules():\r\n            #print(m)  #\u6253\u5370\u6240\u6709\u7f51\u7edc\u5c42\r\n            if isinstance(m, nn.Conv2d):     #\u5377\u79ef\u521d\u59cb\u5316\u6743\u91cdw\u65f6\uff0c\u7528\u51ef\u660e\u521d\u59cb\u5316\r\n                nn.init.kaiming_normal_(m.weight,nonlinearity=\'relu\')    #kaiming\u9488\u5bf9\u4e8e\u6fc0\u6d3b\u51fd\u6570\r\n                if m.bias is not None:   # b\u662f\u504f\u79fb\u503c\uff0c\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u521d\u59cb\u5316\u4e3a0\r\n                    nn.init.constant_(m.bias,0)\r\n            elif isinstance(m, nn.Linear):  #\u5168\u8fde\u63a5\u5c42\u521d\u59cb\u5316\r\n                nn.init.normal_(m.weight,0,0.01)  #\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a0.01\r\n                if m.bias is not None:   # b\u662f\u504f\u79fb\u503c\uff0c\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u521d\u59cb\u5316\u4e3a0\r\n                    nn.init.constant_(m.bias,0)\r\n\r\n\r\n    def forward(self,x):\r\n        x = self.block1(x)\r\n        x = self.block2(x)\r\n        x = self.block3(x)\r\n        x = self.block4(x)\r\n        x = self.block5(x)\r\n        x = self.block6(x)\r\n        return x\r\n\r\nif __name__ == "__main__":\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n    model = VGG16().to(device)\r\n    print(summary(model,(3,224,224)))\n'})}),"\n",(0,a.jsx)(e.h4,{id:"15",children:"densenet"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class DenseLayer(nn.Module):\r\n    def __init__(self, in_channels, growth_rate):\r\n        super(DenseLayer, self).__init__()\r\n        self.bn1 = nn.BatchNorm2d(in_channels)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv1 = nn.Conv2d(in_channels, 4 * growth_rate, kernel_size=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\r\n        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\r\n\r\n    def forward(self, x):\r\n        out = self.bn1(x)\r\n        out = self.relu(out)\r\n        out = self.conv1(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n        out = self.conv2(out)\r\n        out = torch.cat((x, out), 1)  # concatenate input and output\r\n        return out\r\n\r\nclass DenseBlock(nn.Module):\r\n    def __init__(self, in_channels, num_layers, growth_rate):\r\n        super(DenseBlock, self).__init__()\r\n        layers = []\r\n        for _ in range(num_layers):\r\n            layers.append(DenseLayer(in_channels, growth_rate))\r\n            in_channels += growth_rate\r\n        self.dense_block = nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        return self.dense_block(x)\r\n\r\nclass TransitionLayer(nn.Module):\r\n    def __init__(self, in_channels):\r\n        super(TransitionLayer, self).__init__()\r\n        self.bn = nn.BatchNorm2d(in_channels)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, bias=False)\r\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\r\n\r\n    def forward(self, x):\r\n        out = self.bn(x)\r\n        out = self.relu(out)\r\n        out = self.conv(out)\r\n        out = self.pool(out)\r\n        return out\r\n\r\nclass DenseNet(nn.Module):\r\n    def __init__(self, growth_rate=32, num_layers=[6, 12, 24, 16], num_classes=10):\r\n        super(DenseNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        in_channels = 64\r\n        self.dense_blocks = []\r\n        for i in range(len(num_layers)):\r\n            self.dense_blocks.append(DenseBlock(in_channels, num_layers[i], growth_rate))\r\n            in_channels += num_layers[i] * growth_rate\r\n            if i != len(num_layers) - 1:\r\n                self.dense_blocks.append(TransitionLayer(in_channels))\r\n                in_channels //= 2  # halve the channels after transition\r\n\r\n        self.dense_blocks = nn.Sequential(*self.dense_blocks)\r\n        self.bn2 = nn.BatchNorm2d(in_channels)\r\n        self.relu2 = nn.ReLU(inplace=True)\r\n        self.fc = nn.Linear(in_channels, num_classes)\r\n\r\n    def forward(self, x):\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n        out = self.pool(out)\r\n\r\n        out = self.dense_blocks(out)\r\n        out = self.bn2(out)\r\n        out = self.relu2(out)\r\n        out = F.adaptive_avg_pool2d(out, (1, 1))\r\n        out = out.view(out.size(0), -1)\r\n        out = self.fc(out)\r\n        return out\n"})})]})}function c(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(_,{...n})}):_(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>t,x:()=>l});var s=r(6540);const a={},i=s.createContext(a);function t(n){const e=s.useContext(i);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),s.createElement(i.Provider,{value:e},n.children)}}}]);