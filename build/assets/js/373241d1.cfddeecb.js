"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4902],{59:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>_});const a=JSON.parse('{"id":"graphics/video_gen","title":"video_gen","description":"\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210","source":"@site/docs/graphics/video_gen.md","sourceDirName":"graphics","slug":"/graphics/video_gen","permalink":"/docs/graphics/video_gen","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"atmosphere","permalink":"/docs/graphics/atmosphere"},"next":{"title":"intro","permalink":"/docs/intro"}}');var t=r(4848),i=r(8453);const s={},l=void 0,o={},_=[{value:"\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210",id:"\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210",level:2},{value:"IP_LAP-main",id:"ilm",level:4},{value:"\u97f3\u9891\u9884\u5904\u7406",id:"app",level:4},{value:"\u89c6\u9891\u9884\u5904\u7406",id:"vpp",level:4},{value:"audio.py",id:"ap",level:4},{value:"\u751f\u6210landmark",id:"gl",level:4},{value:"\u751f\u6210pixmap",id:"gp",level:4},{value:"\u751f\u6210\u89c6\u9891",id:"gv",level:4},{value:"\u7ed8\u5236landmark",id:"dl",level:4},{value:"\u63a8\u7406",id:"is",level:4},{value:"Wav2Lip",id:"W2l",level:4},{value:"\u9762\u90e8\u68c0\u6d4b\u6a21\u578b",id:"fd",level:4},{value:"conv",id:"conv",level:4},{value:"syncnet",id:"sync",level:4},{value:"wav2Lip",id:"w2l",level:4},{value:"gen_videos_from_filelist",id:"gvff",level:4},{value:"real_videos_inference",id:"rvi",level:4}];function d(n){const e={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{id:"\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210",children:"\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210"}),"\n",(0,t.jsx)(e.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d](../../video_gen.zip"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"#ilm",children:"IP_LAP-main"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#app",children:"\u97f3\u9891\u9884\u5904\u7406"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#vpp",children:"\u89c6\u9891\u9884\u5904\u7406"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#ap",children:"audio.py"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#gl",children:"\u751f\u6210landmark"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#gp",children:"\u751f\u6210pixmap"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#gv",children:"\u751f\u6210\u89c6\u9891"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#dl",children:"\u7ed8\u5236landmark"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#is",children:"\u63a8\u7406"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"#W2l",children:"Wav2lip"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#fd",children:"\u9762\u90e8\u68c0\u6d4b\u6a21\u578b"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#conv",children:"conv"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#sync",children:"syncnet"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#w2l",children:"wav2Lip"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#gvff",children:"gen_videos_from_filelist"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"#rvi",children:"real_videos_inference"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"\u200b"}),"\n",(0,t.jsx)(e.h4,{id:"ilm",children:"IP_LAP-main"}),"\n",(0,t.jsx)(e.h4,{id:"app",children:"\u97f3\u9891\u9884\u5904\u7406"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import sys\r\nsys.path.append(\"..\")\r\nfrom  models import audio\r\nfrom os import  path\r\nfrom concurrent.futures import as_completed, ProcessPoolExecutor\r\nimport numpy as np\r\nimport argparse, os, cv2, traceback, subprocess\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--process_num', type=int, default=6) #number of process to preprocess the audio\r\nparser.add_argument(\"--data_root\", type=str,help=\"Root folder of the LRS2 dataset\", required=True)\r\nparser.add_argument(\"--out_root\", help=\"output audio root\", required=True)\r\nargs = parser.parse_args()\r\nsample_rate=16000  # 16000Hz\r\ntemplate = 'ffmpeg -loglevel panic -y -i {} -strict -2 {}'\r\n\r\ndef process_audio_file(vfile, args):\r\n    vidname = os.path.basename(vfile).split('.')[0]\r\n    dirname = vfile.split('/')[-2]\r\n\r\n    fulldir = path.join(args.out_root, dirname, vidname)\r\n    os.makedirs(fulldir, exist_ok=True)\r\n    wavpath = path.join(fulldir, 'audio.wav')\r\n\r\n    command = template.format(vfile.replace(' ', r'\\ '), wavpath.replace(' ', r'\\ '))\r\n    subprocess.run(command, shell=True)\r\n    wav = audio.load_wav(wavpath, sample_rate)\r\n    orig_mel = audio.melspectrogram(wav).T\r\n    np.save(path.join(fulldir, 'audio'), orig_mel)\r\n\r\n\r\ndef mp_handler_audio(job):\r\n    vfile, args = job\r\n    try:\r\n        process_audio_file(vfile, args)\r\n    except KeyboardInterrupt:\r\n        exit(0)\r\n    except:\r\n        traceback.print_exc()\r\n\r\n\r\ndef main(args):\r\n    print(\"looking up paths.... from\", args.data_root)\r\n    filelist = glob(path.join(args.data_root, '*/*.mp4'))\r\n\r\n    jobs = [(vfile, args) for i, vfile in enumerate(filelist)]\r\n    p_audio = ProcessPoolExecutor(args.process_num)\r\n    futures_audio = [p_audio.submit(mp_handler_audio, j) for j in jobs]\r\n\r\n    _ = [r.result() for r in tqdm(as_completed(futures_audio), total=len(futures_audio))]\r\n    print(\"complete, output to\",args.out_root)\r\n\r\nif __name__ == '__main__':\r\n    main(args)\n"})}),"\n",(0,t.jsx)(e.h4,{id:"vpp",children:"\u89c6\u9891\u9884\u5904\u7406"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import os.path\r\nimport mediapipe as mp\r\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\r\nimport os, traceback\r\nfrom tqdm import tqdm\r\nimport glob\r\nimport argparse\r\nimport math\r\nfrom typing import List, Mapping, Optional, Tuple, Union\r\nimport cv2\r\nimport dataclasses\r\nimport numpy as np\r\nfrom mediapipe.framework.formats import landmark_pb2\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--process_num', type=int, default=6) #number of process in ThreadPool to preprocess the dataset\r\nparser.add_argument('--dataset_video_root', type=str, required=True)\r\nparser.add_argument('--output_sketch_root', type=str, default='./lrs2_sketch128')\r\nparser.add_argument('--output_face_root', type=str, default='./lrs2_face128')\r\nparser.add_argument('--output_landmark_root', type=str, default='./lrs2_landmarks')\r\n\r\nargs = parser.parse_args()\r\n\r\ninput_mp4_root = args.dataset_video_root\r\noutput_sketch_root = args.output_sketch_root\r\noutput_face_root=args.output_face_root\r\noutput_landmark_root=args.output_landmark_root\r\n\r\n\r\n\r\n\"\"\"MediaPipe solution drawing utils.\"\"\"\r\n_PRESENCE_THRESHOLD = 0.5\r\n_VISIBILITY_THRESHOLD = 0.5\r\n_BGR_CHANNELS = 3\r\n\r\nWHITE_COLOR = (224, 224, 224)\r\nBLACK_COLOR = (0, 0, 0)\r\nRED_COLOR = (0, 0, 255)\r\nGREEN_COLOR = (0, 128, 0)\r\nBLUE_COLOR = (255, 0, 0)\r\n\r\n@dataclasses.dataclass\r\nclass DrawingSpec:\r\n    # Color for drawing the annotation. Default to the white color.\r\n    color: Tuple[int, int, int] = WHITE_COLOR\r\n    # Thickness for drawing the annotation. Default to 2 pixels.\r\n    thickness: int = 2\r\n    # Circle radius. Default to 2 pixels.\r\n    circle_radius: int = 2\r\n\r\n\r\ndef _normalized_to_pixel_coordinates(\r\n        normalized_x: float, normalized_y: float, image_width: int,\r\n        image_height: int) -> Union[None, Tuple[int, int]]:\r\n    \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\r\n\r\n    # Checks if the float value is between 0 and 1.\r\n    def is_valid_normalized_value(value: float) -> bool:\r\n        return (value > 0 or math.isclose(0, value)) and (value < 1 or\r\n                                                          math.isclose(1, value))\r\n\r\n    if not (is_valid_normalized_value(normalized_x) and\r\n            is_valid_normalized_value(normalized_y)):\r\n        # TODO: Draw coordinates even if it's outside of the image bounds.\r\n        return None\r\n    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\r\n    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\r\n    return x_px, y_px\r\n\r\n\r\nFACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\r\n                           (17, 314), (314, 405), (405, 321), (321, 375),\r\n                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\r\n                           (37, 0), (0, 267),\r\n                           (267, 269), (269, 270), (270, 409), (409, 291),\r\n                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\r\n                           (14, 317), (317, 402), (402, 318), (318, 324),\r\n                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\r\n                           (82, 13), (13, 312), (312, 311), (311, 310),\r\n                           (310, 415), (415, 308)])\r\n\r\nFACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\r\n                               (374, 380), (380, 381), (381, 382), (382, 362),\r\n                               (263, 466), (466, 388), (388, 387), (387, 386),\r\n                               (386, 385), (385, 384), (384, 398), (398, 362)])\r\n\r\nFACEMESH_LEFT_IRIS = frozenset([(474, 475), (475, 476), (476, 477),\r\n                                (477, 474)])\r\n\r\nFACEMESH_LEFT_EYEBROW = frozenset([(276, 283), (283, 282), (282, 295),\r\n                                   (295, 285), (300, 293), (293, 334),\r\n                                   (334, 296), (296, 336)])\r\n\r\nFACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\r\n                                (145, 153), (153, 154), (154, 155), (155, 133),\r\n                                (33, 246), (246, 161), (161, 160), (160, 159),\r\n                                (159, 158), (158, 157), (157, 173), (173, 133)])\r\n\r\nFACEMESH_RIGHT_EYEBROW = frozenset([(46, 53), (53, 52), (52, 65), (65, 55),\r\n                                    (70, 63), (63, 105), (105, 66), (66, 107)])\r\n\r\nFACEMESH_RIGHT_IRIS = frozenset([(469, 470), (470, 471), (471, 472),\r\n                                 (472, 469)])\r\n\r\nFACEMESH_FACE_OVAL = frozenset([(389, 356), (356, 454),\r\n                                (454, 323), \r\n                                (323, 361), (361, 288), (288, 397),\r\n                                (397, 365), (365, 379), (379, 378), (378, 400),\r\n                                (400, 377), (377, 152), (152, 148), (148, 176),\r\n                                (176, 149), (149, 150), (150, 136), (136, 172),\r\n                                (172, 58), (58, 132), (132, 93), \r\n                                (93, 234),\r\n                                (234, 127), (127, 162)])\r\n# (10, 338), (338, 297), (297, 332), (332, 284),(284, 251), (251, 389) (162, 21), (21, 54),(54, 103), (103, 67), (67, 109), (109, 10)\r\n\r\nFACEMESH_NOSE = frozenset([(168, 6), (6, 197), (197, 195), (195, 5), (5, 4), \\\r\n                           (4, 45), (45, 220), (220, 115), (115, 48), \\\r\n                           (4, 275), (275, 440), (440, 344), (344, 278), ])\r\nFACEMESH_FULL = frozenset().union(*[\r\n    FACEMESH_LIPS, FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, FACEMESH_RIGHT_EYE,\r\n    FACEMESH_RIGHT_EYEBROW, FACEMESH_FACE_OVAL, FACEMESH_NOSE\r\n])\r\ndef summarize_landmarks(edge_set):\r\n    landmarks = set()\r\n    for a, b in edge_set:\r\n        landmarks.add(a)\r\n        landmarks.add(b)\r\n    return landmarks\r\n\r\nall_landmark_idx = summarize_landmarks(FACEMESH_FULL)\r\npose_landmark_idx = \\\r\n    summarize_landmarks(FACEMESH_NOSE.union(*[FACEMESH_RIGHT_EYEBROW, FACEMESH_RIGHT_EYE, \\\r\n                                              FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, ])).union(\r\n        [162, 127, 234, 93, 389, 356, 454, 323])\r\ncontent_landmark_idx = all_landmark_idx - pose_landmark_idx\r\n\r\ndef draw_landmarks(\r\n        image: np.ndarray,\r\n        landmark_list: landmark_pb2.NormalizedLandmarkList,\r\n        connections: Optional[List[Tuple[int, int]]] = None,\r\n        landmark_drawing_spec: Union[DrawingSpec,\r\n        Mapping[int, DrawingSpec]] = DrawingSpec(\r\n            color=RED_COLOR),\r\n        connection_drawing_spec: Union[DrawingSpec,\r\n        Mapping[Tuple[int, int],\r\n        DrawingSpec]] = DrawingSpec()):\r\n    \"\"\"Draws the landmarks and the connections on the image.\r\n  Args:\r\n    image: A three channel BGR image represented as numpy ndarray.\r\n    landmark_list: A normalized landmark list proto message to be annotated on\r\n      the image.\r\n    connections: A list of landmark index tuples that specifies how landmarks to\r\n      be connected in the drawing.\r\n    landmark_drawing_spec: Either a DrawingSpec object or a mapping from\r\n      hand landmarks to the DrawingSpecs that specifies the landmarks' drawing\r\n      settings such as color, line thickness, and circle radius.\r\n      If this argument is explicitly set to None, no landmarks will be drawn.\r\n    connection_drawing_spec: Either a DrawingSpec object or a mapping from\r\n      hand connections to the DrawingSpecs that specifies the\r\n      connections' drawing settings such as color and line thickness.\r\n      If this argument is explicitly set to None, no landmark connections will\r\n      be drawn.\r\n\r\n  Raises:\r\n    ValueError: If one of the followings:\r\n      a) If the input image is not three channel BGR.\r\n      b) If any connetions contain invalid landmark index.\r\n  \"\"\"\r\n    if not landmark_list:\r\n        return\r\n    if image.shape[2] != _BGR_CHANNELS:\r\n        raise ValueError('Input image must contain three channel bgr data.')\r\n    image_rows, image_cols, _ = image.shape\r\n    idx_to_coordinates = {}\r\n    for idx, landmark in enumerate(landmark_list.landmark):\r\n        if ((landmark.HasField('visibility') and\r\n             landmark.visibility < _VISIBILITY_THRESHOLD) or\r\n                (landmark.HasField('presence') and\r\n                 landmark.presence < _PRESENCE_THRESHOLD)):\r\n            continue\r\n        landmark_px = _normalized_to_pixel_coordinates(landmark.x, landmark.y,\r\n                                                       image_cols, image_rows)\r\n        if landmark_px:\r\n            idx_to_coordinates[idx] = landmark_px\r\n    if connections:\r\n        num_landmarks = len(landmark_list.landmark)\r\n        # Draws the connections if the start and end landmarks are both visible.\r\n        for connection in connections:\r\n            start_idx = connection[0]\r\n            end_idx = connection[1]\r\n            if not (0 <= start_idx < num_landmarks and 0 <= end_idx < num_landmarks):\r\n                raise ValueError(f'Landmark index is out of range. Invalid connection '\r\n                                 f'from landmark #{start_idx} to landmark #{end_idx}.')\r\n            if start_idx in idx_to_coordinates and end_idx in idx_to_coordinates:\r\n                drawing_spec = connection_drawing_spec[connection] if isinstance(\r\n                    connection_drawing_spec, Mapping) else connection_drawing_spec\r\n                # if start_idx in content_landmark and end_idx in content_landmark:\r\n                cv2.line(image, idx_to_coordinates[start_idx],\r\n                         idx_to_coordinates[end_idx], drawing_spec.color,\r\n                         drawing_spec.thickness)\r\n    # Draws landmark points after finishing the connection lines, which is\r\n    # aesthetically better.\r\n\r\nmp_drawing = mp.solutions.drawing_utils\r\nmp_face_mesh = mp.solutions.face_mesh\r\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\r\n\r\ndef process_video_file(mp4_path):\r\n    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True,\r\n                               min_detection_confidence=0.5) as face_mesh:\r\n        video_stream = cv2.VideoCapture(mp4_path)\r\n        fps = round(video_stream.get(cv2.CAP_PROP_FPS))\r\n        if fps != 25:\r\n            print(mp4_path, ' fps is not 25!!!')\r\n            exit()\r\n        frames = []\r\n        while 1:\r\n            still_reading, frame = video_stream.read()\r\n            if not still_reading:\r\n                video_stream.release()\r\n                break\r\n            frames.append(frame)\r\n\r\n        for frame_idx,full_frame in enumerate(frames):\r\n            h, w = full_frame.shape[0], full_frame.shape[1]\r\n            results = face_mesh.process(cv2.cvtColor(full_frame, cv2.COLOR_BGR2RGB))\r\n            if not results.multi_face_landmarks:\r\n                continue  # not detect\r\n            face_landmarks=results.multi_face_landmarks[0]\r\n\r\n            #(1)normalize landmarks\r\n            x_min=999\r\n            x_max=-999\r\n            y_min=999\r\n            y_max=-999\r\n            pose_landmarks, content_landmarks = [], []\r\n            for idx, landmark in enumerate(face_landmarks.landmark):\r\n                if idx in all_landmark_idx:\r\n                    if landmark.x<x_min:\r\n                        x_min=landmark.x\r\n                    if landmark.x>x_max:\r\n                        x_max=landmark.x\r\n\r\n                    if landmark.y<y_min:\r\n                        y_min=landmark.y\r\n                    if landmark.y>y_max:\r\n                        y_max=landmark.y\r\n                ######\r\n                if idx in pose_landmark_idx:\r\n                    pose_landmarks.append((idx,landmark.x,landmark.y))\r\n                if idx in content_landmark_idx:\r\n                    content_landmarks.append((idx,landmark.x,landmark.y))\r\n            ##########plus 5 pixel to size##########\r\n            x_min=max(x_min-5/w,0)\r\n            x_max = min(x_max + 5 / w, 1)\r\n            #\r\n            y_min = max(y_min - 5 / h, 0)\r\n            y_max = min(y_max + 5 / h, 1)\r\n            face_frame=cv2.resize(full_frame[int(y_min*h):int(y_max*h),int(x_min*w):int(x_max*w)],(128,128))\r\n\r\n            # update landmarks\r\n            pose_landmarks=[ \\\r\n                (idx,(x-x_min)/(x_max-x_min),(y-y_min)/(y_max-y_min)) for idx,x,y in pose_landmarks]\r\n            content_landmarks=[\\\r\n                (idx, (x - x_min) / (x_max - x_min), (y - y_min) / (y_max - y_min)) for idx, x, y in content_landmarks]\r\n            # update drawed landmarks\r\n            for idx,x,y in pose_landmarks + content_landmarks:\r\n                face_landmarks.landmark[idx].x=x\r\n                face_landmarks.landmark[idx].y=y\r\n            #save landmarks\r\n            result_dict={}\r\n            result_dict['pose_landmarks']=pose_landmarks\r\n            result_dict['content_landmarks']=content_landmarks\r\n            out_dir = os.path.join(output_landmark_root, '/'.join(mp4_path[:-4].split('/')[-2:]))\r\n            os.makedirs(out_dir, exist_ok=True)\r\n            np.save(os.path.join(out_dir,str(frame_idx)),result_dict)\r\n\r\n            #save sketch\r\n            h_new=(y_max-y_min)*h\r\n            w_new = (x_max - x_min) * w\r\n            annotated_image = np.zeros((int(h_new * 128 / min(h_new, w_new)), int(w_new * 128 / min(h_new, w_new)), 3))\r\n            draw_landmarks(\r\n                image=annotated_image,\r\n                landmark_list=face_landmarks,  # FACEMESH_CONTOURS  FACEMESH_LIPS\r\n                connections=FACEMESH_FULL,\r\n                connection_drawing_spec=drawing_spec)  # landmark_drawing_spec=None,\r\n            annotated_image = cv2.resize(annotated_image, (128, 128))\r\n\r\n            out_dir = os.path.join(output_sketch_root, '/'.join(mp4_path[:-4].split('/')[-2:]))\r\n            os.makedirs(out_dir, exist_ok=True)\r\n            cv2.imwrite(os.path.join(out_dir, str(frame_idx)+'.png'), annotated_image)\r\n\r\n            #save face frame\r\n            out_dir = os.path.join(output_face_root, '/'.join(mp4_path[:-4].split('/')[-2:]))\r\n            os.makedirs(out_dir, exist_ok=True)\r\n            cv2.imwrite(os.path.join(out_dir, str(frame_idx) + '.png'), face_frame)\r\n\r\ndef mp_handler(mp4_path):\r\n    try:\r\n        process_video_file(mp4_path)\r\n    except KeyboardInterrupt:\r\n        exit(0)\r\n    except:\r\n        traceback.print_exc()\r\n\r\n\r\ndef main():\r\n    print('looking up videos.... ')\r\n    mp4_list = glob.glob(input_mp4_root + '/*/*.mp4')  #example: .../lrs2_video/5536038039829982468/00001.mp4\r\n    print('total videos :', len(mp4_list))\r\n\r\n    process_num = args.process_num\r\n    print('process_num: ', process_num)\r\n    p_frames = ThreadPoolExecutor(process_num)\r\n    futures_frames = [p_frames.submit(mp_handler, mp4_path) for mp4_path in mp4_list]\r\n    _ = [r.result() for r in tqdm(as_completed(futures_frames), total=len(futures_frames))]\r\n    print(\"complete task!\")\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\n"})}),"\n",(0,t.jsx)(e.h4,{id:"ap",children:"audio.py"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'import librosa\r\nimport librosa.filters\r\nimport numpy as np\r\nfrom scipy import signal\r\nfrom scipy.io import wavfile\r\nimport lws\r\n\r\nclass HParams:\r\n    def __init__(self, **kwargs):\r\n        self.data = {}\r\n\r\n        for key, value in kwargs.items():\r\n            self.data[key] = value\r\n\r\n    def __getattr__(self, key):\r\n        if key not in self.data:\r\n            raise AttributeError("\'HParams\' object has no attribute %s" % key)\r\n        return self.data[key]\r\n\r\n    def set_hparam(self, key, value):\r\n        self.data[key] = value\r\n\r\n\r\n# Default hyperparameters\r\nhp = HParams(\r\n    num_mels=80,  # Number of mel-spectrogram channels and local conditioning dimensionality\r\n    #  network\r\n    rescale=True,  # Whether to rescale audio prior to preprocessing\r\n    rescaling_max=0.9,  # Rescaling value\r\n\r\n    # Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\r\n    # It"s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\r\n    # Does not work if n_ffit is not multiple of hop_size!!\r\n    use_lws=False,\r\n\r\n    n_fft=800,  # Extra window size is filled with 0 paddings to match this parameter\r\n    hop_size=200,  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\r\n    win_size=800,  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\r\n    sample_rate=16000,  # 16000Hz (corresponding to librispeech) (sox --i <filename>)\r\n\r\n    frame_shift_ms=None,  # Can replace hop_size parameter. (Recommended: 12.5)\r\n\r\n    # Mel and Linear spectrograms normalization/scaling and clipping\r\n    signal_normalization=True,\r\n    # Whether to normalize mel spectrograms to some predefined range (following below parameters)\r\n    allow_clipping_in_normalization=True,  # Only relevant if mel_normalization = True\r\n    symmetric_mels=True,\r\n    # Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,\r\n    # faster and cleaner convergence)\r\n    max_abs_value=4.,\r\n    # max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not\r\n    # be too big to avoid gradient explosion,\r\n    # not too small for fast convergence)\r\n    # Contribution by @begeekmyfriend\r\n    # Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude\r\n    # levels. Also allows for better G&L phase reconstruction)\r\n    preemphasize=True,  # whether to apply filter\r\n    preemphasis=0.97,  # filter coefficient.\r\n\r\n    # Limits\r\n    min_level_db=-100,\r\n    ref_level_db=20,\r\n    fmin=55,\r\n    # Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To\r\n    # test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\r\n    fmax=7600,  # To be increased/reduced depending on data.\r\n\r\n    ###################### Our training parameters #################################\r\n    img_size=288,\r\n    fps=25,\r\n\r\n    batch_size=8,\r\n    initial_learning_rate=1e-4,\r\n    nepochs=200000000000000000,\r\n    ### ctrl + c, stop whenever eval loss is consistently greater than train loss for ~10 epochs\r\n    num_workers=4,\r\n    checkpoint_interval=6000,\r\n    eval_interval=6000,\r\n    save_optimizer_state=True,\r\n\r\n    syncnet_wt=0.0,  # is initially zero, will be set automatically to 0.03 later. Leads to faster convergence.\r\n    syncnet_batch_size=128,\r\n    syncnet_lr=1e-4,\r\n    syncnet_eval_interval=4500,\r\n    syncnet_checkpoint_interval=4500,\r\n\r\n    disc_wt=0.07,\r\n    disc_initial_learning_rate=1e-4,\r\n)\r\n\r\n\r\ndef load_wav(path, sr):\r\n    return librosa.core.load(path, sr=sr)[0]\r\n\r\n\r\ndef save_wav(wav, path, sr):\r\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\r\n    # proposed by @dsmiller\r\n    wavfile.write(path, sr, wav.astype(np.int16))\r\n\r\n\r\ndef save_wavenet_wav(wav, path, sr):\r\n    librosa.output.write_wav(path, wav, sr=sr)\r\n\r\n\r\ndef preemphasis(wav, k, preemphasize=True):\r\n    if preemphasize:\r\n        return signal.lfilter([1, -k], [1], wav)\r\n    return wav\r\n\r\n\r\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\r\n    if inv_preemphasize:\r\n        return signal.lfilter([1], [1, -k], wav)\r\n    return wav\r\n\r\n\r\ndef get_hop_size():\r\n    hop_size = hp.hop_size\r\n    if hop_size is None:\r\n        assert hp.frame_shift_ms is not None\r\n        hop_size = int(hp.frame_shift_ms / 1000 * hp.sample_rate)\r\n    return hop_size\r\n\r\n\r\ndef linearspectrogram(wav):\r\n    D = _stft(preemphasis(wav, hp.preemphasis, hp.preemphasize))\r\n    S = _amp_to_db(np.abs(D)) - hp.ref_level_db\r\n\r\n    if hp.signal_normalization:\r\n        return _normalize(S)\r\n    return S\r\n\r\n\r\ndef melspectrogram(wav):\r\n    D = _stft(preemphasis(wav, hp.preemphasis, hp.preemphasize))\r\n    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hp.ref_level_db\r\n\r\n    if hp.signal_normalization:\r\n        return _normalize(S)\r\n    return S\r\n\r\n\r\ndef _lws_processor():\r\n    return lws.lws(hp.n_fft, get_hop_size(), fftsize=hp.win_size, mode="speech")\r\n\r\n\r\ndef _stft(y):\r\n    if hp.use_lws:\r\n        return _lws_processor(hp).stft(y).T\r\n    else:\r\n        return librosa.stft(y=y, n_fft=hp.n_fft, hop_length=get_hop_size(), win_length=hp.win_size)\r\n\r\n\r\n##########################################################\r\n# Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\r\ndef num_frames(length, fsize, fshift):\r\n    """Compute number of time frames of spectrogram\r\n    """\r\n    pad = (fsize - fshift)\r\n    if length % fshift == 0:\r\n        M = (length + pad * 2 - fsize) // fshift + 1\r\n    else:\r\n        M = (length + pad * 2 - fsize) // fshift + 2\r\n    return M\r\n\r\n\r\ndef pad_lr(x, fsize, fshift):\r\n    """Compute left and right padding\r\n    """\r\n    M = num_frames(len(x), fsize, fshift)\r\n    pad = (fsize - fshift)\r\n    T = len(x) + 2 * pad\r\n    r = (M - 1) * fshift + fsize - T\r\n    return pad, pad + r\r\n\r\n\r\n##########################################################\r\n# Librosa correct padding\r\ndef librosa_pad_lr(x, fsize, fshift):\r\n    return 0, (x.shape[0] // fshift + 1) * fshift - x.shape[0]\r\n\r\n\r\n# Conversions\r\n_mel_basis = None\r\n\r\n\r\ndef _linear_to_mel(spectogram):\r\n    global _mel_basis\r\n    if _mel_basis is None:\r\n        _mel_basis = _build_mel_basis()\r\n    return np.dot(_mel_basis, spectogram)\r\n\r\n\r\ndef _build_mel_basis():\r\n    assert hp.fmax <= hp.sample_rate // 2\r\n    return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\r\n                               fmin=hp.fmin, fmax=hp.fmax)\r\n\r\n\r\ndef _amp_to_db(x):\r\n    min_level = np.exp(hp.min_level_db / 20 * np.log(10))\r\n    return 20 * np.log10(np.maximum(min_level, x))\r\n\r\n\r\ndef _db_to_amp(x):\r\n    return np.power(10.0, (x) * 0.05)\r\n\r\n\r\ndef _normalize(S):\r\n    if hp.allow_clipping_in_normalization:\r\n        if hp.symmetric_mels:\r\n            return np.clip((2 * hp.max_abs_value) * ((S - hp.min_level_db) / (-hp.min_level_db)) - hp.max_abs_value,\r\n                           -hp.max_abs_value, hp.max_abs_value)\r\n        else:\r\n            return np.clip(hp.max_abs_value * ((S - hp.min_level_db) / (-hp.min_level_db)), 0, hp.max_abs_value)\r\n\r\n    assert S.max() <= 0 and S.min() - hp.min_level_db >= 0\r\n    if hp.symmetric_mels:\r\n        return (2 * hp.max_abs_value) * ((S - hp.min_level_db) / (-hp.min_level_db)) - hp.max_abs_value\r\n    else:\r\n        return hp.max_abs_value * ((S - hp.min_level_db) / (-hp.min_level_db))\r\n\r\n\r\ndef _denormalize(D):\r\n    if hp.allow_clipping_in_normalization:\r\n        if hp.symmetric_mels:\r\n            return (((np.clip(D, -hp.max_abs_value,\r\n                              hp.max_abs_value) + hp.max_abs_value) * -hp.min_level_db / (2 * hp.max_abs_value))\r\n                    + hp.min_level_db)\r\n        else:\r\n            return ((np.clip(D, 0, hp.max_abs_value) * -hp.min_level_db / hp.max_abs_value) + hp.min_level_db)\r\n\r\n    if hp.symmetric_mels:\r\n        return (((D + hp.max_abs_value) * -hp.min_level_db / (2 * hp.max_abs_value)) + hp.min_level_db)\r\n    else:\r\n        return ((D * -hp.min_level_db / hp.max_abs_value) + hp.min_level_db)\r\n\n'})}),"\n",(0,t.jsx)(e.h4,{id:"gl",children:"\u751f\u6210landmark"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'import librosa\r\nimport librosa.filters\r\nimport numpy as np\r\nfrom scipy import signal\r\nfrom scipy.io import wavfile\r\nimport lws\r\n\r\nclass HParams:\r\n    def __init__(self, **kwargs):\r\n        self.data = {}\r\n\r\n        for key, value in kwargs.items():\r\n            self.data[key] = value\r\n\r\n    def __getattr__(self, key):\r\n        if key not in self.data:\r\n            raise AttributeError("\'HParams\' object has no attribute %s" % key)\r\n        return self.data[key]\r\n\r\n    def set_hparam(self, key, value):\r\n        self.data[key] = value\r\n\r\n\r\n# Default hyperparameters\r\nhp = HParams(\r\n    num_mels=80,  # Number of mel-spectrogram channels and local conditioning dimensionality\r\n    #  network\r\n    rescale=True,  # Whether to rescale audio prior to preprocessing\r\n    rescaling_max=0.9,  # Rescaling value\r\n\r\n    # Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\r\n    # It"s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\r\n    # Does not work if n_ffit is not multiple of hop_size!!\r\n    use_lws=False,\r\n\r\n    n_fft=800,  # Extra window size is filled with 0 paddings to match this parameter\r\n    hop_size=200,  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\r\n    win_size=800,  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\r\n    sample_rate=16000,  # 16000Hz (corresponding to librispeech) (sox --i <filename>)\r\n\r\n    frame_shift_ms=None,  # Can replace hop_size parameter. (Recommended: 12.5)\r\n\r\n    # Mel and Linear spectrograms normalization/scaling and clipping\r\n    signal_normalization=True,\r\n    # Whether to normalize mel spectrograms to some predefined range (following below parameters)\r\n    allow_clipping_in_normalization=True,  # Only relevant if mel_normalization = True\r\n    symmetric_mels=True,\r\n    # Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,\r\n    # faster and cleaner convergence)\r\n    max_abs_value=4.,\r\n    # max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not\r\n    # be too big to avoid gradient explosion,\r\n    # not too small for fast convergence)\r\n    # Contribution by @begeekmyfriend\r\n    # Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude\r\n    # levels. Also allows for better G&L phase reconstruction)\r\n    preemphasize=True,  # whether to apply filter\r\n    preemphasis=0.97,  # filter coefficient.\r\n\r\n    # Limits\r\n    min_level_db=-100,\r\n    ref_level_db=20,\r\n    fmin=55,\r\n    # Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To\r\n    # test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\r\n    fmax=7600,  # To be increased/reduced depending on data.\r\n\r\n    ###################### Our training parameters #################################\r\n    img_size=288,\r\n    fps=25,\r\n\r\n    batch_size=8,\r\n    initial_learning_rate=1e-4,\r\n    nepochs=200000000000000000,\r\n    ### ctrl + c, stop whenever eval loss is consistently greater than train loss for ~10 epochs\r\n    num_workers=4,\r\n    checkpoint_interval=6000,\r\n    eval_interval=6000,\r\n    save_optimizer_state=True,\r\n\r\n    syncnet_wt=0.0,  # is initially zero, will be set automatically to 0.03 later. Leads to faster convergence.\r\n    syncnet_batch_size=128,\r\n    syncnet_lr=1e-4,\r\n    syncnet_eval_interval=4500,\r\n    syncnet_checkpoint_interval=4500,\r\n\r\n    disc_wt=0.07,\r\n    disc_initial_learning_rate=1e-4,\r\n)\r\n\r\n\r\ndef load_wav(path, sr):\r\n    return librosa.core.load(path, sr=sr)[0]\r\n\r\n\r\ndef save_wav(wav, path, sr):\r\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\r\n    # proposed by @dsmiller\r\n    wavfile.write(path, sr, wav.astype(np.int16))\r\n\r\n\r\ndef save_wavenet_wav(wav, path, sr):\r\n    librosa.output.write_wav(path, wav, sr=sr)\r\n\r\n\r\ndef preemphasis(wav, k, preemphasize=True):\r\n    if preemphasize:\r\n        return signal.lfilter([1, -k], [1], wav)\r\n    return wav\r\n\r\n\r\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\r\n    if inv_preemphasize:\r\n        return signal.lfilter([1], [1, -k], wav)\r\n    return wav\r\n\r\n\r\ndef get_hop_size():\r\n    hop_size = hp.hop_size\r\n    if hop_size is None:\r\n        assert hp.frame_shift_ms is not None\r\n        hop_size = int(hp.frame_shift_ms / 1000 * hp.sample_rate)\r\n    return hop_size\r\n\r\n\r\ndef linearspectrogram(wav):\r\n    D = _stft(preemphasis(wav, hp.preemphasis, hp.preemphasize))\r\n    S = _amp_to_db(np.abs(D)) - hp.ref_level_db\r\n\r\n    if hp.signal_normalization:\r\n        return _normalize(S)\r\n    return S\r\n\r\n\r\ndef melspectrogram(wav):\r\n    D = _stft(preemphasis(wav, hp.preemphasis, hp.preemphasize))\r\n    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hp.ref_level_db\r\n\r\n    if hp.signal_normalization:\r\n        return _normalize(S)\r\n    return S\r\n\r\n\r\ndef _lws_processor():\r\n    return lws.lws(hp.n_fft, get_hop_size(), fftsize=hp.win_size, mode="speech")\r\n\r\n\r\ndef _stft(y):\r\n    if hp.use_lws:\r\n        return _lws_processor(hp).stft(y).T\r\n    else:\r\n        return librosa.stft(y=y, n_fft=hp.n_fft, hop_length=get_hop_size(), win_length=hp.win_size)\r\n\r\n\r\n##########################################################\r\n# Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\r\ndef num_frames(length, fsize, fshift):\r\n    """Compute number of time frames of spectrogram\r\n    """\r\n    pad = (fsize - fshift)\r\n    if length % fshift == 0:\r\n        M = (length + pad * 2 - fsize) // fshift + 1\r\n    else:\r\n        M = (length + pad * 2 - fsize) // fshift + 2\r\n    return M\r\n\r\n\r\ndef pad_lr(x, fsize, fshift):\r\n    """Compute left and right padding\r\n    """\r\n    M = num_frames(len(x), fsize, fshift)\r\n    pad = (fsize - fshift)\r\n    T = len(x) + 2 * pad\r\n    r = (M - 1) * fshift + fsize - T\r\n    return pad, pad + r\r\n\r\n\r\n##########################################################\r\n# Librosa correct padding\r\ndef librosa_pad_lr(x, fsize, fshift):\r\n    return 0, (x.shape[0] // fshift + 1) * fshift - x.shape[0]\r\n\r\n\r\n# Conversions\r\n_mel_basis = None\r\n\r\n\r\ndef _linear_to_mel(spectogram):\r\n    global _mel_basis\r\n    if _mel_basis is None:\r\n        _mel_basis = _build_mel_basis()\r\n    return np.dot(_mel_basis, spectogram)\r\n\r\n\r\ndef _build_mel_basis():\r\n    assert hp.fmax <= hp.sample_rate // 2\r\n    return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\r\n                               fmin=hp.fmin, fmax=hp.fmax)\r\n\r\n\r\ndef _amp_to_db(x):\r\n    min_level = np.exp(hp.min_level_db / 20 * np.log(10))\r\n    return 20 * np.log10(np.maximum(min_level, x))\r\n\r\n\r\ndef _db_to_amp(x):\r\n    return np.power(10.0, (x) * 0.05)\r\n\r\n\r\ndef _normalize(S):\r\n    if hp.allow_clipping_in_normalization:\r\n        if hp.symmetric_mels:\r\n            return np.clip((2 * hp.max_abs_value) * ((S - hp.min_level_db) / (-hp.min_level_db)) - hp.max_abs_value,\r\n                           -hp.max_abs_value, hp.max_abs_value)\r\n        else:\r\n            return np.clip(hp.max_abs_value * ((S - hp.min_level_db) / (-hp.min_level_db)), 0, hp.max_abs_value)\r\n\r\n    assert S.max() <= 0 and S.min() - hp.min_level_db >= 0\r\n    if hp.symmetric_mels:\r\n        return (2 * hp.max_abs_value) * ((S - hp.min_level_db) / (-hp.min_level_db)) - hp.max_abs_value\r\n    else:\r\n        return hp.max_abs_value * ((S - hp.min_level_db) / (-hp.min_level_db))\r\n\r\n\r\ndef _denormalize(D):\r\n    if hp.allow_clipping_in_normalization:\r\n        if hp.symmetric_mels:\r\n            return (((np.clip(D, -hp.max_abs_value,\r\n                              hp.max_abs_value) + hp.max_abs_value) * -hp.min_level_db / (2 * hp.max_abs_value))\r\n                    + hp.min_level_db)\r\n        else:\r\n            return ((np.clip(D, 0, hp.max_abs_value) * -hp.min_level_db / hp.max_abs_value) + hp.min_level_db)\r\n\r\n    if hp.symmetric_mels:\r\n        return (((D + hp.max_abs_value) * -hp.min_level_db / (2 * hp.max_abs_value)) + hp.min_level_db)\r\n    else:\r\n        return ((D * -hp.min_level_db / hp.max_abs_value) + hp.min_level_db)\r\n\n'})}),"\n",(0,t.jsx)(e.h4,{id:"gp",children:"\u751f\u6210pixmap"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import torch\r\nimport torch.nn as nn\r\nimport functools\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\n\r\ndef weights_init(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find('Conv') != -1:\r\n        m.weight.data.normal_(0.0, 0.02)\r\n    elif classname.find('BatchNorm2d') != -1:\r\n        m.weight.data.normal_(1.0, 0.02)\r\n        m.bias.data.fill_(0)\r\n\r\n\r\ndef define_D(input_nc=3, ndf=64, n_layers_D=3, norm='instance', use_sigmoid=False, num_D=2, getIntermFeat=True):\r\n    #('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')\r\n    #('--input_nc', type=int, default=3, help='# of input image channels')\r\n    #('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\r\n   # ('--num_D', type=int, default=2, help='number of discriminators to use')\r\n\r\n    norm_layer = get_norm_layer(norm_type=norm)\r\n    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)\r\n    #print(netD)\r\n    netD.apply(weights_init)\r\n    return netD\r\n\r\n\r\nclass NLayerDiscriminator(nn.Module):\r\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\r\n        super(NLayerDiscriminator, self).__init__()\r\n        self.getIntermFeat = getIntermFeat\r\n        self.n_layers = n_layers\r\n\r\n        kw = 4\r\n        padw = int(np.ceil((kw-1.0)/2))\r\n        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\r\n\r\n        nf = ndf\r\n        for n in range(1, n_layers):\r\n            nf_prev = nf\r\n            nf = min(nf * 2, 512)\r\n            sequence += [[\r\n                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\r\n                norm_layer(nf), nn.LeakyReLU(0.2, True)\r\n            ]]\r\n\r\n        nf_prev = nf\r\n        nf = min(nf * 2, 512)\r\n        sequence += [[\r\n            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\r\n            norm_layer(nf),\r\n            nn.LeakyReLU(0.2, True)\r\n        ]]\r\n\r\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\r\n\r\n        if use_sigmoid:\r\n            sequence += [[nn.Sigmoid()]]\r\n\r\n        if getIntermFeat:\r\n            for n in range(len(sequence)):\r\n                setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\r\n        else:\r\n            sequence_stream = []\r\n            for n in range(len(sequence)):\r\n                sequence_stream += sequence[n]\r\n            self.model = nn.Sequential(*sequence_stream)\r\n\r\n    def forward(self, input):\r\n\r\n        if self.getIntermFeat:\r\n            res = [input]\r\n            for n in range(self.n_layers+2):\r\n                model = getattr(self, 'model'+str(n))\r\n                res.append(model(res[-1]))\r\n            return res[1:]\r\n        else:\r\n            return self.model(input)\r\n\r\n\r\ndef get_norm_layer(norm_type='instance'):\r\n    if norm_type == 'batch':\r\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n    elif norm_type == 'instance':\r\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\r\n    else:\r\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n    return norm_layer\r\n\r\n\r\n\r\n\r\nclass MultiscaleDiscriminator(nn.Module):\r\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d,\r\n                 use_sigmoid=False, num_D=3, getIntermFeat=False):\r\n        super(MultiscaleDiscriminator, self).__init__()\r\n        self.num_D = num_D\r\n        self.n_layers = n_layers\r\n        self.getIntermFeat = getIntermFeat\r\n\r\n        for i in range(num_D):\r\n            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\r\n            if getIntermFeat:\r\n                for j in range(n_layers + 2):\r\n                    setattr(self, 'scale' + str(i) + '_layer' + str(j), getattr(netD, 'model' + str(j)))\r\n            else:\r\n                setattr(self, 'layer' + str(i), netD.model)\r\n\r\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\r\n\r\n    def singleD_forward(self, model, input):\r\n        if self.getIntermFeat:\r\n            result = [input]\r\n            for i in range(len(model)):\r\n                result.append(model[i](result[-1]))\r\n            return result[1:]\r\n        else:\r\n            return [model(input)]\r\n\r\n    def forward(self, input): #: (B,T,C,H,W)\r\n        # input = torch.cat([input[i,:] for i in range(input.size(0))], dim=0)# : (B*T,C,H,W)\r\n        num_D = self.num_D\r\n        result = []\r\n        input_downsampled = input\r\n        for i in range(num_D):\r\n            if self.getIntermFeat:\r\n                model = [getattr(self, 'scale' + str(num_D - 1 - i) + '_layer' + str(j)) for j in\r\n                         range(self.n_layers + 2)]\r\n            else:\r\n                model = getattr(self, 'layer' + str(num_D - 1 - i))\r\n            result.append(self.singleD_forward(model, input_downsampled))\r\n            if i != (num_D - 1):\r\n                input_downsampled = self.downsample(input_downsampled)\r\n        return result\n"})}),"\n",(0,t.jsx)(e.h4,{id:"gv",children:"\u751f\u6210\u89c6\u9891"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"from torch.nn import functional as F\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchvision\r\n\r\n\r\n\r\nclass AdaINLayer(nn.Module):\r\n    def __init__(self, input_nc, modulation_nc):\r\n        super().__init__()\r\n\r\n        self.InstanceNorm2d = nn.InstanceNorm2d(input_nc, affine=False)\r\n\r\n        nhidden = 128\r\n        use_bias=True\r\n\r\n        self.mlp_shared = nn.Sequential(\r\n            nn.Linear(modulation_nc, nhidden, bias=use_bias),\r\n            nn.ReLU()\r\n        )\r\n        self.mlp_gamma = nn.Linear(nhidden, input_nc, bias=use_bias)\r\n        self.mlp_beta = nn.Linear(nhidden, input_nc, bias=use_bias)\r\n\r\n    def forward(self, input, modulation_input):\r\n\r\n        # Part 1. generate parameter-free normalized activations\r\n        normalized = self.InstanceNorm2d(input)\r\n\r\n        # Part 2. produce scaling and bias conditioned on feature\r\n        modulation_input = modulation_input.view(modulation_input.size(0), -1)#B 512\r\n        actv = self.mlp_shared(modulation_input)# b 128\r\n        gamma = self.mlp_gamma(actv)# b 128\r\n        beta = self.mlp_beta(actv)# 128\r\n\r\n        # apply scale and bias\r\n        gamma = gamma.view(*gamma.size()[:2], 1,1)\r\n        beta = beta.view(*beta.size()[:2], 1,1)\r\n        out = normalized * (1 + gamma) + beta\r\n        return out\r\n\r\nclass AdaIN(torch.nn.Module):\r\n\r\n    def __init__(self, input_channel, modulation_channel,kernel_size=3, stride=1, padding=1):\r\n        super(AdaIN, self).__init__()\r\n        self.conv_1 = torch.nn.Conv2d(input_channel, input_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        self.conv_2 = torch.nn.Conv2d(input_channel, input_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        self.leaky_relu = torch.nn.LeakyReLU(0.2)\r\n        self.adain_layer_1 = AdaINLayer(input_channel, modulation_channel)\r\n        self.adain_layer_2 = AdaINLayer(input_channel, modulation_channel)\r\n\r\n    def forward(self, x, modulation):\r\n\r\n        x = self.adain_layer_1(x, modulation)\r\n        x = self.leaky_relu(x)\r\n        x = self.conv_1(x)\r\n        x = self.adain_layer_2(x, modulation)\r\n        x = self.leaky_relu(x)\r\n        x = self.conv_2(x)\r\n\r\n        return x\r\n\r\n\r\n\r\n\r\nclass SPADELayer(torch.nn.Module):\r\n    def __init__(self, input_channel, modulation_channel, hidden_size=256, kernel_size=3, stride=1, padding=1):\r\n        super(SPADELayer, self).__init__()\r\n        self.instance_norm = torch.nn.InstanceNorm2d(input_channel)\r\n\r\n        self.conv1 = torch.nn.Conv2d(modulation_channel, hidden_size, kernel_size=kernel_size, stride=stride,\r\n                                     padding=padding)\r\n        self.gamma = torch.nn.Conv2d(hidden_size, input_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        self.beta = torch.nn.Conv2d(hidden_size, input_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n\r\n    def forward(self, input, modulation):\r\n        norm = self.instance_norm(input)\r\n\r\n        conv_out = self.conv1(modulation)\r\n\r\n        gamma = self.gamma(conv_out)\r\n        beta = self.beta(conv_out)\r\n\r\n        return norm + norm * gamma + beta\r\n\r\n\r\nclass SPADE(torch.nn.Module):\r\n    def __init__(self, num_channel, num_channel_modulation, hidden_size=256, kernel_size=3, stride=1, padding=1):\r\n        super(SPADE, self).__init__()\r\n        self.conv_1 = torch.nn.Conv2d(num_channel, num_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        self.conv_2 = torch.nn.Conv2d(num_channel, num_channel, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        self.leaky_relu = torch.nn.LeakyReLU(0.2)\r\n        self.spade_layer_1 = SPADELayer(num_channel, num_channel_modulation, hidden_size, kernel_size=kernel_size,\r\n                                        stride=stride, padding=padding)\r\n        self.spade_layer_2 = SPADELayer(num_channel, num_channel_modulation, hidden_size, kernel_size=kernel_size,\r\n                                        stride=stride, padding=padding)\r\n\r\n    def forward(self, input, modulations):\r\n        input = self.spade_layer_1(input, modulations)\r\n        input = self.leaky_relu(input)\r\n        input = self.conv_1(input)\r\n        input = self.spade_layer_2(input, modulations)\r\n        input = self.leaky_relu(input)\r\n        input = self.conv_2(input)\r\n        return input\r\n\r\nclass Conv2d(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\r\n                            nn.BatchNorm2d(cout)\r\n                            )\r\n        self.act = nn.ReLU()\r\n        self.residual = residual\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        if self.residual:\r\n            out += x\r\n        return self.act(out)\r\n\r\ndef downsample(x, size):\r\n    if len(x.size()) == 5:\r\n        size = (x.size(2), size[0], size[1])\r\n        return torch.nn.functional.interpolate(x, size=size, mode='nearest')\r\n    return  torch.nn.functional.interpolate(x, size=size, mode='nearest')\r\n\r\n\r\ndef convert_flow_to_deformation(flow):\r\n    r\"\"\"convert flow fields to deformations.\r\n    Args:\r\n        flow (tensor): Flow field obtained by the model\r\n    Returns:\r\n        deformation (tensor): The deformation used for warpping\r\n    \"\"\"\r\n    b, c, h, w = flow.shape\r\n    flow_norm = 2 * torch.cat([flow[:, :1, ...] / (w - 1), flow[:, 1:, ...] / (h - 1)], 1)\r\n    grid = make_coordinate_grid(flow)\r\n    deformation = grid + flow_norm.permute(0, 2, 3, 1) #(B*T,128,128,2)\r\n    return deformation\r\n\r\n\r\ndef make_coordinate_grid(flow):\r\n    r\"\"\"obtain coordinate grid with the same size as the flow filed.\r\n    Args:\r\n        flow (tensor): Flow field obtained by the model\r\n    Returns:\r\n        grid (tensor): The grid with the same size as the input flow\r\n    \"\"\"\r\n    b, c, h, w = flow.shape\r\n\r\n    x = torch.arange(w).to(flow)\r\n    y = torch.arange(h).to(flow)\r\n\r\n    x = (2 * (x / (w - 1)) - 1)\r\n    y = (2 * (y / (h - 1)) - 1)\r\n\r\n    yy = y.view(-1, 1).repeat(1, w)\r\n    xx = x.view(1, -1).repeat(h, 1)\r\n\r\n    meshed = torch.cat([xx.unsqueeze_(2), yy.unsqueeze_(2)], 2)\r\n    meshed = meshed.expand(b, -1, -1, -1)\r\n    return meshed\r\n\r\n\r\ndef warping(source_image, deformation):\r\n    r\"\"\"warp the input image according to the deformation\r\n    Args:\r\n        source_image (tensor): source images to be warpped\r\n        deformation (tensor): deformations used to warp the images; value in range (-1, 1)\r\n    Returns:\r\n        output (tensor): the warpped images\r\n    \"\"\"\r\n    _, h_old, w_old, _ = deformation.shape # B 128 128 2\r\n    _, _, h, w = source_image.shape# B 32 128 128\r\n    if h_old != h or w_old != w:\r\n        deformation = deformation.permute(0, 3, 1, 2) #B 2 128 128\r\n        deformation = torch.nn.functional.interpolate(deformation, size=(h, w), mode='bilinear')\r\n        deformation = deformation.permute(0, 2, 3, 1)\r\n    return torch.nn.functional.grid_sample(source_image, deformation)\r\n\r\n\r\nclass DenseFlowNetwork(torch.nn.Module):\r\n    def __init__(self, num_channel=6, num_channel_modulation=3*5, hidden_size=256):\r\n        super(DenseFlowNetwork, self).__init__()\r\n\r\n        # Convolutional Layers\r\n        self.conv1 = torch.nn.Conv2d(num_channel, 32, kernel_size=7, stride=1, padding=3)\r\n        self.conv1_bn = torch.nn.BatchNorm2d(num_features=32, affine=True)\r\n        self.conv1_relu = torch.nn.ReLU()\r\n\r\n        self.conv2 = torch.nn.Conv2d(32, 256, kernel_size=3, stride=2, padding=1)\r\n        self.conv2_bn = torch.nn.BatchNorm2d(num_features=256, affine=True)\r\n        self.conv2_relu = torch.nn.ReLU()\r\n\r\n\r\n        # SPADE Blocks\r\n        self.spade_layer_1 = SPADE(256, num_channel_modulation, hidden_size)\r\n        self.spade_layer_2 = SPADE(256, num_channel_modulation, hidden_size)\r\n        self.pixel_shuffle_1 = torch.nn.PixelShuffle(2)\r\n        self.spade_layer_4 = SPADE(64, num_channel_modulation, hidden_size)\r\n\r\n        # Final Convolutional Layer\r\n        self.conv_4 = torch.nn.Conv2d(64, 2, kernel_size=7, stride=1, padding=3)\r\n        self.conv_5= nn.Sequential(torch.nn.Conv2d(64, 32, kernel_size=7, stride=1, padding=3),\r\n                                   torch.nn.ReLU(),\r\n                                   torch.nn.Conv2d(32, 1, kernel_size=7, stride=1, padding=3),\r\n                                   torch.nn.Sigmoid(),\r\n                                   )#predict weight\r\n\r\n    def forward(self, ref_N_frame_img, ref_N_frame_sketch, T_driving_sketch): #to output: (B*T,3,H,W)\r\n                   #   (B, N, 3, H, W)(B, N, 3, H, W)    (B, 5, 3, H, W)  #\r\n        ref_N = ref_N_frame_img.size(1)\r\n\r\n        driving_sketch=torch.cat([T_driving_sketch[:,i] for i in range(T_driving_sketch.size(1))], dim=1)  #(B, 3*5, H, W)\r\n\r\n        wrapped_h1_sum, wrapped_h2_sum, wrapped_ref_sum=0.,0.,0.\r\n        softmax_denominator=0.\r\n        T = 1  # during rendering, generate T=1 image  at a time\r\n        for ref_idx in range(ref_N): # each ref img provide information for each B*T frame\r\n            ref_img= ref_N_frame_img[:, ref_idx]  #(B, 3, H, W)\r\n            ref_img = ref_img.unsqueeze(1).expand(-1, T, -1, -1, -1)  # (B,T, 3, H, W)\r\n            ref_img = torch.cat([ref_img[i] for i in range(ref_img.size(0))], dim=0)  # (B*T, 3, H, W)\r\n\r\n            ref_sketch = ref_N_frame_sketch[:, ref_idx] #(B, 3, H, W)\r\n            ref_sketch = ref_sketch.unsqueeze(1).expand(-1, T, -1, -1, -1)  # (B,T, 3, H, W)\r\n            ref_sketch = torch.cat([ref_sketch[i] for i in range(ref_sketch.size(0))], dim=0)  # (B*T, 3, H, W)\r\n\r\n            #predict flow and weight\r\n            flow_module_input = torch.cat((ref_img, ref_sketch), dim=1)  #(B*T, 3+3, H, W)\r\n            # Convolutional Layers\r\n            h1 = self.conv1_relu(self.conv1_bn(self.conv1(flow_module_input)))   #(32,128,128)\r\n            h2 = self.conv2_relu(self.conv2_bn(self.conv2(h1)))    #(256,64,64)\r\n            # SPADE Blocks\r\n            downsample_64 = downsample(driving_sketch, (64, 64))   # driving_sketch:(B*T, 3, H, W) B*T 3 64 64\r\n\r\n            spade_layer = self.spade_layer_1(h2, downsample_64)  #(256,64,64)\r\n            spade_layer = self.spade_layer_2(spade_layer, downsample_64)   #(256,64,64)\r\n\r\n            spade_layer = self.pixel_shuffle_1(spade_layer)   #(64,128,128)\r\n\r\n            spade_layer = self.spade_layer_4(spade_layer, driving_sketch)    #(64,128,128)\r\n\r\n            # Final Convolutional Layer\r\n            output_flow = self.conv_4(spade_layer)      #   (B*T,2,128,128)\r\n            output_weight=self.conv_5(spade_layer)       #  (B*T,1,128,128)\r\n\r\n            deformation=convert_flow_to_deformation(output_flow) # (B*T 128 128 2)\r\n            wrapped_h1 = warping(h1, deformation)  #(32,128,128)\r\n            wrapped_h2 = warping(h2, deformation)   #(256,64,64)\r\n            wrapped_ref = warping(ref_img, deformation)  #(3,128,128)\r\n\r\n            softmax_denominator+=output_weight\r\n            wrapped_h1_sum+=wrapped_h1*output_weight\r\n            wrapped_h2_sum+=wrapped_h2*downsample(output_weight, (64,64))\r\n            wrapped_ref_sum+=wrapped_ref*output_weight\r\n        #return weighted warped feataure and images\r\n        softmax_denominator+=0.00001\r\n        wrapped_h1_sum=wrapped_h1_sum/softmax_denominator\r\n        wrapped_h2_sum = wrapped_h2_sum / downsample(softmax_denominator, (64,64))\r\n        wrapped_ref_sum = wrapped_ref_sum / softmax_denominator\r\n        return wrapped_h1_sum, wrapped_h2_sum, wrapped_ref_sum\r\n\r\n\r\nclass TranslationNetwork(torch.nn.Module):\r\n    def __init__(self):\r\n        super(TranslationNetwork, self).__init__()\r\n        self.audio_encoder = nn.Sequential(\r\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\r\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0), )\r\n\r\n        # Encoder\r\n        self.conv1 = torch.nn.Conv2d(in_channels=3+3*5, out_channels=32, kernel_size=7, stride=1, padding=3, bias=False)\r\n        self.conv1_bn = torch.nn.BatchNorm2d(num_features=32, affine=True)\r\n        self.conv1_relu = torch.nn.ReLU()\r\n\r\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=256, kernel_size=3, stride=2, padding=1, bias=False)\r\n        self.conv2_bn = torch.nn.BatchNorm2d(num_features=256, affine=True)\r\n        self.conv2_relu = torch.nn.ReLU()\r\n\r\n        # Decoder\r\n        self.spade_1 = SPADE(num_channel=256, num_channel_modulation=256)\r\n        self.adain_1 = AdaIN(256,512)\r\n        self.pixel_suffle_1 = nn.PixelShuffle(upscale_factor=2)\r\n\r\n        self.spade_2 = SPADE(num_channel=64, num_channel_modulation=32)\r\n        self.adain_2 = AdaIN(input_channel=64,modulation_channel=512)\r\n\r\n        self.spade_4 = SPADE(num_channel=64, num_channel_modulation=3)\r\n\r\n        # Final layer\r\n        self.leaky_relu = torch.nn.LeakyReLU()\r\n        self.conv_last = torch.nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3, bias=False)\r\n        self.Sigmoid=torch.nn.Sigmoid()\r\n    def forward(self, translation_input, wrapped_ref, wrapped_h1, wrapped_h2, T_mels):\r\n        #              (B,3+3*5,H,W)   (B,3,128,128)  (B,32,128,128) (B,256,64,64) (B,T,1,h,w)  #T=1\r\n        # Encoder\r\n        T_mels=torch.cat([T_mels[i] for i in range(T_mels.size(0))],dim=0)# B*T,1,h,w\r\n        x = self.conv1_relu(self.conv1_bn(self.conv1(translation_input)))    #32,128,128\r\n        x = self.conv2_relu(self.conv2_bn(self.conv2(x)))  #256,64,64\r\n\r\n        audio_feature = self.audio_encoder(T_mels).squeeze(-1).permute(0,2,1) #(B*T,1,512)\r\n\r\n        # Decoder\r\n        x = self.spade_1(x, wrapped_h2) # (C=256,64,64)\r\n        x = self.adain_1(x, audio_feature)  # (C=256,64,64)\r\n        x = self.pixel_suffle_1(x)   # (C=64,128,128)\r\n\r\n        x = self.spade_2(x, wrapped_h1)   # (64,128,128)\r\n        x = self.adain_2(x, audio_feature)  # (64,128,128)\r\n        x = self.spade_4(x, wrapped_ref)    # (64,128,128)\r\n\r\n        # output layer\r\n        x = self.leaky_relu(x)\r\n        x = self.conv_last(x)\r\n        x = self.Sigmoid(x)\r\n        return x\r\n\r\nclass Renderer(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Renderer, self).__init__()\r\n\r\n        # 1.flow Network\r\n        self.flow_module = DenseFlowNetwork()\r\n        #2. translation Network\r\n        self.translation = TranslationNetwork()\r\n        #3.return loss\r\n        self.perceptual = PerceptualLoss(network='vgg19',\r\n                                         layers=['relu_1_1', 'relu_2_1', 'relu_3_1', 'relu_4_1', 'relu_5_1'],\r\n                                         num_scales=2)\r\n\r\n    def forward(self, face_frame_img, target_sketches, ref_N_frame_img, ref_N_frame_sketch, audio_mels): #T=1\r\n        #            (B,1,3,H,W)   (B,5,3,H,W)       (B,N,3,H,W)   (B,N,3,H,W)  (B,T,1,hv,wv)T=1\r\n        # (1)warping reference images and their feature\r\n        wrapped_h1, wrapped_h2, wrapped_ref = self.flow_module(ref_N_frame_img, ref_N_frame_sketch, target_sketches)\r\n        #(B,C,H,W)\r\n\r\n        # (2)translation module\r\n        target_sketches = torch.cat([target_sketches[:, i] for i in range(target_sketches.size(1))], dim=1)\r\n        # (B,3*T,H,W)\r\n        gt_face = torch.cat([face_frame_img[i] for i in range(face_frame_img.size(0))], dim=0)\r\n        # (B,3,H,W)\r\n        gt_mask_face = gt_face.clone()\r\n        gt_mask_face[:, :, gt_mask_face.size(2) // 2:, :] = 0  # (B,3,H,W)\r\n        #\r\n        translation_input=torch.cat([gt_mask_face, target_sketches], dim=1) #  (B,3+3*5,H,W)\r\n        generated_face = self.translation(translation_input, wrapped_ref, wrapped_h1, wrapped_h2, audio_mels) #translation_input\r\n\r\n        perceptual_gen_loss = self.perceptual(generated_face, gt_face, use_style_loss=True,\r\n                                              weight_style_to_perceptual=250).mean()\r\n        perceptual_warp_loss = self.perceptual(wrapped_ref, gt_face, use_style_loss=False,\r\n                                               weight_style_to_perceptual=0.).mean()\r\n        return generated_face, wrapped_ref, torch.unsqueeze(perceptual_warp_loss, 0), torch.unsqueeze(\r\n            perceptual_gen_loss, 0)\r\n        # (B,3,H,W) and losses\r\n\r\n#the following is the code for Perceptual(VGG) loss\r\n\r\ndef apply_imagenet_normalization(input):\r\n    r\"\"\"Normalize using ImageNet mean and std.\r\n\r\n    Args:\r\n        input (4D tensor NxCxHxW): The input images, assuming to be [-1, 1].\r\n\r\n    Returns:\r\n        Normalized inputs using the ImageNet normalization.\r\n    \"\"\"\r\n    # normalize the input back to [0, 1]\r\n    normalized_input = (input + 1) / 2\r\n    # normalize the input using the ImageNet mean and std\r\n    mean = normalized_input.new_tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\r\n    std = normalized_input.new_tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\r\n    output = (normalized_input - mean) / std\r\n    return output\r\n\r\nclass _PerceptualNetwork(nn.Module):\r\n    r\"\"\"The network that extracts features to compute the perceptual loss.\r\n\r\n    Args:\r\n        network (nn.Sequential) : The network that extracts features.\r\n        layer_name_mapping (dict) : The dictionary that\r\n            maps a layer's index to its name.\r\n        layers (list of str): The list of layer names that we are using.\r\n    \"\"\"\r\n\r\n    def __init__(self, network, layer_name_mapping, layers):\r\n        super().__init__()\r\n        assert isinstance(network, nn.Sequential), \\\r\n            'The network needs to be of type \"nn.Sequential\".'\r\n        self.network = network\r\n        self.layer_name_mapping = layer_name_mapping\r\n        self.layers = layers\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n    def forward(self, x):\r\n        r\"\"\"Extract perceptual features.\"\"\"\r\n        output = {}\r\n        for i, layer in enumerate(self.network):\r\n            x = layer(x)\r\n            layer_name = self.layer_name_mapping.get(i, None)\r\n            if layer_name in self.layers:\r\n                # If the current layer is used by the perceptual loss.\r\n                output[layer_name] = x\r\n        return output\r\n\r\ndef _vgg19(layers):\r\n    r\"\"\"Get vgg19 layers\"\"\"\r\n    network = torchvision.models.vgg19(pretrained=True).features\r\n    layer_name_mapping = {1: 'relu_1_1',\r\n                          3: 'relu_1_2',\r\n                          6: 'relu_2_1',\r\n                          8: 'relu_2_2',\r\n                          11: 'relu_3_1',\r\n                          13: 'relu_3_2',\r\n                          15: 'relu_3_3',\r\n                          17: 'relu_3_4',\r\n                          20: 'relu_4_1',\r\n                          22: 'relu_4_2',\r\n                          24: 'relu_4_3',\r\n                          26: 'relu_4_4',\r\n                          29: 'relu_5_1'}\r\n    return _PerceptualNetwork(network, layer_name_mapping, layers)\r\n\r\nclass PerceptualLoss(nn.Module):\r\n    r\"\"\"Perceptual loss initialization.\r\n\r\n    Args:\r\n        network (str) : The name of the loss network: 'vgg16' | 'vgg19'.\r\n        layers (str or list of str) : The layers used to compute the loss.\r\n        weights (float or list of float : The loss weights of each layer.\r\n        criterion (str): The type of distance function: 'l1' | 'l2'.\r\n        resize (bool) : If ``True``, resize the input images to 224x224.\r\n        resize_mode (str): Algorithm used for resizing.\r\n        instance_normalized (bool): If ``True``, applies instance normalization\r\n            to the feature maps before computing the distance.\r\n        num_scales (int): The loss will be evaluated at original size and\r\n            this many times downsampled sizes.\r\n    \"\"\"\r\n\r\n    def __init__(self, network='vgg19', layers='relu_4_1', weights=None,\r\n                 criterion='l1', resize=False, resize_mode='bilinear',\r\n                 instance_normalized=False, num_scales=1,):\r\n        super().__init__()\r\n        if isinstance(layers, str):\r\n            layers = [layers]\r\n        if weights is None:\r\n            weights = [1.] * len(layers)\r\n        elif isinstance(layers, float) or isinstance(layers, int):\r\n            weights = [weights]\r\n\r\n        assert len(layers) == len(weights), \\\r\n            'The number of layers (%s) must be equal to ' \\\r\n            'the number of weights (%s).' % (len(layers), len(weights))\r\n        if network == 'vgg19':\r\n            self.model = _vgg19(layers)\r\n        else:\r\n            raise ValueError('Network %s is not recognized' % network)\r\n\r\n        self.num_scales = num_scales\r\n        self.layers = layers\r\n        self.weights = weights\r\n        if criterion == 'l1':\r\n            self.criterion = nn.L1Loss()\r\n        elif criterion == 'l2' or criterion == 'mse':\r\n            self.criterion = nn.MSELoss()\r\n        else:\r\n            raise ValueError('Criterion %s is not recognized' % criterion)\r\n        self.resize = resize\r\n        self.resize_mode = resize_mode\r\n        self.instance_normalized = instance_normalized\r\n\r\n\r\n        print('Perceptual loss:')\r\n        print('\\tMode: {}'.format(network))\r\n\r\n    def forward(self, inp, target, mask=None,use_style_loss=False,weight_style_to_perceptual=0.):\r\n        r\"\"\"Perceptual loss forward.\r\n\r\n        Args:\r\n           inp (4D tensor) : Input tensor.\r\n           target (4D tensor) : Ground truth tensor, same shape as the input.\r\n\r\n        Returns:\r\n           (scalar tensor) : The perceptual loss.\r\n        \"\"\"\r\n        # Perceptual loss should operate in eval mode by default.\r\n        self.model.eval()\r\n        inp, target = \\\r\n            apply_imagenet_normalization(inp), \\\r\n            apply_imagenet_normalization(target)\r\n        if self.resize:\r\n            inp = F.interpolate(\r\n                inp, mode=self.resize_mode, size=(256, 256),\r\n                align_corners=False)\r\n            target = F.interpolate(\r\n                target, mode=self.resize_mode, size=(256, 256),\r\n                align_corners=False)\r\n\r\n        # Evaluate perceptual loss at each scale.\r\n        loss = 0\r\n        style_loss=0\r\n        for scale in range(self.num_scales):\r\n            input_features, target_features = \\\r\n                self.model(inp), self.model(target)\r\n            for layer, weight in zip(self.layers, self.weights):\r\n                # Example per-layer VGG19 loss values after applying\r\n                # [0.03125, 0.0625, 0.125, 0.25, 1.0] weighting.\r\n                # relu_1_1, 0.014698\r\n                # relu_2_1, 0.085817\r\n                # relu_3_1, 0.349977\r\n                # relu_4_1, 0.544188\r\n                # relu_5_1, 0.906261\r\n                input_feature = input_features[layer]\r\n                target_feature = target_features[layer].detach()\r\n                if self.instance_normalized:\r\n                    input_feature = F.instance_norm(input_feature)\r\n                    target_feature = F.instance_norm(target_feature)\r\n\r\n                if mask is not None:\r\n                    mask_ = F.interpolate(mask, input_feature.shape[2:],\r\n                                          mode='bilinear',\r\n                                          align_corners=False)\r\n                    input_feature = input_feature * mask_\r\n                    target_feature = target_feature * mask_\r\n                    # print('mask',mask_.shape)\r\n\r\n\r\n                loss += weight * self.criterion(input_feature,\r\n                                                target_feature)\r\n                if use_style_loss and scale==0:\r\n                    style_loss += self.criterion(self.compute_gram(input_feature),\r\n                                                 self.compute_gram(target_feature))\r\n\r\n            # Downsample the input and target.\r\n            if scale != self.num_scales - 1:\r\n                inp = F.interpolate(\r\n                    inp, mode=self.resize_mode, scale_factor=0.5,\r\n                    align_corners=False, recompute_scale_factor=True)\r\n                target = F.interpolate(\r\n                    target, mode=self.resize_mode, scale_factor=0.5,\r\n                    align_corners=False, recompute_scale_factor=True)\r\n\r\n        if use_style_loss:\r\n            return loss + style_loss*weight_style_to_perceptual\r\n        else:\r\n            return loss\r\n\r\n\r\n    def compute_gram(self, x):\r\n        b, ch, h, w = x.size()\r\n        f = x.view(b, ch, w * h)\r\n        f_T = f.transpose(1, 2)\r\n        G = f.bmm(f_T) / (h * w * ch)\r\n        return G\n"})}),"\n",(0,t.jsx)(e.h4,{id:"dl",children:"\u7ed8\u5236landmark"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:'"""MediaPipe solution drawing utils."""\r\nimport math\r\nfrom typing import List, Mapping, Optional, Tuple, Union\r\nimport cv2\r\nimport dataclasses\r\nimport numpy as np\r\nimport tqdm\r\nfrom mediapipe.framework.formats import landmark_pb2\r\n_PRESENCE_THRESHOLD = 0.5\r\n_VISIBILITY_THRESHOLD = 0.5 \r\n_BGR_CHANNELS = 3\r\n\r\nWHITE_COLOR = (224, 224, 224)\r\nBLACK_COLOR = (0, 0, 0)\r\nRED_COLOR = (0, 0, 255)\r\nGREEN_COLOR = (0, 128, 0)\r\nBLUE_COLOR = (255, 0, 0)\r\n\r\n\r\n@dataclasses.dataclass\r\nclass DrawingSpec:\r\n  # Color for drawing the annotation. Default to the white color.\r\n  color: Tuple[int, int, int] = WHITE_COLOR\r\n  # Thickness for drawing the annotation. Default to 2 pixels.\r\n  thickness: int = 2\r\n  # Circle radius. Default to 2 pixels.\r\n  circle_radius: int = 2\r\n\r\ndef _normalized_to_pixel_coordinates(\r\n    normalized_x: float, normalized_y: float, image_width: int,\r\n    image_height: int) -> Union[None, Tuple[int, int]]:\r\n  """Converts normalized value pair to pixel coordinates."""\r\n\r\n  # Checks if the float value is between 0 and 1.\r\n  def is_valid_normalized_value(value: float) -> bool:\r\n    return (value > 0 or math.isclose(0, value)) and (value < 1 or\r\n                                                      math.isclose(1, value))\r\n\r\n  if not (is_valid_normalized_value(normalized_x) and\r\n          is_valid_normalized_value(normalized_y)):\r\n    # TODO: Draw coordinates even if it\'s outside of the image bounds.\r\n    return None\r\n  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\r\n  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\r\n  return x_px, y_px\r\n\r\n\r\nFACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\r\n                           (17, 314), (314, 405), (405, 321), (321, 375),\r\n                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\r\n                           (37, 0), (0, 267),\r\n                           (267, 269), (269, 270), (270, 409), (409, 291),\r\n                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\r\n                           (14, 317), (317, 402), (402, 318), (318, 324),\r\n                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\r\n                           (82, 13), (13, 312), (312, 311), (311, 310),\r\n                           (310, 415), (415, 308)])\r\n\r\nFACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\r\n                               (374, 380), (380, 381), (381, 382), (382, 362),\r\n                               (263, 466), (466, 388), (388, 387), (387, 386),\r\n                               (386, 385), (385, 384), (384, 398), (398, 362)])\r\n\r\nFACEMESH_LEFT_IRIS = frozenset([(474, 475), (475, 476), (476, 477),\r\n                                 (477, 474)])\r\n\r\nFACEMESH_LEFT_EYEBROW = frozenset([(276, 283), (283, 282), (282, 295),\r\n                                   (295, 285), (300, 293), (293, 334),\r\n                                   (334, 296), (296, 336)])\r\n\r\nFACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\r\n                                (145, 153), (153, 154), (154, 155), (155, 133),\r\n                                (33, 246), (246, 161), (161, 160), (160, 159),\r\n                                (159, 158), (158, 157), (157, 173), (173, 133)])\r\n\r\nFACEMESH_RIGHT_EYEBROW = frozenset([(46, 53), (53, 52), (52, 65), (65, 55),\r\n                                    (70, 63), (63, 105), (105, 66), (66, 107)])\r\n\r\nFACEMESH_RIGHT_IRIS = frozenset([(469, 470), (470, 471), (471, 472),\r\n                                 (472, 469)])\r\n\r\nFACEMESH_FACE_OVAL = frozenset([(389, 356), (356, 454),\r\n                                (454, 323), (323, 361), (361, 288), (288, 397),\r\n                                (397, 365), (365, 379), (379, 378), (378, 400),\r\n                                (400, 377), (377, 152), (152, 148), (148, 176),\r\n                                (176, 149), (149, 150), (150, 136), (136, 172),\r\n                                (172, 58), (58, 132), (132, 93), (93, 234),\r\n                                (234, 127), (127, 162)])\r\n#(10, 338), (338, 297), (297, 332), (332, 284),(284, 251), (251, 389) (162, 21), (21, 54),(54, 103), (103, 67), (67, 109), (109, 10)\r\n\r\nFACEMESH_NOSE= frozenset([(168, 6),(6,197),(197,195),(195,5),(5,4),\\\r\n                          (4,45),(45,220),(220,115),(115,48),\\\r\n                          (4,275),(275,440),(440,344),(344,278),])\r\nFACEMESH_FULL = frozenset().union(*[\r\n    FACEMESH_LIPS, FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, FACEMESH_RIGHT_EYE,\r\n    FACEMESH_RIGHT_EYEBROW, FACEMESH_FACE_OVAL,FACEMESH_NOSE\r\n])\r\nconnections=FACEMESH_FULL\r\n\r\ndef summary_landmark(edge_set):\r\n    landmarks=set()\r\n    for a,b in edge_set:\r\n        landmarks.add(a)\r\n        landmarks.add(b)\r\n    return landmarks\r\nall_landmark_idx=summary_landmark(FACEMESH_FULL)\r\npose_landmark_idx=\\\r\nsummary_landmark(FACEMESH_NOSE.union(*[FACEMESH_RIGHT_EYEBROW,FACEMESH_RIGHT_EYE,\\\r\n                                       FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW,])).union([162,127,234,93,389,356,454,323])\r\ncontent_landmark_idx= all_landmark_idx - pose_landmark_idx\r\n\r\n\r\ndef draw_landmarks(\r\n    image: np.ndarray,\r\n    landmark_list: List,\r\n    connections: Optional[List[Tuple[int, int]]] = None,\r\n    landmark_drawing_spec: Union[DrawingSpec,\r\n                                 Mapping[int, DrawingSpec]] = DrawingSpec(\r\n                                     color=RED_COLOR),\r\n    connection_drawing_spec: Union[DrawingSpec,\r\n                                   Mapping[Tuple[int, int],\r\n                                           DrawingSpec]] = DrawingSpec()):\r\n  """Draws the landmarks and the connections on the image.\r\n\r\n  Args:\r\n    image: A three channel BGR image represented as numpy ndarray.\r\n    landmark_list: A normalized landmark list proto message to be annotated on\r\n      the image.\r\n    connections: A list of landmark index tuples that specifies how landmarks to\r\n      be connected in the drawing.\r\n    landmark_drawing_spec: Either a DrawingSpec object or a mapping from\r\n      hand landmarks to the DrawingSpecs that specifies the landmarks\' drawing\r\n      settings such as color, line thickness, and circle radius.\r\n      If this argument is explicitly set to None, no landmarks will be drawn.\r\n    connection_drawing_spec: Either a DrawingSpec object or a mapping from\r\n      hand connections to the DrawingSpecs that specifies the\r\n      connections\' drawing settings such as color and line thickness.\r\n      If this argument is explicitly set to None, no landmark connections will\r\n      be drawn.\r\n\r\n  Raises:\r\n    ValueError: If one of the followings:\r\n      a) If the input image is not three channel BGR.\r\n      b) If any connetions contain invalid landmark index.\r\n  """\r\n  if not landmark_list:\r\n    return\r\n  if image.shape[2] != _BGR_CHANNELS:\r\n    raise ValueError(\'Input image must contain three channel bgr data.\')\r\n  image_rows, image_cols, _ = image.shape\r\n  idx_to_coordinates = {}\r\n  for landmark in landmark_list:\r\n    # if ((landmark.HasField(\'visibility\') and\r\n    #      landmark.visibility < _VISIBILITY_THRESHOLD) or\r\n    #     (landmark.HasField(\'presence\') and\r\n    #      landmark.presence < _PRESENCE_THRESHOLD)):\r\n    #   continue\r\n    idx=landmark.idx\r\n    landmark_px = _normalized_to_pixel_coordinates(landmark.x, landmark.y,\r\n                                                 image_cols, image_rows)\r\n    if landmark_px:\r\n      idx_to_coordinates[idx] = landmark_px\r\n\r\n  if connections:\r\n    num_landmarks = len(landmark_list)\r\n    # Draws the connections if the start and end landmarks are both visible.\r\n    for connection in connections:\r\n      start_idx = connection[0]\r\n      end_idx = connection[1]\r\n      # if not (0 <= start_idx < num_landmarks and 0 <= end_idx < num_landmarks):\r\n      #   raise ValueError(f\'Landmark index is out of range. Invalid connection \'\r\n      #                    f\'from landmark #{start_idx} to landmark #{end_idx}.\')\r\n      if start_idx in idx_to_coordinates and end_idx in idx_to_coordinates:\r\n        drawing_spec = connection_drawing_spec[connection] if isinstance(\r\n            connection_drawing_spec, Mapping) else connection_drawing_spec\r\n        # if start_idx in content_landmark and end_idx in content_landmark:\r\n        cv2.line(image, idx_to_coordinates[start_idx],\r\n                 idx_to_coordinates[end_idx], drawing_spec.color,\r\n                 drawing_spec.thickness)\r\n  return image\r\n  # Draws landmark points after finishing the connection lines, which is\r\n  # aesthetically better.\r\n  # if landmark_drawing_spec:\r\n  #   for idx, landmark_px in idx_to_coordinates.items():\r\n  #     drawing_spec = landmark_drawing_spec[idx] if isinstance(\r\n  #         landmark_drawing_spec, Mapping) else landmark_drawing_spec\r\n  #     # White circle border\r\n      # circle_border_radius = max(drawing_spec.circle_radius + 1,\r\n      #                            int(drawing_spec.circle_radius * 1.2))\r\n      # circle_border_radius=circle_border_radius*0.1\r\n      # cv2.circle(image, landmark_px, circle_border_radius, WHITE_COLOR,\r\n      #            drawing_spec.thickness)\r\n      # Fill color into the circle\r\n\r\n      # cv2.circle(image, landmark_px, 1,\r\n      #            drawing_spec.color, drawing_spec.thickness)\r\n      # cv2.putText(image,str(idx),landmark_px,cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),1,cv2.LINE_AA)\n'})}),"\n",(0,t.jsx)(e.h4,{id:"is",children:"\u63a8\u7406"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"\r\nimport numpy as np\r\nimport cv2, os, argparse\r\nimport subprocess\r\nfrom tqdm import tqdm\r\nfrom models import Renderer\r\nimport torch\r\nfrom models import Landmark_generator as Landmark_transformer\r\nimport face_alignment\r\nfrom models import audio\r\nfrom draw_landmark import draw_landmarks\r\nimport mediapipe as mp\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--input', '--input_template_video', type=str, default='./test/template_video/129.mp4')\r\n#'./test/template_video/129.mp4'\r\n\r\nparser.add_argument('--audio', type=str, default='./test/template_video/audio2.wav')\r\n#'./test/template_video/abstract.mp3'\r\n#'./test/template_video/audio2.wav'\r\nparser.add_argument('--output_dir', type=str, default='./test_result')\r\nparser.add_argument('--static', type=bool, help='whether only use  the first frame for inference', default=False)\r\nparser.add_argument('--landmark_gen_checkpoint_path', type=str, default='./test/checkpoints/landmarkgenerator_checkpoint.pth')\r\nparser.add_argument('--renderer_checkpoint_path', type=str, default='./test/checkpoints/renderer_checkpoint.pth')\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nargs = parser.parse_args()\r\n\r\nref_img_N = 25\r\nNl = 15\r\nT = 5\r\nmel_step_size = 16\r\nimg_size = 128\r\n\r\nmp_face_mesh = mp.solutions.face_mesh\r\ndrawing_spec = mp.solutions.drawing_utils.DrawingSpec(thickness=1, circle_radius=1)\r\nfa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device='cuda')\r\nlip_index = [0, 17]  # the index of the midpoints of the upper lip and lower lip\r\nlandmark_gen_checkpoint_path = args.landmark_gen_checkpoint_path\r\nrenderer_checkpoint_path =args.renderer_checkpoint_path\r\noutput_dir = args.output_dir\r\ntemp_dir = 'tempfile_of_{}'.format(output_dir.split('/')[-1])\r\nos.makedirs(output_dir, exist_ok=True)\r\nos.makedirs(temp_dir, exist_ok=True)\r\ninput_video_path = args.input\r\ninput_audio_path = args.audio\r\n\r\n# the following is the index sequence for fical landmarks detected by mediapipe\r\nori_sequence_idx = [162, 127, 234, 93, 132, 58, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288,\r\n                    361, 323, 454, 356, 389,  #\r\n                    70, 63, 105, 66, 107, 55, 65, 52, 53, 46,  #\r\n                    336, 296, 334, 293, 300, 276, 283, 282, 295, 285,  #\r\n                    168, 6, 197, 195, 5,  #\r\n                    48, 115, 220, 45, 4, 275, 440, 344, 278,  #\r\n                    33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7,  #\r\n                    362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382,  #\r\n                    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146,  #\r\n                    78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\r\n\r\n# the following is the connections of landmarks for drawing sketch image\r\nFACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\r\n                           (17, 314), (314, 405), (405, 321), (321, 375),\r\n                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\r\n                           (37, 0), (0, 267),\r\n                           (267, 269), (269, 270), (270, 409), (409, 291),\r\n                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\r\n                           (14, 317), (317, 402), (402, 318), (318, 324),\r\n                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\r\n                           (82, 13), (13, 312), (312, 311), (311, 310),\r\n                           (310, 415), (415, 308)])\r\nFACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\r\n                               (374, 380), (380, 381), (381, 382), (382, 362),\r\n                               (263, 466), (466, 388), (388, 387), (387, 386),\r\n                               (386, 385), (385, 384), (384, 398), (398, 362)])\r\nFACEMESH_LEFT_EYEBROW = frozenset([(276, 283), (283, 282), (282, 295),\r\n                                   (295, 285), (300, 293), (293, 334),\r\n                                   (334, 296), (296, 336)])\r\nFACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\r\n                                (145, 153), (153, 154), (154, 155), (155, 133),\r\n                                (33, 246), (246, 161), (161, 160), (160, 159),\r\n                                (159, 158), (158, 157), (157, 173), (173, 133)])\r\nFACEMESH_RIGHT_EYEBROW = frozenset([(46, 53), (53, 52), (52, 65), (65, 55),\r\n                                    (70, 63), (63, 105), (105, 66), (66, 107)])\r\nFACEMESH_FACE_OVAL = frozenset([(389, 356), (356, 454),\r\n                                (454, 323), (323, 361), (361, 288), (288, 397),\r\n                                (397, 365), (365, 379), (379, 378), (378, 400),\r\n                                (400, 377), (377, 152), (152, 148), (148, 176),\r\n                                (176, 149), (149, 150), (150, 136), (136, 172),\r\n                                (172, 58), (58, 132), (132, 93), (93, 234),\r\n                                (234, 127), (127, 162)])\r\nFACEMESH_NOSE = frozenset([(168, 6), (6, 197), (197, 195), (195, 5), (5, 4),\r\n                           (4, 45), (45, 220), (220, 115), (115, 48),\r\n                           (4, 275), (275, 440), (440, 344), (344, 278), ])\r\nFACEMESH_CONNECTION = frozenset().union(*[\r\n    FACEMESH_LIPS, FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, FACEMESH_RIGHT_EYE,\r\n    FACEMESH_RIGHT_EYEBROW, FACEMESH_FACE_OVAL, FACEMESH_NOSE\r\n])\r\n\r\nfull_face_landmark_sequence = [*list(range(0, 4)), *list(range(21, 25)), *list(range(25, 91)),  #upper-half face\r\n                               *list(range(4, 21)),  # jaw\r\n                               *list(range(91, 131))]  # mouth\r\n\r\ndef summarize_landmark(edge_set):  # summarize all ficial landmarks used to construct edge\r\n    landmarks = set()\r\n    for a, b in edge_set:\r\n        landmarks.add(a)\r\n        landmarks.add(b)\r\n    return landmarks\r\n\r\nall_landmarks_idx = summarize_landmark(FACEMESH_CONNECTION)\r\npose_landmark_idx = \\\r\n    summarize_landmark(FACEMESH_NOSE.union(*[FACEMESH_RIGHT_EYEBROW, FACEMESH_RIGHT_EYE,\r\n                                             FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, ])).union(\r\n        [162, 127, 234, 93, 389, 356, 454, 323])\r\n# pose landmarks are landmarks of the upper-half face(eyes,nose,cheek) that represents the pose information\r\n\r\ncontent_landmark_idx = all_landmarks_idx - pose_landmark_idx\r\n# content_landmark include landmarks of lip and jaw which are inferred from audio\r\n\r\nif os.path.isfile(input_video_path) and input_video_path.split('.')[1] in ['jpg', 'png', 'jpeg']:\r\n    args.static = True\r\n\r\noutfile_path = os.path.join(output_dir,\r\n                       '{}_N_{}_Nl_{}.mp4'.format(input_video_path.split('/')[-1][:-4] + 'result', ref_img_N, Nl))\r\nif os.path.isfile(input_video_path) and input_video_path.split('.')[1] in ['jpg', 'png', 'jpeg']:\r\n    args.static = True\r\n\r\n\r\ndef swap_masked_region(target_img, src_img, mask): #function used in post-process\r\n    \"\"\"From src_img crop masked region to replace corresponding masked region\r\n       in target_img\r\n    \"\"\"  # swap_masked_region(src_frame, generated_frame, mask=mask_img)\r\n    mask_img = cv2.GaussianBlur(mask, (21, 21), 11)\r\n    mask1 = mask_img / 255\r\n    mask1 = np.tile(np.expand_dims(mask1, axis=2), (1, 1, 3))\r\n    img = src_img * mask1 + target_img * (1 - mask1)\r\n    return img.astype(np.uint8)\r\n\r\ndef merge_face_contour_only(src_frame, generated_frame, face_region_coord, fa): #function used in post-process\r\n    \"\"\"Merge the face from generated_frame into src_frame\r\n    \"\"\"\r\n    input_img = src_frame\r\n    y1, y2, x1, x2 = 0, 0, 0, 0\r\n    if face_region_coord is not None:\r\n        y1, y2, x1, x2 = face_region_coord\r\n        input_img = src_frame[y1:y2, x1:x2]\r\n    ### 1) Detect the facial landmarks\r\n    preds = fa.get_landmarks(input_img)[0]  # 68x2\r\n    if face_region_coord is not None:\r\n        preds += np.array([x1, y1])\r\n    lm_pts = preds.astype(int)\r\n    contour_idx = list(range(0, 17)) + list(range(17, 27))[::-1]\r\n    contour_pts = lm_pts[contour_idx]\r\n    ### 2) Make the landmark region mark image\r\n    mask_img = np.zeros((src_frame.shape[0], src_frame.shape[1], 1), np.uint8)\r\n    cv2.fillConvexPoly(mask_img, contour_pts, 255)\r\n    ### 3) Do swap\r\n    img = swap_masked_region(src_frame, generated_frame, mask=mask_img)\r\n    return img\r\n\r\n\r\ndef _load(checkpoint_path):\r\n    if device == 'cuda':\r\n        checkpoint = torch.load(checkpoint_path)\r\n    else:\r\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n    return checkpoint\r\ndef load_model(model, path):\r\n    print(\"Load checkpoint from: {}\".format(path))\r\n    checkpoint = _load(path)\r\n    s = checkpoint[\"state_dict\"]\r\n    new_s = {}\r\n    for k, v in s.items():\r\n        if k[:6] == 'module':\r\n            new_k=k.replace('module.', '', 1)\r\n        else:\r\n            new_k =k\r\n        new_s[new_k] = v\r\n    model.load_state_dict(new_s)\r\n    model = model.to(device)\r\n    return model.eval()\r\n\r\nclass LandmarkDict(dict):# Makes a dictionary that behave like an object to represent each landmark\r\n    def __init__(self, idx, x, y):\r\n        self['idx'] = idx\r\n        self['x'] = x\r\n        self['y'] = y\r\n    def __getattr__(self, name):\r\n        try:\r\n            return self[name]\r\n        except:\r\n            raise AttributeError(name)\r\n    def __setattr__(self, name, value):\r\n        self[name] = value\r\nprint(\" landmark_generator_model loaded from : \", landmark_gen_checkpoint_path)\r\nprint(\" renderer loaded from : \", renderer_checkpoint_path)\r\nlandmark_generator_model = load_model(\r\n    model=Landmark_transformer(T=T, d_model=512, nlayers=4, nhead=4, dim_feedforward=1024, dropout=0.1),\r\n    path=landmark_gen_checkpoint_path)\r\nrenderer = load_model(model=Renderer(), path=renderer_checkpoint_path)\r\n\r\n##(1) Reading input video frames  ###\r\nprint('Reading video frames ... from', input_video_path)\r\nif not os.path.isfile(input_video_path):\r\n    raise ValueError('the input video file does not exist')\r\nelif input_video_path.split('.')[1] in ['jpg', 'png', 'jpeg']: #if input a single image for testing\r\n    ori_background_frames = [cv2.imread(input_video_path)]\r\nelse:\r\n    video_stream = cv2.VideoCapture(input_video_path)\r\n    fps = video_stream.get(cv2.CAP_PROP_FPS)\r\n    if fps != 25:\r\n        print(\" input video fps:\", fps,',converting to 25fps...')\r\n        command = 'ffmpeg -y -i ' + input_video_path + ' -r 25 ' + '{}/temp_25fps.avi'.format(temp_dir)\r\n        subprocess.call(command, shell=True)\r\n        input_video_path = '{}/temp_25fps.avi'.format(temp_dir)\r\n        video_stream.release()\r\n        video_stream = cv2.VideoCapture(input_video_path)\r\n        fps = video_stream.get(cv2.CAP_PROP_FPS)\r\n    assert fps == 25\r\n\r\n    ori_background_frames = [] #input videos frames (includes background as well as face)\r\n    frame_idx = 0\r\n    while 1:\r\n        still_reading, frame = video_stream.read()\r\n        if not still_reading:\r\n            video_stream.release()\r\n            break\r\n        ori_background_frames.append(frame)\r\n        frame_idx = frame_idx + 1\r\ninput_vid_len = len(ori_background_frames)\r\n\r\n##(2) Extracting audio####\r\nif not input_audio_path.endswith('.wav'):\r\n    command = 'ffmpeg -y -i {} -strict -2 {}'.format(input_audio_path, '{}/temp.wav'.format(temp_dir))\r\n    subprocess.call(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\r\n    input_audio_path = '{}/temp.wav'.format(temp_dir)\r\nwav = audio.load_wav(input_audio_path, 16000)\r\nmel = audio.melspectrogram(wav)  # (H,W)   extract mel-spectrum\r\n##read audio mel into list###\r\nmel_chunks = []  # each mel chunk correspond to 5 video frames, used to generate one video frame\r\nmel_idx_multiplier = 80. / fps\r\nmel_chunk_idx = 0\r\nwhile 1:\r\n    start_idx = int(mel_chunk_idx * mel_idx_multiplier)\r\n    if start_idx + mel_step_size > len(mel[0]):\r\n        break\r\n    mel_chunks.append(mel[:, start_idx: start_idx + mel_step_size])  # mel for generate one video frame\r\n    mel_chunk_idx += 1\r\n# mel_chunks = mel_chunks[:(len(mel_chunks) // T) * T]\r\n\r\n##(3) detect facial landmarks using mediapipe tool\r\nboxes = []  #bounding boxes of human face\r\nlip_dists = [] #lip dists\r\n#we define the lip dist(openness): distance between the  midpoints of the upper lip and lower lip\r\nface_crop_results = []\r\nall_pose_landmarks, all_content_landmarks = [], []  #content landmarks include lip and jaw landmarks\r\nwith mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True,\r\n                           min_detection_confidence=0.5) as face_mesh:\r\n    # (1) get bounding boxes and lip dist\r\n    for frame_idx, full_frame in enumerate(ori_background_frames):\r\n        h, w = full_frame.shape[0], full_frame.shape[1]\r\n        results = face_mesh.process(cv2.cvtColor(full_frame, cv2.COLOR_BGR2RGB))\r\n        if not results.multi_face_landmarks:\r\n            raise NotImplementedError  # not detect face\r\n        face_landmarks = results.multi_face_landmarks[0]\r\n\r\n        ## calculate the lip dist\r\n        dx = face_landmarks.landmark[lip_index[0]].x - face_landmarks.landmark[lip_index[1]].x\r\n        dy = face_landmarks.landmark[lip_index[0]].y - face_landmarks.landmark[lip_index[1]].y\r\n        dist = np.linalg.norm((dx, dy))\r\n        lip_dists.append((frame_idx, dist))\r\n\r\n        # (1)get the marginal landmarks to crop face\r\n        x_min,x_max,y_min,y_max = 999,-999,999,-999\r\n        for idx, landmark in enumerate(face_landmarks.landmark):\r\n            if idx in all_landmarks_idx:\r\n                if landmark.x < x_min:\r\n                    x_min = landmark.x\r\n                if landmark.x > x_max:\r\n                    x_max = landmark.x\r\n                if landmark.y < y_min:\r\n                    y_min = landmark.y\r\n                if landmark.y > y_max:\r\n                    y_max = landmark.y\r\n        ##########plus some pixel to the marginal region##########\r\n        #note:the landmarks coordinates returned by mediapipe range 0~1\r\n        plus_pixel = 25\r\n        x_min = max(x_min - plus_pixel / w, 0)\r\n        x_max = min(x_max + plus_pixel / w, 1)\r\n\r\n        y_min = max(y_min - plus_pixel / h, 0)\r\n        y_max = min(y_max + plus_pixel / h, 1)\r\n        y1, y2, x1, x2 = int(y_min * h), int(y_max * h), int(x_min * w), int(x_max * w)\r\n        boxes.append([y1, y2, x1, x2])\r\n    boxes = np.array(boxes)\r\n\r\n    # (2)croppd face\r\n    face_crop_results = [[image[y1:y2, x1:x2], (y1, y2, x1, x2)] \\\r\n                         for image, (y1, y2, x1, x2) in zip(ori_background_frames, boxes)]\r\n\r\n    # (3)detect facial landmarks\r\n    for frame_idx, full_frame in enumerate(ori_background_frames):\r\n        h, w = full_frame.shape[0], full_frame.shape[1]\r\n        results = face_mesh.process(cv2.cvtColor(full_frame, cv2.COLOR_BGR2RGB))\r\n        if not results.multi_face_landmarks:\r\n            raise ValueError(\"not detect face in some frame!\")  # not detect\r\n        face_landmarks = results.multi_face_landmarks[0]\r\n\r\n\r\n\r\n        pose_landmarks, content_landmarks = [], []\r\n        for idx, landmark in enumerate(face_landmarks.landmark):\r\n            if idx in pose_landmark_idx:\r\n                pose_landmarks.append((idx, w * landmark.x, h * landmark.y))\r\n            if idx in content_landmark_idx:\r\n                content_landmarks.append((idx, w * landmark.x, h * landmark.y))\r\n\r\n        # normalize landmarks to 0~1\r\n        y_min, y_max, x_min, x_max = face_crop_results[frame_idx][1]  #bounding boxes\r\n        pose_landmarks = [ \\\r\n            [idx, (x - x_min) / (x_max - x_min), (y - y_min) / (y_max - y_min)] for idx, x, y in pose_landmarks]\r\n        content_landmarks = [ \\\r\n            [idx, (x - x_min) / (x_max - x_min), (y - y_min) / (y_max - y_min)] for idx, x, y in content_landmarks]\r\n        all_pose_landmarks.append(pose_landmarks)\r\n        all_content_landmarks.append(content_landmarks)\r\n\r\n# smooth landmarks\r\ndef get_smoothened_landmarks(all_landmarks, windows_T=1):\r\n    for i in range(len(all_landmarks)):  # frame i\r\n        if i + windows_T > len(all_landmarks):\r\n            window = all_landmarks[len(all_landmarks) - windows_T:]\r\n        else:\r\n            window = all_landmarks[i: i + windows_T]\r\n        #####\r\n        for j in range(len(all_landmarks[i])):  # landmark j\r\n            all_landmarks[i][j][1] = np.mean([frame_landmarks[j][1] for frame_landmarks in window])  # x\r\n            all_landmarks[i][j][2] = np.mean([frame_landmarks[j][2] for frame_landmarks in window])  # y\r\n    return all_landmarks\r\n\r\nall_pose_landmarks = get_smoothened_landmarks(all_pose_landmarks, windows_T=1)\r\nall_content_landmarks=get_smoothened_landmarks(all_content_landmarks,windows_T=1)\r\n\r\n\r\n##randomly select N_l reference landmarks for landmark transformer##\r\ndists_sorted = sorted(lip_dists, key=lambda x: x[1])\r\nlip_dist_idx = np.asarray([idx for idx, dist in dists_sorted])  #the frame idxs sorted by lip openness\r\n\r\nNl_idxs = [lip_dist_idx[int(i)] for i in torch.linspace(0, input_vid_len - 1, steps=Nl)]\r\nNl_pose_landmarks, Nl_content_landmarks = [], []  #Nl_pose + Nl_content=Nl reference landmarks\r\nfor reference_idx in Nl_idxs:\r\n    frame_pose_landmarks = all_pose_landmarks[reference_idx]\r\n    frame_content_landmarks = all_content_landmarks[reference_idx]\r\n    Nl_pose_landmarks.append(frame_pose_landmarks)\r\n    Nl_content_landmarks.append(frame_content_landmarks)\r\n\r\nNl_pose = torch.zeros((Nl, 2, 74))  # 74 landmark\r\nNl_content = torch.zeros((Nl, 2, 57))  # 57 landmark\r\nfor idx in range(Nl):\r\n    #arrange the landmark in a certain order, since the landmark index returned by mediapipe is is chaotic\r\n    Nl_pose_landmarks[idx] = sorted(Nl_pose_landmarks[idx],\r\n                                    key=lambda land_tuple: ori_sequence_idx.index(land_tuple[0]))\r\n    Nl_content_landmarks[idx] = sorted(Nl_content_landmarks[idx],\r\n                                       key=lambda land_tuple: ori_sequence_idx.index(land_tuple[0]))\r\n\r\n    Nl_pose[idx, 0, :] = torch.FloatTensor(\r\n        [Nl_pose_landmarks[idx][i][1] for i in range(len(Nl_pose_landmarks[idx]))])  # x\r\n    Nl_pose[idx, 1, :] = torch.FloatTensor(\r\n        [Nl_pose_landmarks[idx][i][2] for i in range(len(Nl_pose_landmarks[idx]))])  # y\r\n    Nl_content[idx, 0, :] = torch.FloatTensor(\r\n        [Nl_content_landmarks[idx][i][1] for i in range(len(Nl_content_landmarks[idx]))])  # x\r\n    Nl_content[idx, 1, :] = torch.FloatTensor(\r\n        [Nl_content_landmarks[idx][i][2] for i in range(len(Nl_content_landmarks[idx]))])  # y\r\nNl_content = Nl_content.unsqueeze(0)  # (1,Nl, 2, 57)\r\nNl_pose = Nl_pose.unsqueeze(0)  # (1,Nl,2,74)\r\n\r\n##select reference images and draw sketches for rendering according to lip openness##\r\nref_img_idx = [int(lip_dist_idx[int(i)]) for i in torch.linspace(0, input_vid_len - 1, steps=ref_img_N)]\r\nref_imgs = [face_crop_results[idx][0] for idx in ref_img_idx]\r\n## (N,H,W,3)\r\nref_img_pose_landmarks, ref_img_content_landmarks = [], []\r\nfor idx in ref_img_idx:\r\n    ref_img_pose_landmarks.append(all_pose_landmarks[idx])\r\n    ref_img_content_landmarks.append(all_content_landmarks[idx])\r\n\r\nref_img_pose = torch.zeros((ref_img_N, 2, 74))  # 74 landmark\r\nref_img_content = torch.zeros((ref_img_N, 2, 57))  # 57 landmark\r\n\r\nfor idx in range(ref_img_N):\r\n    ref_img_pose_landmarks[idx] = sorted(ref_img_pose_landmarks[idx],\r\n                                         key=lambda land_tuple: ori_sequence_idx.index(land_tuple[0]))\r\n    ref_img_content_landmarks[idx] = sorted(ref_img_content_landmarks[idx],\r\n                                            key=lambda land_tuple: ori_sequence_idx.index(land_tuple[0]))\r\n    ref_img_pose[idx, 0, :] = torch.FloatTensor(\r\n        [ref_img_pose_landmarks[idx][i][1] for i in range(len(ref_img_pose_landmarks[idx]))])  # x\r\n    ref_img_pose[idx, 1, :] = torch.FloatTensor(\r\n        [ref_img_pose_landmarks[idx][i][2] for i in range(len(ref_img_pose_landmarks[idx]))])  # y\r\n\r\n    ref_img_content[idx, 0, :] = torch.FloatTensor(\r\n        [ref_img_content_landmarks[idx][i][1] for i in range(len(ref_img_content_landmarks[idx]))])  # x\r\n    ref_img_content[idx, 1, :] = torch.FloatTensor(\r\n        [ref_img_content_landmarks[idx][i][2] for i in range(len(ref_img_content_landmarks[idx]))])  # y\r\n\r\nref_img_full_face_landmarks = torch.cat([ref_img_pose, ref_img_content], dim=2).cpu().numpy()  # (N,2,131)\r\nref_img_sketches = []\r\nfor frame_idx in range(ref_img_full_face_landmarks.shape[0]):  # N\r\n    full_landmarks = ref_img_full_face_landmarks[frame_idx]  # (2,131)\r\n    h, w = ref_imgs[frame_idx].shape[0], ref_imgs[frame_idx].shape[1]\r\n    drawn_sketech = np.zeros((int(h * img_size / min(h, w)), int(w * img_size / min(h, w)), 3))\r\n    mediapipe_format_landmarks = [LandmarkDict(ori_sequence_idx[full_face_landmark_sequence[idx]], full_landmarks[0, idx],\r\n                                               full_landmarks[1, idx]) for idx in range(full_landmarks.shape[1])]\r\n    drawn_sketech = draw_landmarks(drawn_sketech, mediapipe_format_landmarks, connections=FACEMESH_CONNECTION,\r\n                                   connection_drawing_spec=drawing_spec)\r\n    drawn_sketech = cv2.resize(drawn_sketech, (img_size, img_size))  # (128, 128, 3)\r\n    ref_img_sketches.append(drawn_sketech)\r\nref_img_sketches = torch.FloatTensor(np.asarray(ref_img_sketches) / 255.0).cuda().unsqueeze(0).permute(0, 1, 4, 2, 3)\r\n# (1,N, 3, 128, 128)\r\nref_imgs = [cv2.resize(face.copy(), (img_size, img_size)) for face in ref_imgs]\r\nref_imgs = torch.FloatTensor(np.asarray(ref_imgs) / 255.0).unsqueeze(0).permute(0, 1, 4, 2, 3).cuda()\r\n# (1,N,3,H,W)\r\n\r\n##prepare output video strame##\r\nframe_h, frame_w = ori_background_frames[0].shape[:-1]\r\nout_stream = cv2.VideoWriter('{}/result.avi'.format(temp_dir), cv2.VideoWriter_fourcc(*'DIVX'), fps,\r\n                             (frame_w * 2, frame_h))  # +frame_h*3\r\n\r\n\r\n##generate final face image and output video##\r\ninput_mel_chunks_len = len(mel_chunks)\r\ninput_frame_sequence = torch.arange(input_vid_len).tolist()\r\n#the input template video may be shorter than audio\r\n#in this case we repeat the input template video as following\r\nnum_of_repeat=input_mel_chunks_len//input_vid_len+1\r\ninput_frame_sequence = input_frame_sequence + list(reversed(input_frame_sequence))\r\ninput_frame_sequence=input_frame_sequence*((num_of_repeat+1)//2)\r\n\r\n\r\nfor batch_idx, batch_start_idx in tqdm(enumerate(range(0, input_mel_chunks_len - 2, 1)),\r\n                                       total=len(range(0, input_mel_chunks_len - 2, 1))):\r\n    T_input_frame, T_ori_face_coordinates = [], []\r\n    #note: input_frame include background as well as face\r\n    T_mel_batch, T_crop_face,T_pose_landmarks = [], [],[]\r\n\r\n    # (1) for each batch of T frame, generate corresponding landmarks using landmark generator\r\n    for mel_chunk_idx in range(batch_start_idx, batch_start_idx + T):  # for each T frame\r\n        # 1 input audio\r\n        T_mel_batch.append(mel_chunks[max(0, mel_chunk_idx - 2)])\r\n\r\n        # 2.input face\r\n        input_frame_idx = int(input_frame_sequence[mel_chunk_idx])\r\n        face, coords = face_crop_results[input_frame_idx]\r\n        T_crop_face.append(face)\r\n        T_ori_face_coordinates.append((face, coords))  ##input face\r\n        # 3.pose landmarks\r\n        T_pose_landmarks.append(all_pose_landmarks[input_frame_idx])\r\n        # 3.background\r\n        T_input_frame.append(ori_background_frames[input_frame_idx].copy())\r\n    T_mels = torch.FloatTensor(np.asarray(T_mel_batch)).unsqueeze(1).unsqueeze(0)  # 1,T,1,h,w\r\n    #prepare pose landmarks\r\n    T_pose = torch.zeros((T, 2, 74))  # 74 landmark\r\n    for idx in range(T):\r\n        T_pose_landmarks[idx] = sorted(T_pose_landmarks[idx],\r\n                                       key=lambda land_tuple: ori_sequence_idx.index(land_tuple[0]))\r\n        T_pose[idx, 0, :] = torch.FloatTensor(\r\n            [T_pose_landmarks[idx][i][1] for i in range(len(T_pose_landmarks[idx]))])  # x\r\n        T_pose[idx, 1, :] = torch.FloatTensor(\r\n            [T_pose_landmarks[idx][i][2] for i in range(len(T_pose_landmarks[idx]))])  # y\r\n    T_pose = T_pose.unsqueeze(0)  # (1,T, 2,74)\r\n\r\n    #landmark  generator inference\r\n    Nl_pose, Nl_content = Nl_pose.cuda(), Nl_content.cuda() # (Nl,2,74)  (Nl,2,57)\r\n    T_mels, T_pose = T_mels.cuda(), T_pose.cuda()\r\n    with torch.no_grad():  # require    (1,T,1,hv,wv)(1,T,2,74)(1,T,2,57)\r\n        predict_content = landmark_generator_model(T_mels, T_pose, Nl_pose, Nl_content)  # (1*T,2,57)\r\n    T_pose = torch.cat([T_pose[i] for i in range(T_pose.size(0))], dim=0)  # (1*T,2,74)\r\n    T_predict_full_landmarks = torch.cat([T_pose, predict_content], dim=2).cpu().numpy()  # (1*T,2,131)\r\n\r\n    #1.draw target sketch\r\n    T_target_sketches = []\r\n    for frame_idx in range(T):\r\n        full_landmarks = T_predict_full_landmarks[frame_idx]  # (2,131)\r\n        h, w = T_crop_face[frame_idx].shape[0], T_crop_face[frame_idx].shape[1]\r\n        drawn_sketech = np.zeros((int(h * img_size / min(h, w)), int(w * img_size / min(h, w)), 3))\r\n        mediapipe_format_landmarks = [LandmarkDict(ori_sequence_idx[full_face_landmark_sequence[idx]]\r\n                                                   , full_landmarks[0, idx], full_landmarks[1, idx]) for idx in\r\n                                      range(full_landmarks.shape[1])]\r\n        drawn_sketech = draw_landmarks(drawn_sketech, mediapipe_format_landmarks, connections=FACEMESH_CONNECTION,\r\n                                       connection_drawing_spec=drawing_spec)\r\n        drawn_sketech = cv2.resize(drawn_sketech, (img_size, img_size))  # (128, 128, 3)\r\n        if frame_idx == 2:\r\n            show_sketch = cv2.resize(drawn_sketech, (frame_w, frame_h)).astype(np.uint8)\r\n        T_target_sketches.append(torch.FloatTensor(drawn_sketech) / 255)\r\n    T_target_sketches = torch.stack(T_target_sketches, dim=0).permute(0, 3, 1, 2)  # (T,3,128, 128)\r\n    target_sketches = T_target_sketches.unsqueeze(0).cuda()  # (1,T,3,128, 128)\r\n\r\n    # 2.lower-half masked face\r\n    ori_face_img = torch.FloatTensor(cv2.resize(T_crop_face[2], (img_size, img_size)) / 255).permute(2, 0, 1).unsqueeze(\r\n        0).unsqueeze(0).cuda()  #(1,1,3,H, W)\r\n\r\n    # 3. render the full face\r\n    # require (1,1,3,H,W)   (1,T,3,H,W)  (1,N,3,H,W)   (1,N,3,H,W)  (1,1,1,h,w)\r\n    # return  (1,3,H,W)\r\n    with torch.no_grad():\r\n        generated_face, _, _, _ = renderer(ori_face_img, target_sketches, ref_imgs, ref_img_sketches,\r\n                                                    T_mels[:, 2].unsqueeze(0))  # T=1\r\n    gen_face = (generated_face.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)  # (H,W,3)\r\n\r\n    # 4. paste each generated face\r\n    y1, y2, x1, x2 = T_ori_face_coordinates[2][1]  # coordinates of face bounding box\r\n    original_background = T_input_frame[2].copy()\r\n    T_input_frame[2][y1:y2, x1:x2] = cv2.resize(gen_face,(x2 - x1, y2 - y1))  #resize and paste generated face\r\n    # 5. post-process\r\n    full = merge_face_contour_only(original_background, T_input_frame[2], T_ori_face_coordinates[2][1],fa)   #(H,W,3)\r\n    # 6.output\r\n    full = np.concatenate([show_sketch, full], axis=1)\r\n    out_stream.write(full)\r\n    if batch_idx == 0:\r\n        out_stream.write(full)\r\n        out_stream.write(full)\r\nout_stream.release()\r\ncommand = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(input_audio_path, '{}/result.avi'.format(temp_dir), outfile_path)\r\nsubprocess.call(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\r\nprint(\"succeed output results to:\", outfile_path)\r\n\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u8ba1\u7b97\u5f97\u5206\u7565\uff0c\u8be6\u89c1\u6e90\u4ee3\u7801"}),"\n",(0,t.jsx)(e.h4,{id:"W2l",children:"Wav2Lip"}),"\n",(0,t.jsx)(e.h4,{id:"fd",children:"\u9762\u90e8\u68c0\u6d4b\u6a21\u578b"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport math\r\n\r\n\r\ndef conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\r\n    \"3x3 convolution with padding\"\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\r\n                     stride=strd, padding=padding, bias=bias)\r\n\r\n\r\nclass ConvBlock(nn.Module):\r\n    def __init__(self, in_planes, out_planes):\r\n        super(ConvBlock, self).__init__()\r\n        self.bn1 = nn.BatchNorm2d(in_planes)\r\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\r\n        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\r\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\r\n        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\r\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\r\n\r\n        if in_planes != out_planes:\r\n            self.downsample = nn.Sequential(\r\n                nn.BatchNorm2d(in_planes),\r\n                nn.ReLU(True),\r\n                nn.Conv2d(in_planes, out_planes,\r\n                          kernel_size=1, stride=1, bias=False),\r\n            )\r\n        else:\r\n            self.downsample = None\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out1 = self.bn1(x)\r\n        out1 = F.relu(out1, True)\r\n        out1 = self.conv1(out1)\r\n\r\n        out2 = self.bn2(out1)\r\n        out2 = F.relu(out2, True)\r\n        out2 = self.conv2(out2)\r\n\r\n        out3 = self.bn3(out2)\r\n        out3 = F.relu(out3, True)\r\n        out3 = self.conv3(out3)\r\n\r\n        out3 = torch.cat((out1, out2, out3), 1)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(residual)\r\n\r\n        out3 += residual\r\n\r\n        return out3\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass HourGlass(nn.Module):\r\n    def __init__(self, num_modules, depth, num_features):\r\n        super(HourGlass, self).__init__()\r\n        self.num_modules = num_modules\r\n        self.depth = depth\r\n        self.features = num_features\r\n\r\n        self._generate_network(self.depth)\r\n\r\n    def _generate_network(self, level):\r\n        self.add_module('b1_' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        self.add_module('b2_' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        if level > 1:\r\n            self._generate_network(level - 1)\r\n        else:\r\n            self.add_module('b2_plus_' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        self.add_module('b3_' + str(level), ConvBlock(self.features, self.features))\r\n\r\n    def _forward(self, level, inp):\r\n        # Upper branch\r\n        up1 = inp\r\n        up1 = self._modules['b1_' + str(level)](up1)\r\n\r\n        # Lower branch\r\n        low1 = F.avg_pool2d(inp, 2, stride=2)\r\n        low1 = self._modules['b2_' + str(level)](low1)\r\n\r\n        if level > 1:\r\n            low2 = self._forward(level - 1, low1)\r\n        else:\r\n            low2 = low1\r\n            low2 = self._modules['b2_plus_' + str(level)](low2)\r\n\r\n        low3 = low2\r\n        low3 = self._modules['b3_' + str(level)](low3)\r\n\r\n        up2 = F.interpolate(low3, scale_factor=2, mode='nearest')\r\n\r\n        return up1 + up2\r\n\r\n    def forward(self, x):\r\n        return self._forward(self.depth, x)\r\n\r\n\r\nclass FAN(nn.Module):\r\n\r\n    def __init__(self, num_modules=1):\r\n        super(FAN, self).__init__()\r\n        self.num_modules = num_modules\r\n\r\n        # Base part\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.conv2 = ConvBlock(64, 128)\r\n        self.conv3 = ConvBlock(128, 128)\r\n        self.conv4 = ConvBlock(128, 256)\r\n\r\n        # Stacking part\r\n        for hg_module in range(self.num_modules):\r\n            self.add_module('m' + str(hg_module), HourGlass(1, 4, 256))\r\n            self.add_module('top_m_' + str(hg_module), ConvBlock(256, 256))\r\n            self.add_module('conv_last' + str(hg_module),\r\n                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\r\n            self.add_module('bn_end' + str(hg_module), nn.BatchNorm2d(256))\r\n            self.add_module('l' + str(hg_module), nn.Conv2d(256,\r\n                                                            68, kernel_size=1, stride=1, padding=0))\r\n\r\n            if hg_module < self.num_modules - 1:\r\n                self.add_module(\r\n                    'bl' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\r\n                self.add_module('al' + str(hg_module), nn.Conv2d(68,\r\n                                                                 256, kernel_size=1, stride=1, padding=0))\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.bn1(self.conv1(x)), True)\r\n        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\r\n        x = self.conv3(x)\r\n        x = self.conv4(x)\r\n\r\n        previous = x\r\n\r\n        outputs = []\r\n        for i in range(self.num_modules):\r\n            hg = self._modules['m' + str(i)](previous)\r\n\r\n            ll = hg\r\n            ll = self._modules['top_m_' + str(i)](ll)\r\n\r\n            ll = F.relu(self._modules['bn_end' + str(i)]\r\n                        (self._modules['conv_last' + str(i)](ll)), True)\r\n\r\n            # Predict heatmaps\r\n            tmp_out = self._modules['l' + str(i)](ll)\r\n            outputs.append(tmp_out)\r\n\r\n            if i < self.num_modules - 1:\r\n                ll = self._modules['bl' + str(i)](ll)\r\n                tmp_out_ = self._modules['al' + str(i)](tmp_out)\r\n                previous = previous + ll + tmp_out_\r\n\r\n        return outputs\r\n\r\n\r\nclass ResNetDepth(nn.Module):\r\n\r\n    def __init__(self, block=Bottleneck, layers=[3, 8, 36, 3], num_classes=68):\r\n        self.inplanes = 64\r\n        super(ResNetDepth, self).__init__()\r\n        self.conv1 = nn.Conv2d(3 + 68, 64, kernel_size=7, stride=2, padding=3,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\r\n        self.avgpool = nn.AvgPool2d(7)\r\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(planes * block.expansion),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n\r\n        x = self.avgpool(x)\r\n        x = x.view(x.size(0), -1)\r\n        x = self.fc(x)\r\n\r\n        return x\n"})}),"\n",(0,t.jsx)(e.h4,{id:"conv",children:"conv"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nclass Conv2d(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\r\n                            nn.BatchNorm2d(cout)\r\n                            )\r\n        self.act = nn.ReLU()\r\n        self.residual = residual\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        if self.residual:\r\n            out += x\r\n        return self.act(out)\r\n\r\nclass nonorm_Conv2d(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\r\n                            )\r\n        self.act = nn.LeakyReLU(0.01, inplace=True)\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        return self.act(out)\r\n\r\nclass Conv2dTranspose(nn.Module):\r\n    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.conv_block = nn.Sequential(\r\n                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\r\n                            nn.BatchNorm2d(cout)\r\n                            )\r\n        self.act = nn.ReLU()\r\n\r\n    def forward(self, x):\r\n        out = self.conv_block(x)\r\n        return self.act(out)\n"})}),"\n",(0,t.jsx)(e.h4,{id:"sync",children:"syncnet"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nfrom .conv import Conv2d\r\n\r\nclass SyncNet_color(nn.Module):\r\n    def __init__(self):\r\n        super(SyncNet_color, self).__init__()\r\n\r\n        self.face_encoder = nn.Sequential(\r\n            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\r\n\r\n            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\r\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\r\n\r\n        self.audio_encoder = nn.Sequential(\r\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\r\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\r\n\r\n    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\r\n        face_embedding = self.face_encoder(face_sequences)\r\n        audio_embedding = self.audio_encoder(audio_sequences)\r\n\r\n        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\r\n        face_embedding = face_embedding.view(face_embedding.size(0), -1)\r\n\r\n        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\r\n        face_embedding = F.normalize(face_embedding, p=2, dim=1)\r\n\r\n\r\n        return audio_embedding, face_embedding\n"})}),"\n",(0,t.jsx)(e.h4,{id:"w2l",children:"wav2Lip"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"import torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nimport math\r\n\r\nfrom .conv import Conv2dTranspose, Conv2d, nonorm_Conv2d\r\n\r\nclass Wav2Lip(nn.Module):\r\n    def __init__(self):\r\n        super(Wav2Lip, self).__init__()\r\n\r\n        self.face_encoder_blocks = nn.ModuleList([\r\n            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\r\n\r\n            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\r\n\r\n            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\r\n\r\n            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\r\n\r\n            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\r\n\r\n            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\r\n            \r\n            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\r\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\r\n\r\n        self.audio_encoder = nn.Sequential(\r\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n\r\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\r\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\r\n\r\n        self.face_decoder_blocks = nn.ModuleList([\r\n            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\r\n\r\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\r\n\r\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\r\n\r\n            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\r\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\r\n\r\n            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\r\n\r\n            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\r\n\r\n            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\r\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\r\n\r\n        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\r\n            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\r\n            nn.Sigmoid()) \r\n\r\n    def forward(self, audio_sequences, face_sequences):\r\n        # audio_sequences = (B, T, 1, 80, 16)\r\n        B = audio_sequences.size(0)\r\n\r\n        input_dim_size = len(face_sequences.size())\r\n        if input_dim_size > 4:\r\n            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\r\n            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\r\n\r\n        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\r\n\r\n        feats = []\r\n        x = face_sequences\r\n        for f in self.face_encoder_blocks:\r\n            x = f(x)\r\n            feats.append(x)\r\n\r\n        x = audio_embedding\r\n        for f in self.face_decoder_blocks:\r\n            x = f(x)\r\n            try:\r\n                x = torch.cat((x, feats[-1]), dim=1)\r\n            except Exception as e:\r\n                print(x.size())\r\n                print(feats[-1].size())\r\n                raise e\r\n            \r\n            feats.pop()\r\n\r\n        x = self.output_block(x)\r\n\r\n        if input_dim_size > 4:\r\n            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\r\n            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\r\n\r\n        else:\r\n            outputs = x\r\n            \r\n        return outputs\r\n\r\nclass Wav2Lip_disc_qual(nn.Module):\r\n    def __init__(self):\r\n        super(Wav2Lip_disc_qual, self).__init__()\r\n\r\n        self.face_encoder_blocks = nn.ModuleList([\r\n            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\r\n\r\n            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\r\n            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\r\n\r\n            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\r\n            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\r\n\r\n            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\r\n            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\r\n\r\n            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\r\n            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\r\n\r\n            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\r\n            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\r\n            \r\n            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\r\n            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\r\n\r\n        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\r\n        self.label_noise = .0\r\n\r\n    def get_lower_half(self, face_sequences):\r\n        return face_sequences[:, :, face_sequences.size(2)//2:]\r\n\r\n    def to_2d(self, face_sequences):\r\n        B = face_sequences.size(0)\r\n        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\r\n        return face_sequences\r\n\r\n    def perceptual_forward(self, false_face_sequences):\r\n        false_face_sequences = self.to_2d(false_face_sequences)\r\n        false_face_sequences = self.get_lower_half(false_face_sequences)\r\n\r\n        false_feats = false_face_sequences\r\n        for f in self.face_encoder_blocks:\r\n            false_feats = f(false_feats)\r\n\r\n        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \r\n                                        torch.ones((len(false_feats), 1)).cuda())\r\n\r\n        return false_pred_loss\r\n\r\n    def forward(self, face_sequences):\r\n        face_sequences = self.to_2d(face_sequences)\r\n        face_sequences = self.get_lower_half(face_sequences)\r\n\r\n        x = face_sequences\r\n        for f in self.face_encoder_blocks:\r\n            x = f(x)\r\n\r\n        return self.binary_pred(x).view(len(x), -1)\r\n\n"})}),"\n",(0,t.jsx)(e.h4,{id:"gvff",children:"gen_videos_from_filelist"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"from os import listdir, path\r\nimport numpy as np\r\nimport scipy, cv2, os, sys, argparse\r\nimport dlib, json, subprocess\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\nimport torch\r\n\r\nsys.path.append('../')\r\nimport audio\r\nimport face_detection\r\nfrom models import Wav2Lip\r\n\r\nparser = argparse.ArgumentParser(description='Code to generate results for test filelists')\r\n\r\nparser.add_argument('--filelist', type=str, \r\n\t\t\t\t\thelp='Filepath of filelist file to read', required=True)\r\nparser.add_argument('--results_dir', type=str, help='Folder to save all results into', \r\n\t\t\t\t\t\t\t\t\trequired=True)\r\nparser.add_argument('--data_root', type=str, required=True)\r\nparser.add_argument('--checkpoint_path', type=str, \r\n\t\t\t\t\thelp='Name of saved checkpoint to load weights from', required=True)\r\n\r\nparser.add_argument('--pads', nargs='+', type=int, default=[0, 0, 0, 0], \r\n\t\t\t\t\thelp='Padding (top, bottom, left, right)')\r\nparser.add_argument('--face_det_batch_size', type=int, \r\n\t\t\t\t\thelp='Single GPU batch size for face detection', default=64)\r\nparser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip', default=128)\r\n\r\n# parser.add_argument('--resize_factor', default=1, type=int)\r\n\r\nargs = parser.parse_args()\r\nargs.img_size = 96\r\n\r\ndef get_smoothened_boxes(boxes, T):\r\n\tfor i in range(len(boxes)):\r\n\t\tif i + T > len(boxes):\r\n\t\t\twindow = boxes[len(boxes) - T:]\r\n\t\telse:\r\n\t\t\twindow = boxes[i : i + T]\r\n\t\tboxes[i] = np.mean(window, axis=0)\r\n\treturn boxes\r\n\r\ndef face_detect(images):\r\n\tbatch_size = args.face_det_batch_size\r\n\t\r\n\twhile 1:\r\n\t\tpredictions = []\r\n\t\ttry:\r\n\t\t\tfor i in range(0, len(images), batch_size):\r\n\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\r\n\t\texcept RuntimeError:\r\n\t\t\tif batch_size == 1:\r\n\t\t\t\traise RuntimeError('Image too big to run face detection on GPU')\r\n\t\t\tbatch_size //= 2\r\n\t\t\targs.face_det_batch_size = batch_size\r\n\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\r\n\t\t\tcontinue\r\n\t\tbreak\r\n\r\n\tresults = []\r\n\tpady1, pady2, padx1, padx2 = args.pads\r\n\tfor rect, image in zip(predictions, images):\r\n\t\tif rect is None:\r\n\t\t\traise ValueError('Face not detected!')\r\n\r\n\t\ty1 = max(0, rect[1] - pady1)\r\n\t\ty2 = min(image.shape[0], rect[3] + pady2)\r\n\t\tx1 = max(0, rect[0] - padx1)\r\n\t\tx2 = min(image.shape[1], rect[2] + padx2)\r\n\t\t\r\n\t\tresults.append([x1, y1, x2, y2])\r\n\r\n\tboxes = get_smoothened_boxes(np.array(results), T=5)\r\n\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2), True] for image, (x1, y1, x2, y2) in zip(images, boxes)]\r\n\r\n\treturn results \r\n\r\ndef datagen(frames, face_det_results, mels):\r\n\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\r\n\r\n\tfor i, m in enumerate(mels):\r\n\t\tif i >= len(frames): raise ValueError('Equal or less lengths only')\r\n\r\n\t\tframe_to_save = frames[i].copy()\r\n\t\tface, coords, valid_frame = face_det_results[i].copy()\r\n\t\tif not valid_frame:\r\n\t\t\tcontinue\r\n\r\n\t\tface = cv2.resize(face, (args.img_size, args.img_size))\r\n\t\t\t\r\n\t\timg_batch.append(face)\r\n\t\tmel_batch.append(m)\r\n\t\tframe_batch.append(frame_to_save)\r\n\t\tcoords_batch.append(coords)\r\n\r\n\t\tif len(img_batch) >= args.wav2lip_batch_size:\r\n\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\r\n\r\n\t\t\timg_masked = img_batch.copy()\r\n\t\t\timg_masked[:, args.img_size//2:] = 0\r\n\r\n\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\r\n\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\r\n\r\n\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\r\n\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\r\n\r\n\tif len(img_batch) > 0:\r\n\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\r\n\r\n\t\timg_masked = img_batch.copy()\r\n\t\timg_masked[:, args.img_size//2:] = 0\r\n\r\n\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\r\n\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\r\n\r\n\t\tyield img_batch, mel_batch, frame_batch, coords_batch\r\n\r\nfps = 25\r\nmel_step_size = 16\r\nmel_idx_multiplier = 80./fps\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nprint('Using {} for inference.'.format(device))\r\n\r\ndetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \r\n\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\r\n\r\ndef _load(checkpoint_path):\r\n\tif device == 'cuda':\r\n\t\tcheckpoint = torch.load(checkpoint_path)\r\n\telse:\r\n\t\tcheckpoint = torch.load(checkpoint_path,\r\n\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\r\n\treturn checkpoint\r\n\r\ndef load_model(path):\r\n\tmodel = Wav2Lip()\r\n\tprint(\"Load checkpoint from: {}\".format(path))\r\n\tcheckpoint = _load(path)\r\n\ts = checkpoint[\"state_dict\"]\r\n\tnew_s = {}\r\n\tfor k, v in s.items():\r\n\t\tnew_s[k.replace('module.', '')] = v\r\n\tmodel.load_state_dict(new_s)\r\n\r\n\tmodel = model.to(device)\r\n\treturn model.eval()\r\n\r\nmodel = load_model(args.checkpoint_path)\r\n\r\ndef main():\r\n\tassert args.data_root is not None\r\n\tdata_root = args.data_root\r\n\r\n\tif not os.path.isdir(args.results_dir): os.makedirs(args.results_dir)\r\n\r\n\twith open(args.filelist, 'r') as filelist:\r\n\t\tlines = filelist.readlines()\r\n\r\n\tfor idx, line in enumerate(tqdm(lines)):\r\n\t\taudio_src, video = line.strip().split()\r\n\r\n\t\taudio_src = os.path.join(data_root, audio_src) + '.mp4'\r\n\t\tvideo = os.path.join(data_root, video) + '.mp4'\r\n\r\n\t\tcommand = 'ffmpeg -loglevel panic -y -i {} -strict -2 {}'.format(audio_src, '../temp/temp.wav')\r\n\t\tsubprocess.call(command, shell=True)\r\n\t\ttemp_audio = '../temp/temp.wav'\r\n\r\n\t\twav = audio.load_wav(temp_audio, 16000)\r\n\t\tmel = audio.melspectrogram(wav)\r\n\t\tif np.isnan(mel.reshape(-1)).sum() > 0:\r\n\t\t\tcontinue\r\n\r\n\t\tmel_chunks = []\r\n\t\ti = 0\r\n\t\twhile 1:\r\n\t\t\tstart_idx = int(i * mel_idx_multiplier)\r\n\t\t\tif start_idx + mel_step_size > len(mel[0]):\r\n\t\t\t\tbreak\r\n\t\t\tmel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\r\n\t\t\ti += 1\r\n\r\n\t\tvideo_stream = cv2.VideoCapture(video)\r\n\t\t\t\r\n\t\tfull_frames = []\r\n\t\twhile 1:\r\n\t\t\tstill_reading, frame = video_stream.read()\r\n\t\t\tif not still_reading or len(full_frames) > len(mel_chunks):\r\n\t\t\t\tvideo_stream.release()\r\n\t\t\t\tbreak\r\n\t\t\tfull_frames.append(frame)\r\n\r\n\t\tif len(full_frames) < len(mel_chunks):\r\n\t\t\tcontinue\r\n\r\n\t\tfull_frames = full_frames[:len(mel_chunks)]\r\n\r\n\t\ttry:\r\n\t\t\tface_det_results = face_detect(full_frames.copy())\r\n\t\texcept ValueError as e:\r\n\t\t\tcontinue\r\n\r\n\t\tbatch_size = args.wav2lip_batch_size\r\n\t\tgen = datagen(full_frames.copy(), face_det_results, mel_chunks)\r\n\r\n\t\tfor i, (img_batch, mel_batch, frames, coords) in enumerate(gen):\r\n\t\t\tif i == 0:\r\n\t\t\t\tframe_h, frame_w = full_frames[0].shape[:-1]\r\n\t\t\t\tout = cv2.VideoWriter('../temp/result.avi', \r\n\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\r\n\r\n\t\t\timg_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\r\n\t\t\tmel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\r\n\r\n\t\t\twith torch.no_grad():\r\n\t\t\t\tpred = model(mel_batch, img_batch)\r\n\t\t\t\t\t\r\n\r\n\t\t\tpred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\r\n\t\t\t\r\n\t\t\tfor pl, f, c in zip(pred, frames, coords):\r\n\t\t\t\ty1, y2, x1, x2 = c\r\n\t\t\t\tpl = cv2.resize(pl.astype(np.uint8), (x2 - x1, y2 - y1))\r\n\t\t\t\tf[y1:y2, x1:x2] = pl\r\n\t\t\t\tout.write(f)\r\n\r\n\t\tout.release()\r\n\r\n\t\tvid = os.path.join(args.results_dir, '{}.mp4'.format(idx))\r\n\r\n\t\tcommand = 'ffmpeg -loglevel panic -y -i {} -i {} -strict -2 -q:v 1 {}'.format(temp_audio, \r\n\t\t\t\t\t\t\t\t'../temp/result.avi', vid)\r\n\t\tsubprocess.call(command, shell=True)\r\n\r\nif __name__ == '__main__':\r\n\tmain()\r\n\n"})}),"\n",(0,t.jsx)(e.h4,{id:"rvi",children:"real_videos_inference"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-py",children:"from os import listdir, path\r\nimport numpy as np\r\nimport scipy, cv2, os, sys, argparse\r\nimport dlib, json, subprocess\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\nimport torch\r\n\r\nsys.path.append('../')\r\nimport audio\r\nimport face_detection\r\nfrom models import Wav2Lip\r\n\r\nparser = argparse.ArgumentParser(description='Code to generate results on ReSyncED evaluation set')\r\n\r\nparser.add_argument('--mode', type=str, \r\n\t\t\t\t\thelp='random | dubbed | tts', required=True)\r\n\r\nparser.add_argument('--filelist', type=str, \r\n\t\t\t\t\thelp='Filepath of filelist file to read', default=None)\r\n\r\nparser.add_argument('--results_dir', type=str, help='Folder to save all results into', \r\n\t\t\t\t\t\t\t\t\trequired=True)\r\nparser.add_argument('--data_root', type=str, required=True)\r\nparser.add_argument('--checkpoint_path', type=str, \r\n\t\t\t\t\thelp='Name of saved checkpoint to load weights from', required=True)\r\nparser.add_argument('--pads', nargs='+', type=int, default=[0, 10, 0, 0], \r\n\t\t\t\t\thelp='Padding (top, bottom, left, right)')\r\n\r\nparser.add_argument('--face_det_batch_size', type=int, \r\n\t\t\t\t\thelp='Single GPU batch size for face detection', default=16)\r\n\r\nparser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip', default=128)\r\nparser.add_argument('--face_res', help='Approximate resolution of the face at which to test', default=180)\r\nparser.add_argument('--min_frame_res', help='Do not downsample further below this frame resolution', default=480)\r\nparser.add_argument('--max_frame_res', help='Downsample to at least this frame resolution', default=720)\r\n# parser.add_argument('--resize_factor', default=1, type=int)\r\n\r\nargs = parser.parse_args()\r\nargs.img_size = 96\r\n\r\ndef get_smoothened_boxes(boxes, T):\r\n\tfor i in range(len(boxes)):\r\n\t\tif i + T > len(boxes):\r\n\t\t\twindow = boxes[len(boxes) - T:]\r\n\t\telse:\r\n\t\t\twindow = boxes[i : i + T]\r\n\t\tboxes[i] = np.mean(window, axis=0)\r\n\treturn boxes\r\n\r\ndef rescale_frames(images):\r\n\trect = detector.get_detections_for_batch(np.array([images[0]]))[0]\r\n\tif rect is None:\r\n\t\traise ValueError('Face not detected!')\r\n\th, w = images[0].shape[:-1]\r\n\r\n\tx1, y1, x2, y2 = rect\r\n\r\n\tface_size = max(np.abs(y1 - y2), np.abs(x1 - x2))\r\n\r\n\tdiff = np.abs(face_size - args.face_res)\r\n\tfor factor in range(2, 16):\r\n\t\tdownsampled_res = face_size // factor\r\n\t\tif min(h//factor, w//factor) < args.min_frame_res: break \r\n\t\tif np.abs(downsampled_res - args.face_res) >= diff: break\r\n\r\n\tfactor -= 1\r\n\tif factor == 1: return images\r\n\r\n\treturn [cv2.resize(im, (im.shape[1]//(factor), im.shape[0]//(factor))) for im in images]\r\n\r\n\r\ndef face_detect(images):\r\n\tbatch_size = args.face_det_batch_size\r\n\timages = rescale_frames(images)\r\n\r\n\twhile 1:\r\n\t\tpredictions = []\r\n\t\ttry:\r\n\t\t\tfor i in range(0, len(images), batch_size):\r\n\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\r\n\t\texcept RuntimeError:\r\n\t\t\tif batch_size == 1:\r\n\t\t\t\traise RuntimeError('Image too big to run face detection on GPU')\r\n\t\t\tbatch_size //= 2\r\n\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\r\n\t\t\tcontinue\r\n\t\tbreak\r\n\r\n\tresults = []\r\n\tpady1, pady2, padx1, padx2 = args.pads\r\n\tfor rect, image in zip(predictions, images):\r\n\t\tif rect is None:\r\n\t\t\traise ValueError('Face not detected!')\r\n\r\n\t\ty1 = max(0, rect[1] - pady1)\r\n\t\ty2 = min(image.shape[0], rect[3] + pady2)\r\n\t\tx1 = max(0, rect[0] - padx1)\r\n\t\tx2 = min(image.shape[1], rect[2] + padx2)\r\n\t\t\r\n\t\tresults.append([x1, y1, x2, y2])\r\n\r\n\tboxes = get_smoothened_boxes(np.array(results), T=5)\r\n\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2), True] for image, (x1, y1, x2, y2) in zip(images, boxes)]\r\n\r\n\treturn results, images \r\n\r\ndef datagen(frames, face_det_results, mels):\r\n\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\r\n\r\n\tfor i, m in enumerate(mels):\r\n\t\tif i >= len(frames): raise ValueError('Equal or less lengths only')\r\n\r\n\t\tframe_to_save = frames[i].copy()\r\n\t\tface, coords, valid_frame = face_det_results[i].copy()\r\n\t\tif not valid_frame:\r\n\t\t\tcontinue\r\n\r\n\t\tface = cv2.resize(face, (args.img_size, args.img_size))\r\n\t\t\t\r\n\t\timg_batch.append(face)\r\n\t\tmel_batch.append(m)\r\n\t\tframe_batch.append(frame_to_save)\r\n\t\tcoords_batch.append(coords)\r\n\r\n\t\tif len(img_batch) >= args.wav2lip_batch_size:\r\n\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\r\n\r\n\t\t\timg_masked = img_batch.copy()\r\n\t\t\timg_masked[:, args.img_size//2:] = 0\r\n\r\n\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\r\n\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\r\n\r\n\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\r\n\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\r\n\r\n\tif len(img_batch) > 0:\r\n\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\r\n\r\n\t\timg_masked = img_batch.copy()\r\n\t\timg_masked[:, args.img_size//2:] = 0\r\n\r\n\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\r\n\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\r\n\r\n\t\tyield img_batch, mel_batch, frame_batch, coords_batch\r\n\r\ndef increase_frames(frames, l):\r\n\t## evenly duplicating frames to increase length of video\r\n\twhile len(frames) < l:\r\n\t\tdup_every = float(l) / len(frames)\r\n\r\n\t\tfinal_frames = []\r\n\t\tnext_duplicate = 0.\r\n\r\n\t\tfor i, f in enumerate(frames):\r\n\t\t\tfinal_frames.append(f)\r\n\r\n\t\t\tif int(np.ceil(next_duplicate)) == i:\r\n\t\t\t\tfinal_frames.append(f)\r\n\r\n\t\t\tnext_duplicate += dup_every\r\n\r\n\t\tframes = final_frames\r\n\r\n\treturn frames[:l]\r\n\r\nmel_step_size = 16\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nprint('Using {} for inference.'.format(device))\r\n\r\ndetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \r\n\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\r\n\r\ndef _load(checkpoint_path):\r\n\tif device == 'cuda':\r\n\t\tcheckpoint = torch.load(checkpoint_path)\r\n\telse:\r\n\t\tcheckpoint = torch.load(checkpoint_path,\r\n\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\r\n\treturn checkpoint\r\n\r\ndef load_model(path):\r\n\tmodel = Wav2Lip()\r\n\tprint(\"Load checkpoint from: {}\".format(path))\r\n\tcheckpoint = _load(path)\r\n\ts = checkpoint[\"state_dict\"]\r\n\tnew_s = {}\r\n\tfor k, v in s.items():\r\n\t\tnew_s[k.replace('module.', '')] = v\r\n\tmodel.load_state_dict(new_s)\r\n\r\n\tmodel = model.to(device)\r\n\treturn model.eval()\r\n\r\nmodel = load_model(args.checkpoint_path)\r\n\r\ndef main():\r\n\tif not os.path.isdir(args.results_dir): os.makedirs(args.results_dir)\r\n\r\n\tif args.mode == 'dubbed':\r\n\t\tfiles = listdir(args.data_root)\r\n\t\tlines = ['{} {}'.format(f, f) for f in files]\r\n\r\n\telse:\r\n\t\tassert args.filelist is not None\r\n\t\twith open(args.filelist, 'r') as filelist:\r\n\t\t\tlines = filelist.readlines()\r\n\r\n\tfor idx, line in enumerate(tqdm(lines)):\r\n\t\tvideo, audio_src = line.strip().split()\r\n\r\n\t\taudio_src = os.path.join(args.data_root, audio_src)\r\n\t\tvideo = os.path.join(args.data_root, video)\r\n\r\n\t\tcommand = 'ffmpeg -loglevel panic -y -i {} -strict -2 {}'.format(audio_src, '../temp/temp.wav')\r\n\t\tsubprocess.call(command, shell=True)\r\n\t\ttemp_audio = '../temp/temp.wav'\r\n\r\n\t\twav = audio.load_wav(temp_audio, 16000)\r\n\t\tmel = audio.melspectrogram(wav)\r\n\r\n\t\tif np.isnan(mel.reshape(-1)).sum() > 0:\r\n\t\t\traise ValueError('Mel contains nan!')\r\n\r\n\t\tvideo_stream = cv2.VideoCapture(video)\r\n\r\n\t\tfps = video_stream.get(cv2.CAP_PROP_FPS)\r\n\t\tmel_idx_multiplier = 80./fps\r\n\r\n\t\tfull_frames = []\r\n\t\twhile 1:\r\n\t\t\tstill_reading, frame = video_stream.read()\r\n\t\t\tif not still_reading:\r\n\t\t\t\tvideo_stream.release()\r\n\t\t\t\tbreak\r\n\r\n\t\t\tif min(frame.shape[:-1]) > args.max_frame_res:\r\n\t\t\t\th, w = frame.shape[:-1]\r\n\t\t\t\tscale_factor = min(h, w) / float(args.max_frame_res)\r\n\t\t\t\th = int(h/scale_factor)\r\n\t\t\t\tw = int(w/scale_factor)\r\n\r\n\t\t\t\tframe = cv2.resize(frame, (w, h))\r\n\t\t\tfull_frames.append(frame)\r\n\r\n\t\tmel_chunks = []\r\n\t\ti = 0\r\n\t\twhile 1:\r\n\t\t\tstart_idx = int(i * mel_idx_multiplier)\r\n\t\t\tif start_idx + mel_step_size > len(mel[0]):\r\n\t\t\t\tbreak\r\n\t\t\tmel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\r\n\t\t\ti += 1\r\n\r\n\t\tif len(full_frames) < len(mel_chunks):\r\n\t\t\tif args.mode == 'tts':\r\n\t\t\t\tfull_frames = increase_frames(full_frames, len(mel_chunks))\r\n\t\t\telse:\r\n\t\t\t\traise ValueError('#Frames, audio length mismatch')\r\n\r\n\t\telse:\r\n\t\t\tfull_frames = full_frames[:len(mel_chunks)]\r\n\r\n\t\ttry:\r\n\t\t\tface_det_results, full_frames = face_detect(full_frames.copy())\r\n\t\texcept ValueError as e:\r\n\t\t\tcontinue\r\n\r\n\t\tbatch_size = args.wav2lip_batch_size\r\n\t\tgen = datagen(full_frames.copy(), face_det_results, mel_chunks)\r\n\r\n\t\tfor i, (img_batch, mel_batch, frames, coords) in enumerate(gen):\r\n\t\t\tif i == 0:\r\n\t\t\t\tframe_h, frame_w = full_frames[0].shape[:-1]\r\n\r\n\t\t\t\tout = cv2.VideoWriter('../temp/result.avi', \r\n\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\r\n\r\n\t\t\timg_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\r\n\t\t\tmel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\r\n\r\n\t\t\twith torch.no_grad():\r\n\t\t\t\tpred = model(mel_batch, img_batch)\r\n\t\t\t\t\t\r\n\r\n\t\t\tpred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\r\n\t\t\t\r\n\t\t\tfor pl, f, c in zip(pred, frames, coords):\r\n\t\t\t\ty1, y2, x1, x2 = c\r\n\t\t\t\tpl = cv2.resize(pl.astype(np.uint8), (x2 - x1, y2 - y1))\r\n\t\t\t\tf[y1:y2, x1:x2] = pl\r\n\t\t\t\tout.write(f)\r\n\r\n\t\tout.release()\r\n\r\n\t\tvid = os.path.join(args.results_dir, '{}.mp4'.format(idx))\r\n\t\tcommand = 'ffmpeg -loglevel panic -y -i {} -i {} -strict -2 -q:v 1 {}'.format('../temp/temp.wav', \r\n\t\t\t\t\t\t\t\t'../temp/result.avi', vid)\r\n\t\tsubprocess.call(command, shell=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n\tmain()\r\n\n"})}),"\n",(0,t.jsx)(e.p,{children:"\u8ba1\u7b97\u5f97\u5206\u7565\uff0c\u8be6\u60c5\u53c2\u89c1\u6e90\u4ee3\u7801"})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>l});var a=r(6540);const t={},i=a.createContext(t);function s(n){const e=a.useContext(i);return a.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);