"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5010],{7635:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"time-forcasting/agriculture","title":"agriculture","description":"\u519c\u4e1a\u751f\u9c9c\u7c7b","source":"@site/docs/time-forcasting/agriculture.md","sourceDirName":"time-forcasting","slug":"/time-forcasting/agriculture","permalink":"/docs/time-forcasting/agriculture","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"intro","permalink":"/docs/intro"},"next":{"title":"cruise","permalink":"/docs/time-forcasting/cruise"}}');var s=n(4848),a=n(8453);const i={},l=void 0,o={},d=[{value:"\u519c\u4e1a\u751f\u9c9c\u7c7b",id:"\u519c\u4e1a\u751f\u9c9c\u7c7b",level:2},{value:"\u7ebf\u6027\u9884\u6d4b",id:"lf",level:4},{value:"\u51b3\u7b56\u6811",id:"dt",level:4},{value:"\u968f\u673a\u68ee\u6797",id:"rf",level:4},{value:"Adaboost",id:"ab",level:4},{value:"GBRT",id:"gbrt",level:4},{value:"XGBoost",id:"xb",level:4},{value:"lightGBM",id:"lg",level:4},{value:"SVR",id:"svr",level:4},{value:"lasso\u56de\u5f52",id:"las",level:4},{value:"LSTM",id:"lstm",level:4},{value:"CNN-GRU-AE",id:"cga",level:4},{value:"Pso_Cnn_BiLSTM_MultAE",id:"pcbma",level:4},{value:"woa_CNN_BiLSTM_MultAE",id:"woa",level:4},{value:"ssa_CNN_BiLSTM_MultAE",id:"ssa",level:4},{value:"BiLSTM_MultAE",id:"blma",level:4},{value:"CNN-MultAE",id:"cma",level:4}];function _(r){const e={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...r.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h2,{id:"\u519c\u4e1a\u751f\u9c9c\u7c7b",children:"\u519c\u4e1a\u751f\u9c9c\u7c7b"}),"\n",(0,s.jsx)(e.p,{children:"[\u6e90\u4ee3\u7801\u4e0b\u8f7d](../../agriculture.zip"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.a,{href:"#LYH",children:"\u674e\u5143\u6d69"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#lf",children:"\u7ebf\u6027\u9884\u6d4b"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#dt",children:"\u51b3\u7b56\u6811"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#rf",children:"\u968f\u673a\u68ee\u6797"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#ab",children:"adaboost"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#gbrt",children:"GBRT"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#xb",children:"XGBoost"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#lg",children:"lightGBM"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#svr",children:"SVR"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#las",children:"lasso\u56de\u5f52"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#lstm",children:"LSTM"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#cga",children:"CNN-GRU-AE"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:"\u5218\u68a6\u96c5"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#pcbma",children:"Pso_Cnn_BiLSTM_MultAE"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#woa",children:"woa_CNN_BiLSTM_MultAE"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#ssa",children:"ssa_CNN_BiLSTM_MultAE"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#blma",children:"BiLSTM_MultAE"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"#cma",children:"CNN-MultAE"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"\u673a\u5668\u5b66\u4e60"})}),"\n",(0,s.jsx)(e.p,{children:"\u6570\u636e\u9884\u5904\u7406"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"import pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.neighbors import KNeighborsRegressor\r\nfrom sklearn.ensemble import AdaBoostRegressor\r\nfrom sklearn.tree import DecisionTreeRegressor\r\n\r\nfrom tensorflow.keras import Sequential,layers,losses,utils,Input\r\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout,concatenate,Flatten, Conv1D, MaxPooling1D,Activation,RepeatVector,TimeDistributed\r\nimport tensorflow as tf\r\n# from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Bidirectional,GRU,Lambda,Dot,Concatenate\r\nfrom sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error,mean_absolute_percentage_error\r\n\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib\r\n\r\nfrom sklearn.decomposition import PCA\r\n\r\ndata=pd.read_csv('changping.csv')\r\ncol=[ 'year', 'month', 'day', 'hour', 'PM2.5', 'PM10', 'SO2', 'NO2',\r\n       'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN',  'WSPM','wd']\r\ndatacol=data[col]\r\ndatacol.isnull().sum()\r\n# \u76f4\u63a5\u5220\u9664\u7f3a\u5931\u503c\r\ndatashan=datacol.dropna()\r\n# \u62fc\u63a5\u65f6\u95f4\u4e3a\u4e00\u5217\uff0c\u7ed8\u5236\u4e00\u5e74\u65f6\u95f4\u964d\u6c34\u5206\u5e03\u56fe\r\n# datag['\u65f6\u95f4(\u5e74\u6708\u65e5\u65f6)']=datag['\u5e74(\u5e74)'].map(str)+''+datag['\u6708(\u6708)'].map(str)+''+datag['\u65e5(\u65e5)'].map(str)+''+datag['\u65f6(\u65f6)'].map(str)\r\ndatashan['datatime']=datashan['year'].map(str)+'-'+datashan['month'].map(str)+'-'+datashan['day'].map(str)+'-'+datashan['hour'].map(str)\r\n\r\n# datag=data.copy()\r\ndatashan['datatime']=pd.to_datetime(datashan['datatime'],format='%Y-%m-%d-%H',errors='coerce')\r\ndatashan.index=datashan['datatime']\r\n\r\nchongcol=[ 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO',\r\n       'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\r\ndatag=datashan[chongcol]\r\n# \u5f52\u4e00\u5316\r\nscaler = MinMaxScaler(feature_range=(0, 1))  # minmaxscaler \u65b9\u6cd5\uff0c\u7528\u6765\u505a\u5f52\u4e00\u5316\r\nsel_col = datag.columns\r\n# print(sel_col)\r\nfor col in sel_col:\r\n    datag[col] = scaler.fit_transform(datag[col].values.reshape(-1, 1))  # reshape\u6210\u4e3a\u4e00\u5217\r\nprint(\"Load dataset LEN: \", datag.shape[0])\r\nprint(sel_col)\n"})}),"\n",(0,s.jsx)(e.p,{children:"PCA\u964d\u7ef4"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'X = []\r\nY = []\r\ncolumns_name = list(datag.columns)\r\ny_index = columns_name.index(\'PM2.5\')\r\ntmp = np.array(datag, dtype=np.float32)\r\nprint("The shape of tmp is:")\r\nprint(tmp.shape)\r\npca = PCA(n_components=0.97)# \u4fdd\u8bc1\u964d\u7ef4\u540e\u7684\u6570\u636e\u4fdd\u630190%\u7684\u4fe1\u606f\r\ntmp2 = pca.fit_transform(tmp[:,1:])\r\nprint("The shape of tmp2 is:")\r\nprint(tmp2.shape)  # PCA\u964d\u7ef4y_index\u8981\u6539\uff0c\u6216\u8005\u5148\u63d0\u53d6\u51fa\u6765\r\nprint("input dim is:")\r\nprint(tmp2.shape[1])\n'})}),"\n",(0,s.jsx)(e.p,{children:"\u5212\u5206\u6570\u636e\u96c6"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"seqX=120\r\nseqY=1\r\nday_delay=1\r\ngap=1\r\nX1=[]\r\nfor i in range(tmp2.shape[0] - (seqX+seqY+day_delay)*gap):\r\n        # seqX, seqX+day_delay, seqX+day_delay+seqY\r\n        # format: i+x*gap\r\n        X.append(tmp2[i:(i + seqX*gap):gap, :])  # tmp2\u662fPCA\u964d\u7ef4\u540e\u7684\u7279\u5f81\uff0c\u5982\u679c\u662ftmp\u90a3\u5c31\u662f\u539f\u59cb\u7279\u5f81\r\n        X1.append(tmp2[(i+(seqX+day_delay)*gap): (i+(seqX+day_delay+seqY)*gap): gap,:])\r\n        Y.append(tmp[(i+(seqX+day_delay)*gap): (i+(seqX+day_delay+seqY)*gap): gap, y_index])\r\nX = np.array(X)\r\nY = np.array(Y)\r\nX1=np.array(X1)\r\n\r\nshendu_trainx, shendu_trainy = X[:int(0.8 * 32559)], Y[:int(0.8 * 32559)]  \r\njiqi_trainx, jiqi_trainy = X1[:int(0.8 * 32559)], Y[:int(0.8 * 32559)]  \r\nshendu_testx, shendu_testy = X[int(0.8 * 32559):], Y[int(0.8 * 32559):] \r\njiqi_testx, jiqi_testy = X1[int(0.8 * 32559):], Y[int(0.8 * 32559):] \r\n#\u8f6c\u5316\u6210\u673a\u5668\u5b66\u4e60\u53ef\u7528\u6570\u636e\r\nrealjiqi_trainx=jiqi_trainx.reshape(26047,7)\r\nrealjiqi_testx=jiqi_testx.reshape(6512,7)\r\n\r\n# \u4fdd\u5b58\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6d4b\u8bd5\u6570\u636e\r\n# \u5c06\u8bad\u7ec3\u96c6\u5b58\u8d77\u6765\r\nnptrainx=np.reshape(shendu_trainx,(-1,7))\r\npdtrainx1=np.reshape(nptrainx,(-1,120,7))\r\n# \u4fdd\u5b58\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6d4b\u8bd5\u6570\u636e\r\n# \u5c06\u8bad\u7ec3\u96c6\u5b58\u8d77\u6765\r\nnptrainx=np.reshape(shendu_trainx,(-1,7))\r\npdtrainx1=np.reshape(nptrainx,(-1,120,7))\r\npdshendux_test=pd.DataFrame(nptestx,columns=['t1','t2','t3','t4','t5','t6','t7'])\r\npdshendux_test.to_csv('shendux_test.csv',index=False)\r\npdshenduy_train=pd.DataFrame(shendu_trainy,columns=['y1'])\r\npdshenduy_train.to_csv('shenduy_train.csv',index=False)\r\npdshenduy_test=pd.DataFrame(shendu_testy,columns=['y1'])\r\npdshenduy_test.to_csv('shenduy_test.csv',index=False)\r\n# \u4fdd\u5b58\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6d4b\u8bd5\u96c6\r\npdjiqitrainx=pd.DataFrame(realjiqi_trainx,columns=['t1','t2','t3','t4','t5','t6','t7'])\r\npdjiqitrainx.to_csv('jiqix_train.csv',index=False)\r\npdjiqitestx=pd.DataFrame(realjiqi_testx,columns=['t1','t2','t3','t4','t5','t6','t7'])\r\npdjiqitestx.to_csv('jiqix_test.csv',index=False)\r\npdjiqiy_train=pd.DataFrame(jiqi_trainy,columns=['y1'])\r\npdjiqiy_train.to_csv('jiqiy_train.csv',index=False)\r\npdjiqiy_test=pd.DataFrame(jiqi_testy,columns=['y1'])\r\npdjiqiy_test.to_csv('jiqiy_test.csv',index=False)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u8bfb\u53d6\u6df1\u5ea6\u5b66\u4e60\u6570\u636e"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# \u8bfb\u53d6\u8bad\u7ec3\u96c6\r\ndushendux_train=pd.read_csv('shendux_train.csv')\r\ndushendux_train1=np.array(dushendux_train)\r\ndushendux_train2=np.reshape(dushendux_train1,(-1,120,7))\r\n\r\ndushenduy_train=pd.read_csv('shenduy_train.csv')\r\ndushenduy_train2=np.array(dushenduy_train)\r\n# \u8bfb\u53d6\u6d4b\u8bd5\u96c6\r\ndushendux_test=pd.read_csv('shendux_test.csv')\r\ndushendux_test1=np.array(dushendux_test)\r\ndushendux_test2=np.reshape(dushendux_test1,(-1,120,7))\r\n\r\ndushenduy_test=pd.read_csv('shenduy_test.csv')\r\ndushenduy_test2=np.array(dushenduy_test)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u8bfb\u53d6\u6570\u636e\u673a\u5668\u5b66\u4e60"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# \u8bfb\u53d6\u8bad\u7ec3\u96c6\r\ndujiqix_train=pd.read_csv('jiqix_train.csv')\r\ndujiqiy_train=pd.read_csv('jiqiy_train.csv')\r\n# \u8bfb\u53d6\u6d4b\u8bd5\u96c6\r\ndujiqix_test=pd.read_csv('jiqix_test.csv')\r\ndujiqiy_test=pd.read_csv('jiqiy_test.csv')\n"})}),"\n",(0,s.jsx)(e.h4,{id:"lf",children:"\u7ebf\u6027\u9884\u6d4b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.linear_model import LinearRegression,RidgeCV,SGDRegressor\r\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score   #\u5747\u65b9\u8bef\u5dee\r\n# # # # 1.\u7ebf\u6027\u56de\u5f52----\u6b64\u5904\u4f7f\u7528\u57fa\u6a21\u578b\uff0c\u53c2\u6570\u5747\u4f7f\u7528\u9ed8\u8ba4\u7684\uff0c\u7528\u4e8e\u540e\u9762\u8c03\u53c2\u5bf9\u6bd4\r\n# Lr=LinearRegression(normalize=False)\r\n# Lr.fit(dujiqix_train,dujiqiy_train)\r\n# xy_predict=Lr.predict(dujiqix_test)\r\n# xy_modify = xy_predict*(xy_predict>=0)\r\n# r2=r2_score(dujiqiy_test,xy_modify)\r\n# # Mae=mean_squared_error(y_test,xy_modify)\r\n# r2\r\n # \u6570\u636e\u9006\u5f52\u4e00\u5316\r\nmaxmin = [datal['PM2.5'].max(), datal['PM2.5'].min()]  # \u539f\u6765\u7684\u6700\u5927\u6700\u5c0f\u503c\uff0c\u53cd\u653e\u7f29\r\nprint(maxmin)\r\n# xpreds = np.array(xy_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# xlabels = np.array(dujiqiy_test)\r\n# xpreds= xpreds.reshape((-1,1))\r\n# xfuture_len= xpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(xfuture_len):\r\n#     xlabels[:, k] = xlabels[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\n# for k in range(xfuture_len):\r\n#     xpreds[:, k] = xpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"dt",children:"\u51b3\u7b56\u6811"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# tree_model=DecisionTreeRegressor()\r\n# tree_model.fit(dux_train,duy_train)\r\n# treey_pre=tree_model.predict(dux_test)\r\n# ty_modify = treey_pre*(treey_pre>=0)\r\n# jr=r2_score(duy_test,ty_modify)\r\n# # treemae2=mean_squared_error(y_test,ty_modify2)\r\n# # print(jr2,treemae2)\r\n# jr\r\n#  # \u6570\u636e\u9006\u5f52\u4e00\u5316\r\n# maxmin = [datal['PM2.5'].max(), datal['PM2.5'].min()]  # \u539f\u6765\u7684\u6700\u5927\u6700\u5c0f\u503c\uff0c\u53cd\u653e\u7f29\r\n# print(maxmin)\r\n# preds = np.array(ty_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(duy_test)\r\n# preds= preds.reshape((-1,1))\r\n# future_len= preds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(future_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\n# for k in range(future_len):\r\n#     preds[:, k] = preds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"rf",children:"\u968f\u673a\u68ee\u6797"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.ensemble import RandomForestRegressor\r\nrd=RandomForestRegressor(max_depth=11,max_features=6,min_samples_split=35,n_estimators=96,min_samples_leaf=9)     #\u57fa\u6a21\u578b\r\nrd.fit(dujiqix_train,dujiqiy_train)\r\nsuiy_pres=rd.predict(dujiqix_test)\r\nyrand_modify = suiy_pres*(suiy_pres>=0)\r\nsuijir=r2_score(dujiqiy_test,yrand_modify)\r\nlabel = np.array(dujiqiy_test)\r\nfor k in range(1):\r\n#     print(k)\r\n    label[:, k] = label[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nnp.array(dujiqiy_test)\r\nspreds = np.array(yrand_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(dujiqiy_test)\r\nspreds= spreds.reshape((-1,1))\r\nsfuture_len= spreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(sfuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(sfuture_len):\r\n    spreds[:, k] = spreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"ab",children:"Adaboost"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.ensemble import AdaBoostRegressor\r\n# \u4e5f\u53ef\u4ee5\u5bf9\u9009\u62e9\u7684\u5f31\u56de\u5f52\u5668\u8fdb\u884c\u53c2\u6570\u9009\u62e9\uff0c\u65b9\u5f0f\u4e3abase_estimator__\u82e5\u56de\u5f52\u5668\u53c2\u6570\u540d\r\nAda_model=AdaBoostRegressor(DecisionTreeRegressor(max_depth=11, min_samples_split=35, min_samples_leaf=35),\r\n                            n_estimators=50,learning_rate=0.1,loss='linear')\r\nAda_model.fit(dujiqix_train,dujiqiy_train)\r\naday_pres=Ada_model.predict(dujiqix_test)\r\naday_modify = aday_pres*(aday_pres>=0)\r\nadar=r2_score(dujiqiy_test,aday_modify)\r\napreds = np.array(aday_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(dujiqiy_test)\r\napreds= apreds.reshape((-1,1))\r\nafuture_len= apreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(afuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(afuture_len):\r\n    apreds[:, k] = apreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"gbrt",children:"GBRT"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.ensemble import GradientBoostingRegressor\r\nGbrt_model= GradientBoostingRegressor()#\u8fd9\u91cc\u4f7f\u752850\u4e2a\u51b3\u7b56\u6811\r\nGbrt_model.fit(dujiqix_train,dujiqiy_train)\r\ngbrty_pres=Gbrt_model.predict(dujiqix_test)\r\ngbrty_modify = gbrty_pres*(gbrty_pres>=0)\r\ngbrtr=r2_score(dujiqiy_test,gbrty_modify)\r\ngpreds = np.array(gbrty_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels2 = np.array(dujiqiy_test)\r\ngpreds= gpreds.reshape((-1,1))\r\ngfuture_len= gpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(gfuture_len):\r\n#     labels2[:, k] = labels2[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(gfuture_len):\r\n    gpreds[:, k] = gpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"xb",children:"XGBoost"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from xgboost import XGBRegressor\r\n\r\nother_params = {'learning_rate': 0.1, 'n_estimators': 75, 'max_depth': 4, 'min_child_weight': 5, \r\n                     'colsample_bytree': 0.7, 'gamma': 0.01,}\r\nxgb_model1=XGBRegressor(**other_params)\r\nxgb_model1.fit(dujiqix_train,dujiqiy_train)\r\nxgby1_pred=xgb_model1.predict(dujiqix_test)\r\nxgby1_modify = xgby1_pred*(xgby1_pred>=0)\r\nxgbr1=r2_score(dujiqiy_test,xgby1_modify)\r\n\r\nother_params1 = {'learning_rate': 0.1, 'n_estimators': 70, 'max_depth': 4, 'min_child_weight': 5, \r\n                     'colsample_bytree': 0.7, 'gamma': 0.01,\r\n                 }\r\nxgb_model=XGBRegressor(**other_params1)\r\nxgb_model.fit(dujiqix_train,dujiqiy_train)\r\nxgby_pred=xgb_model.predict(dujiqix_test)\r\nxgby_modify = xgby_pred*(xgby_pred>=0)\r\nxgbr=r2_score(dujiqiy_test,xgby_modify)\r\nxgbpreds = np.array(xgby1_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels3 = np.array(dujiqiy_test)\r\nxgbpreds= xgbpreds.reshape((-1,1))\r\nxfuture_len= xgbpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(xfuture_len):\r\n#     labels3[:, k] = labels3[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor j in range(xfuture_len):\r\n    xgbpreds[:, j] = xgbpreds[:, j] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"lg",children:"lightGBM"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from lightgbm import LGBMRegressor\r\nimport lightgbm as lgb\r\ntmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=36,\r\n                              learning_rate=0.01, n_estimators=430, max_depth=8, \r\n                              metric='rmse', min_child_samples=4,\r\n                            )#reg_alpha=0.5,reg_lambda=0.5,feature_fraction=0.6,bagging_fraction=0.4,\r\ntmodel_lgb.fit(dujiqix_train,dujiqiy_train)\r\nlgby=tmodel_lgb.predict(dujiqix_test)\r\nlgby_modify=lgby*(lgby>=0)\r\nlgbr=r2_score(dujiqiy_test,lgby_modify)\r\n\r\ntmodel_lgb1 = lgb.LGBMRegressor(objective='regression',num_leaves=30,\r\n                              learning_rate=0.01, n_estimators=500, max_depth=8, \r\n                              metric='rmse', min_child_samples=5,\r\n                            )#reg_alpha=0.5,reg_lambda=0.5,feature_fraction=0.6,bagging_fraction=0.4,\r\ntmodel_lgb1.fit(dujiqix_train,dujiqiy_train)\r\nlgby1=tmodel_lgb1.predict(dujiqix_test)\r\nlgby_modify1=lgby1*(lgby1>=0)\r\nlgbr1=r2_score(dujiqiy_test,lgby_modify1)\r\n\r\nlpreds1 = np.array(lgby_modify1)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# llabel = np.array(dujiqiy_test)\r\nlpreds1= lpreds1.reshape((-1,1))\r\nlfuture_len= lpreds1.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(lfuture_len):\r\n#     llabel[:, k] = llabel[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(lfuture_len):\r\n    lpreds1[:, k] = lpreds1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"svr",children:"SVR"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.svm import SVR\r\nsvr_rbf = SVR(kernel='linear', C=10, gamma=1)\r\nsvr_rbf.fit(dujiqix_train,dujiqiy_train)\r\nsvry_pred= svr_rbf.predict(dujiqix_test)\r\nsvry_modify= svry_pred*(svry_pred>=0)\r\n\r\nsvrr=r2_score(dujiqiy_test,svry_modify)\r\nsvr_rbf2 = SVR(kernel='rbf', C=10, gamma=1)\r\nsvr_rbf2.fit(dujiqix_train,dujiqiy_train)\r\nsvry_pred2= svr_rbf2.predict(dujiqix_test)\r\nsvry_modify2 = svry_pred2*(svry_pred2>=0)\r\nsvrr2=r2_score(dujiqiy_test,svry_modify2)\r\nsvrpreds = np.array(svry_modify2)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(y_test)\r\nsvrpreds= svrpreds.reshape((-1,1))\r\nsvrfuture_len= svrpreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(sfuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(svrfuture_len):\r\n    svrpreds[:, k] = svrpreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\n"})}),"\n",(0,s.jsx)(e.h4,{id:"las",children:"lasso\u56de\u5f52"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from sklearn.linear_model import LassoCV \r\nLa=LassoCV()\r\nLa.fit(dujiqix_train,dujiqiy_train)\r\nlay_predict=La.predict(dujiqix_test)\r\nlay_modify = lay_predict*(lay_predict>=0)\r\nrla=r2_score(dujiqiy_test,lay_modify)\r\nlapreds = np.array(lay_modify)  # \u8f6c\u6362\u4e3anumpy\u5f62\u5f0f\r\n# labels1 = np.array(y_test)\r\nlapreds= lapreds.reshape((-1,1))\r\nlafuture_len= lapreds.shape[1]  # \u83b7\u53d6\u8981\u9884\u6d4b\u7684\u5929\u6570\u957f\u5ea6\uff0c8\u9884\u6d4b5\u7684\u8bdd\u90a3\u5c31\u662f5\r\n# for k in range(lafuture_len):\r\n#     labels1[:, k] = labels1[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\nfor k in range(lafuture_len):\r\n    lapreds[:, k] = lapreds[:, k] * (maxmin[0] - maxmin[1]) + maxmin[1]  # \u53cd\u5f52\u4e00\u5316\r\n    \n"})}),"\n",(0,s.jsx)(e.h4,{id:"lstm",children:"LSTM"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# \u8bfb\u53d6\u8bad\u7ec3\u96c6\r\ndushendux_train=pd.read_csv('shendux_train.csv')\r\ndushendux_train1=np.array(dushendux_train)\r\ndushendux_train2=np.reshape(dushendux_train1,(-1,120,7))\r\n\r\ndushenduy_train=pd.read_csv('shenduy_train.csv')\r\ndushenduy_train2=np.array(dushenduy_train)\r\n# \u8bfb\u53d6\u6d4b\u8bd5\u96c6\r\ndushendux_test=pd.read_csv('shendux_test.csv')\r\ndushendux_test1=np.array(dushendux_test)\r\ndushendux_test2=np.reshape(dushendux_test1,(-1,120,7))\r\n\r\ndushenduy_test=pd.read_csv('shenduy_test.csv')\r\ndushenduy_test2=np.array(dushenduy_test)\r\n\r\n# # # \u6784\u9020\u6279\u6570\u636e\r\ndef create_batch_dataset(x,y,train=True,buffer_size=1000,batch_size=64):#buffer_size=1000\u8868\u793a\u53ef\u4ee5\u6253\u4e71\u7a97\u53e3\u91cc\u9762\u7684\u6570\u636e\r\n    batch_data=tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))#\u6570\u636e\u5c01\u88c5\uff0ctensor\u7c7b\u578b\r\n    if train:\r\n        return batch_data.cache().shuffle(buffer_size).batch(batch_size)\r\n    else:\r\n        return batch_data.batch(batch_size)\r\ntrain_data_single=create_batch_dataset(dushendux_train2, dushenduy_train2,train=True)\r\nval_data_single=create_batch_dataset(dushendux_test2,dushenduy_test2,train=False)\r\n\r\n# \u6dfb\u52a0\u6ce8\u610f\u529b\u673a\u5236\r\ndef attention_bilstm(inputs):\r\n    \"\"\"\r\n        Many-to-one attention mechanism for Keras.\r\n        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\r\n        @return: 2D tensor with shape (batch_size, 128)\r\n        @author: felixhao28, philipperemy.\r\n        \"\"\"\r\n    hidden_states = inputs\r\n    hidden_size = int(hidden_states.shape[2])\r\n        # Inside dense layer\r\n        #              hidden_states            dot               W            =>           score_first_part\r\n        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\r\n        # W is the trainable weight matrix of attention Luong's multiplicative style score\r\n    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\r\n        #            score_first_part           dot        last_hidden_state     => attention_weights\r\n        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\r\n    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\r\n    score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\r\n    attention_weights = Activation('softmax', name='attention_weight')(score)\r\n        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\r\n    context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\r\n    pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\r\n    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\r\n    return attention_vector\r\n\r\n# \u6784\u9020BI_GRU\r\n# attbilstm_model = Sequential()\r\ntime_steps=120\r\ninput_dim=7\r\nmodel_input=Input(shape=(time_steps,input_dim))\r\nbi_gru1=Bidirectional(LSTM(128,return_sequences=True))(model_input)\r\nbi_gru2=Dropout(0.2)(bi_gru1)\r\nbi_gru3=Bidirectional(LSTM(64,return_sequences=True))(bi_gru2)\r\n# attbilstm_model.add(Bidirectional(GRU(64), input_shape=trainx.shape[-2:],return_sequences=True))\r\nattbigru=attention_bilstm(bi_gru3)\r\n# a=Dropout(0.2)(attbigru)\r\nzhong=Dense(1)(attbigru)\r\nattbilstm_model=Model(model_input,zhong)\r\nattbilstm_model.compile(optimizer='adam', loss='mae')# metrics=['accuracy']\r\nattbilstm_history = attbilstm_model.fit(train_data_single, validation_data=val_data_single,epochs=800, verbose=1)\r\n\r\n# \u9884\u6d4b\r\nattbilstm_pres=attbilstm_model.predict(dushendux_test2,verbose=1)\r\nattbilstm_modify = attbilstm_pres*(attbilstm_pres>=0)\r\nlstmr1=r2_score(dushenduy_test2,attbilstm_modify)\n"})}),"\n",(0,s.jsx)(e.h4,{id:"cga",children:"CNN-GRU-AE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# \u8bbe\u7f6e\u65f6\u95f4\u4e3a\u6570\u636e\u7684\u7d22\u5f15\r\ndatacol.index = datacol['day']\r\nblueBerry = ['hardness', 'L', 'a', 'b', 'detE', 'solubleSolid',\r\n       'titrableAcid','weightLoss','rotRate','anthocyanin','totalPhenol','flavonoid','shelfLife']\r\ndatag = datacol[blueBerry]\r\ndatal = datacol[blueBerry]\r\n\r\n# \u7279\u5f81\u9009\u62e9\u540e\u7684\u6570\u636e\r\nfeature_selection = ['totalPhenol', 'titrableAcid', 'flavonoid', 'detE', 'rotRate', 'weightLoss','hardness','shelfLife']\r\ndatal = datal[feature_selection]\r\n\r\ndef prepare_data(data, seq_length):\r\n    X, y = [], []\r\n    for i in range(len(data) - seq_length):\r\n        seq = data.iloc[i:i + seq_length].values  # \u83b7\u53d6\u8fde\u7eed\u7684seq_length\u884c\uff0c\u5e76\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\r\n        X.append(seq)\r\n        y.append(data.iloc[i + seq_length].values)  # \u83b7\u53d6\u4e0b\u4e00\u884c\uff0c\u5e76\u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u4f5c\u4e3a\u6807\u7b7e\r\n    return np.array(X), np.array(y)\r\n# \u8f6c\u6362\u4e3a PyTorch \u5f20\u91cf\r\nX_tensor = torch.from_numpy(X).float()\r\ny_tensor = torch.from_numpy(y).float()\r\n\r\n# \u4f7f\u7528 DataLoader \u5c01\u88c5\u6570\u636e\r\ndataset = TensorDataset(X_tensor, y_tensor)\r\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u6a21\u578b\u7ed3\u6784"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"Attention"})," \u7c7b\u662f\u4e00\u4e2a PyTorch \u7684 ",(0,s.jsx)(e.code,{children:"nn.Module"})," \u5b50\u7c7b\uff0c\u7528\u4e8e\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236\u3002\u5728\u521d\u59cb\u5316\u65b9\u6cd5\u4e2d\uff0c\u5b83\u521b\u5efa\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42\uff0c\u7528\u4e8e\u5c06\u9690\u85cf\u72b6\u6001\u548c\u7f16\u7801\u5668\u8f93\u51fa\u7684\u5408\u5e76\u7ed3\u679c\u6620\u5c04\u5230\u4e00\u4e2a\u4e2d\u95f4\u7ef4\u5ea6\u3002\u8fd9\u91cc\u7684 ",(0,s.jsx)(e.code,{children:"input_size * 2"})," \u5e94\u8be5\u662f ",(0,s.jsx)(e.code,{children:"hidden_size * 2"}),"\uff0c\u56e0\u4e3a ",(0,s.jsx)(e.code,{children:"input_size"})," \u5728\u8fd9\u91cc\u6ca1\u6709\u5b9a\u4e49\uff0c\u800c ",(0,s.jsx)(e.code,{children:"seq_length"})," \u4e5f\u6ca1\u6709\u5728\u7c7b\u4e2d\u5b9a\u4e49\uff0c\u5b83\u5e94\u8be5\u662f\u5e8f\u5217\u7684\u957f\u5ea6\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.code,{children:"NN_GRU_Model"})," \u7c7b\u4e5f\u662f\u4e00\u4e2a ",(0,s.jsx)(e.code,{children:"nn.Module"})," \u5b50\u7c7b\uff0c\u5b83\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5305\u542bCNN\u5c42\u3001GRU\u5c42\u548c\u5168\u8fde\u63a5\u5c42\u7684\u6a21\u578b\uff0c\u5e76\u6dfb\u52a0\u4e86\u4e00\u4e2a ",(0,s.jsx)(e.code,{children:"Attention"})," \u5c42\u3002\u5728\u521d\u59cb\u5316\u65b9\u6cd5\u4e2d\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u4e00\u7ef4\u5377\u79ef\u5c42 ",(0,s.jsx)(e.code,{children:"cnn_layer"}),"\uff0c\u8ba1\u7b97\u4e86\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u5927\u5c0f\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2aGRU\u5c42 ",(0,s.jsx)(e.code,{children:"gru_layer"}),"\uff0c\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 ",(0,s.jsx)(e.code,{children:"fc"}),"\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6ce8\u610f\u529b\u5c42 ",(0,s.jsx)(e.code,{children:"attention"}),"\u3002"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["\u5728 ",(0,s.jsx)(e.code,{children:"forward"})," \u65b9\u6cd5\u4e2d\uff0c\u9996\u5148\u5c06\u8f93\u5165 ",(0,s.jsx)(e.code,{children:"x"})," \u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\u4ea4\u6362\u4f4d\u7f6e\u4ee5\u9002\u5e94\u5377\u79ef\u5c42\u7684\u8981\u6c42\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5377\u79ef\u5c42 ",(0,s.jsx)(e.code,{children:"cnn_layer"})," \u5904\u7406\u8f93\u5165\u3002\u63a5\u7740\uff0c\u91cd\u65b0\u6392\u5217\u7ef4\u5ea6\u4ee5\u9002\u5e94GRU\u5c42\u7684\u8f93\u5165\u8981\u6c42\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u5c42 ",(0,s.jsx)(e.code,{children:"attention"})," \u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6743\u91cd\u5e94\u7528\u5230CNN\u7684\u8f93\u51fa\u4e0a\u4ee5\u83b7\u5f97\u4e0a\u4e0b\u6587\u5411\u91cf ",(0,s.jsx)(e.code,{children:"context"}),"\u3002\u7136\u540e\uff0c\u5c06\u4e0a\u4e0b\u6587\u5411\u91cf\u901a\u8fc7GRU\u5c42 ",(0,s.jsx)(e.code,{children:"gru_layer"})," \u5904\u7406\uff0c\u5e76\u6700\u7ec8\u901a\u8fc7\u5168\u8fde\u63a5\u5c42 ",(0,s.jsx)(e.code,{children:"fc"})," \u5f97\u5230\u8f93\u51fa\u3002"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",metastring:'title="CNN-GRU-AE"',children:"# \u6dfb\u52a0\u6ce8\u610f\u529b\u673a\u5236\r\nclass Attention(nn.Module):\r\n    def __init__(self, hidden_size):\r\n        super(Attention, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.attn = nn.Linear(input_size * 2, seq_length)\r\n\r\n    def forward(self, hidden, encoder_outputs):\r\n        seq_len = encoder_outputs.size(1)\r\n        # h = hidden.repeat(1, seq_len, 1)\r\n        h = hidden\r\n        energy = torch.tanh(self.attn(torch.cat((h, encoder_outputs), dim=2)))\r\n        attention_weights = F.softmax(energy, dim=1)\r\n        context_vector = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)\r\n        return context_vector\r\n\r\n# \u5b9a\u4e49\u6a21\u578b\r\nclass CNN_GRU_Model(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers, kernel_size):\r\n        super(CNN_GRU_Model, self).__init__()\r\n        # \u589e\u52a0\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u901a\u9053\u6570\r\n        self.cnn_layer = nn.Conv1d(in_channels=input_size, out_channels=input_size, kernel_size=kernel_size)\r\n        self.conv_output_size = input_size - kernel_size + 1  # \u8ba1\u7b97\u5377\u79ef\u5c42\u8f93\u51fa\u5927\u5c0f\r\n        self.gru_layer = nn.GRU(self.conv_output_size, hidden_size, num_layers, batch_first=True)\r\n        self.fc = nn.Linear(hidden_size, input_size)\r\n\r\n        # \u6dfb\u52a0\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c42\r\n        self.attention = Attention(hidden_size)\r\n\r\n    def forward(self, x):\r\n        # \u8f6c\u7f6e\u8f93\u5165\u4ee5\u9002\u5e94\u5377\u79ef\u5c42\u7684\u8981\u6c42\r\n        x = x.transpose(1, 2)  # \u5c06\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\u4ea4\u6362\u4f4d\u7f6e\uff0c\u4ece(2, 3, 14)\u53d8\u4e3a(2, 14, 3)\r\n\r\n        # CNN\u5c42\r\n        x = self.cnn_layer(x)  # \u5377\u79ef\u5c42\u8f93\u5165\u5f62\u72b6\u4e3a(batch_size, \u7279\u5f81\u6570, \u65f6\u95f4\u6b65\u957f)\r\n        x = x.permute(0, 2, 1)  # \u91cd\u65b0\u6392\u5217\u7ef4\u5ea6\uff0c\u5c06\u5e8f\u5217\u957f\u5ea6\u653e\u5728\u7b2c 1 \u7ef4\r\n\r\n        # \u6ce8\u610f\u529b\u673a\u5236\r\n        attn_weights = self.attention(x, x)\r\n        # \u5c06\u6ce8\u610f\u529b\u6743\u91cd\u5e94\u7528\u5230CNN\u7684\u8f93\u51fa\u4e0a\r\n        context = torch.bmm(attn_weights.transpose(1,2), x).squeeze(1)\r\n        # GRU\u5c42\r\n        _, h_n = self.gru_layer(context)\r\n\r\n        # \u5168\u8fde\u63a5\u5c42\r\n        out = self.fc(h_n[-1])\r\n\r\n        return out\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u6a21\u578b\u8bad\u7ec3"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"# \u521d\u59cb\u5316\u6a21\u578b\r\ninput_size = X.shape[2]  # \u7279\u5f81\u6570\r\nhidden_size = 64\r\nnum_layers = 1\r\nkernel_size = 1\r\nmodel = CNN_GRU_Model(input_size, hidden_size, num_layers, kernel_size)\r\n# \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\r\ncriterion = nn.MSELoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nnum_epochs = 1000\r\nfor epoch in range(num_epochs):\r\n    for inputs, targets in dataloader:\r\n        outputs = model(inputs)\r\n        loss = criterion(outputs, targets)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n    if (epoch+1) % 100 == 0:\r\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\r\n\r\n# \u9884\u6d4b\u7684\u503c\r\nwith torch.no_grad():\r\n    last_sequence = torch.from_numpy(data[-seq_length:].values).float().unsqueeze(0)\r\n    prediction = model(last_sequence)\r\n    print(\"\u9884\u6d4b\u7ed3\u679c:\", prediction.numpy())\r\n\r\n# \u4fdd\u5b58\u6a21\u578b\r\ntorch.save(model.state_dict(), '1cnn_gru_ae_3_fs_model.pth')\r\n\n"})}),"\n",(0,s.jsx)(e.h4,{id:"pcbma",children:"Pso_Cnn_BiLSTM_MultAE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from keras.models import Model\r\nfrom keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, BatchNormalization, Flatten\r\nfrom tensorflow.keras.layers import MultiHeadAttention\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport numpy as np\r\n\r\n# \u8f93\u5165\u5f62\u72b6\r\ninput_shape = (X_train_expanded.shape[1], 1)\r\ninputs = Input(shape=input_shape)\r\n\r\n# \u5377\u79ef\u5c42\r\nx = Conv1D(filters=13, kernel_size=3, strides=1, padding='valid')(inputs)\r\nx = MaxPooling1D(pool_size=2)(x)\r\nx = BatchNormalization()(x) \r\n# \u53cc\u5411LSTM\u5c42\r\nx = Bidirectional(LSTM(37, return_sequences=True))(x)\r\n\r\n# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5c42\r\nattention_output = MultiHeadAttention(num_heads=5, key_dim=74)(x, x)  # \u6ce8\u610f\u529b\u673a\u5236\u8f93\u5165\u548c\u8f93\u51fa\u5e94\u76f8\u540c\r\n\r\n# \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u5c55\u5e73\r\nx = Flatten()(attention_output)\r\n\r\n# Dropout\u5c42\r\nx = Dropout(0.3)(x)\r\n\r\n# \u5168\u8fde\u63a5\u5c42\r\noutputs = Dense(1)(x)\r\n\r\n# \u6a21\u578b\u5b9a\u4e49\u548c\u7f16\u8bd1\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=Adam(learning_rate=0.00427), loss='mse', metrics=['mse'])\r\n\r\n# \u6dfb\u52a0ModelCheckpoint\u56de\u8c03\u51fd\u6570\r\ncheckpoint = ModelCheckpoint('model/Pso_Cnn_BiLSTM_MultAE.model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(X_train_expanded, y_train, validation_data=(X_val_expanded, y_val), epochs=300, batch_size=32, verbose=1, callbacks=[checkpoint])\r\n\r\n# \u8f7d\u5165\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel.load_weights('model/Pso_Cnn_BiLSTM_MultAE.model.h5')\r\n\r\n# \u9884\u6d4b\u9a8c\u8bc1\u96c6\r\ny_pred = model.predict(X_val_expanded).flatten()\r\n\r\n# \u8ba1\u7b97MSE, RMSE, R2\r\nmse = mean_squared_error(y_val, y_pred)\r\nrmse = np.sqrt(mse)\r\nr2 = r2_score(y_val, y_pred)\r\n\r\n# \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(history.history['loss'], label='Training Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss Over Epochs')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\r\nprint(\"MSE:\", mse)\r\nprint(\"RMSE:\", rmse)\r\nprint(\"R^2 Score:\", r2)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u9884\u6d4b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'# \u4fdd\u5b58\u6a21\u578b\r\n# model.save("model/Pso_Cnn_BiLSTM_MultAE.model.h5")\r\n# print("\u6a21\u578b\u5df2\u4fdd\u5b58\u4e3aCnn_BiLSTM_MultAE.h5")\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel = load_model(\'model/Pso_Cnn_BiLSTM_MultAE.model.h5\')\r\n# \u5c06\u6d4b\u8bd5\u96c6\u6570\u636e\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u5339\u914dBiLSTM\u7684\u8f93\u5165\u8981\u6c42\r\nX_test_expanded = np.expand_dims(X_test, axis=-1)\r\n\r\n# \u4f7f\u7528evaluate\u65b9\u6cd5\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u548cMSE\r\ntest_loss, test_mse = model.evaluate(X_test_expanded, y_test, verbose=1)\r\n\r\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\r\ny_test_pred = model.predict(X_test_expanded).flatten()\r\n\r\n# \u8ba1\u7b97R^2\u5206\u6570\r\nr2_test = r2_score(y_test, y_test_pred)\r\n\r\n# \u8f93\u51fa\u6240\u6709\u7ed3\u679c\r\nprint("Test Loss:", test_loss)\r\nprint("Test MSE:", test_mse)\r\nprint("R^2 Score on Test Set:", r2_test)\n'})}),"\n",(0,s.jsx)(e.h4,{id:"woa",children:"woa_CNN_BiLSTM_MultAE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from keras.models import Model\r\nfrom keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, BatchNormalization, Flatten\r\nfrom tensorflow.keras.layers import MultiHeadAttention\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport numpy as np\r\n\r\n# \u8f93\u5165\u5f62\u72b6\r\ninput_shape = (X_train_expanded.shape[1], 1)\r\ninputs = Input(shape=input_shape)\r\n\r\n# \u5377\u79ef\u5c42\r\nx = Conv1D(filters=15, kernel_size=3, strides=1, padding='valid')(inputs)\r\nx = MaxPooling1D(pool_size=2)(x)\r\nx = BatchNormalization()(x) \r\n# \u53cc\u5411LSTM\u5c42\r\nx = Bidirectional(LSTM(36, return_sequences=True))(x)\r\n\r\n# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5c42\r\nattention_output = MultiHeadAttention(num_heads=4, key_dim=72)(x, x)  # \u6ce8\u610f\u529b\u673a\u5236\u8f93\u5165\u548c\u8f93\u51fa\u5e94\u76f8\u540c\r\n\r\n# \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u5c55\u5e73\r\nx = Flatten()(attention_output)\r\n\r\n# Dropout\u5c42\r\nx = Dropout(0.3)(x)\r\n\r\n# \u5168\u8fde\u63a5\u5c42\r\noutputs = Dense(1)(x)\r\n\r\n# \u6a21\u578b\u5b9a\u4e49\u548c\u7f16\u8bd1\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=Adam(learning_rate=0.00303), loss='mse', metrics=['mse'])\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(X_train_expanded, y_train, validation_data=(X_val_expanded, y_val), epochs=300, batch_size=32, verbose=1)\r\n\r\n# \u9884\u6d4b\u9a8c\u8bc1\u96c6\r\ny_pred = model.predict(X_val_expanded).flatten()\r\n\r\n# \u8ba1\u7b97MSE, RMSE, R2\r\nmse = mean_squared_error(y_val, y_pred)\r\nrmse = np.sqrt(mse)\r\nr2 = r2_score(y_val, y_pred)\r\n\r\n\r\n# \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(history.history['loss'], label='Training Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss Over Epochs')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\r\nprint(\"MSE:\", mse)\r\nprint(\"RMSE:\", rmse)\r\nprint(\"R^2 Score:\", r2)\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n# \u4fdd\u5b58\u6a21\u578b\r\nmodel.save(\"model/woa_Cnn_BiLSTM_MultAE.model.h5\")\r\nprint(\"\u6a21\u578b\u5df2\u4fdd\u5b58\u4e3aCnn_BiLSTM_MultAE.h5\")\r\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel = load_model('model/woa_Cnn_BiLSTM_MultAE.model.h5')\r\n# \u5c06\u6d4b\u8bd5\u96c6\u6570\u636e\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u5339\u914dBiLSTM\u7684\u8f93\u5165\u8981\u6c42\r\nX_test_expanded = np.expand_dims(X_test, axis=-1)\r\n\r\n# \u4f7f\u7528evaluate\u65b9\u6cd5\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u548cMSE\r\ntest_loss, test_mse = model.evaluate(X_test_expanded, y_test, verbose=1)\r\n\r\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\r\ny_test_pred = model.predict(X_test_expanded).flatten()\r\n\r\n# \u8ba1\u7b97R^2\u5206\u6570\r\nr2_test = r2_score(y_test, y_test_pred)\r\n\r\n# \u8f93\u51fa\u6240\u6709\u7ed3\u679c\r\nprint(\"Test Loss:\", test_loss)\r\nprint(\"Test MSE:\", test_mse)\r\nprint(\"R^2 Score on Test Set:\", r2_test)\n"})}),"\n",(0,s.jsx)(e.h4,{id:"ssa",children:"ssa_CNN_BiLSTM_MultAE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from keras.models import Model\r\nfrom keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, BatchNormalization, Flatten\r\nfrom tensorflow.keras.layers import MultiHeadAttention\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport numpy as np\r\n\r\n# \u8f93\u5165\u5f62\u72b6\r\ninput_shape = (X_train_expanded.shape[1], 1)\r\ninputs = Input(shape=input_shape)\r\n\r\n# \u5377\u79ef\u5c42\r\nx = Conv1D(filters=15, kernel_size=3, strides=1, padding='valid')(inputs)\r\nx = MaxPooling1D(pool_size=2)(x)\r\nx = BatchNormalization()(x) \r\n# \u53cc\u5411LSTM\u5c42\r\nx = Bidirectional(LSTM(37, return_sequences=True))(x)\r\n\r\n# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5c42\r\nattention_output = MultiHeadAttention(num_heads=5, key_dim=74)(x, x)  # \u6ce8\u610f\u529b\u673a\u5236\u8f93\u5165\u548c\u8f93\u51fa\u5e94\u76f8\u540c\r\n\r\n# \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u5c55\u5e73\r\nx = Flatten()(attention_output)\r\n\r\n# Dropout\u5c42\r\nx = Dropout(0.3)(x)\r\n\r\n# \u5168\u8fde\u63a5\u5c42\r\noutputs = Dense(1)(x)\r\n\r\n# \u6a21\u578b\u5b9a\u4e49\u548c\u7f16\u8bd1\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=Adam(learning_rate=0.00360), loss='mse', metrics=['mse'])\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(X_train_expanded, y_train, validation_data=(X_val_expanded, y_val), epochs=300, batch_size=32, verbose=1)\r\n\r\n# \u9884\u6d4b\u9a8c\u8bc1\u96c6\r\ny_pred = model.predict(X_val_expanded).flatten()\r\n\r\n# \u8ba1\u7b97MSE, RMSE, R2\r\nmse = mean_squared_error(y_val, y_pred)\r\nrmse = np.sqrt(mse)\r\nr2 = r2_score(y_val, y_pred)\r\n\r\n\r\n# \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(history.history['loss'], label='Training Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss Over Epochs')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\r\nprint(\"MSE:\", mse)\r\nprint(\"RMSE:\", rmse)\r\nprint(\"R^2 Score:\", r2)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u9884\u6d4b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'import numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n# \u4fdd\u5b58\u6a21\u578b\r\nmodel.save("model/ssa_Cnn_BiLSTM_MultAE.model.h5")\r\nprint("\u6a21\u578b\u5df2\u4fdd\u5b58\u4e3aCnn_BiLSTM_MultAE.h5")\r\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel = load_model(\'model/ssa_Cnn_BiLSTM_MultAE.model.h5\')\r\n# \u5c06\u6d4b\u8bd5\u96c6\u6570\u636e\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u5339\u914dBiLSTM\u7684\u8f93\u5165\u8981\u6c42\r\nX_test_expanded = np.expand_dims(X_test, axis=-1)\r\n\r\n# \u4f7f\u7528evaluate\u65b9\u6cd5\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u548cMSE\r\ntest_loss, test_mse = model.evaluate(X_test_expanded, y_test, verbose=1)\r\n\r\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\r\ny_test_pred = model.predict(X_test_expanded).flatten()\r\n\r\n# \u8ba1\u7b97R^2\u5206\u6570\r\nr2_test = r2_score(y_test, y_test_pred)\r\n\r\n# \u8f93\u51fa\u6240\u6709\u7ed3\u679c\r\nprint("Test Loss:", test_loss)\r\nprint("Test MSE:", test_mse)\r\nprint("R^2 Score on Test Set:", r2_test)\n'})}),"\n",(0,s.jsx)(e.h4,{id:"blma",children:"BiLSTM_MultAE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"from keras.models import Model\r\nfrom keras.layers import Input, Bidirectional, LSTM, Dropout, Dense, Flatten\r\nfrom tensorflow.keras.layers import MultiHeadAttention\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport numpy as np\r\n\r\n# \u8f93\u5165\u5f62\u72b6\r\ninput_shape = (X_train_expanded.shape[1], 1)\r\ninputs = Input(shape=input_shape)\r\n\r\n# \u53cc\u5411LSTM\u5c42\r\nx = Bidirectional(LSTM(32, return_sequences=True))(inputs)\r\n\r\n# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5c42\r\nattention_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\r\n\r\n# \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u5c55\u5e73\r\nx = Flatten()(attention_output)\r\n\r\n# Dropout\u5c42\r\nx = Dropout(0.3)(x)\r\n\r\n# \u5168\u8fde\u63a5\u5c42\r\noutputs = Dense(1)(x)\r\n\r\n# \u6a21\u578b\u5b9a\u4e49\u548c\u7f16\u8bd1\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(X_train_expanded, y_train, validation_data=(X_val_expanded, y_val), epochs=300, batch_size=32, verbose=1)\r\n\r\n# \u9884\u6d4b\u9a8c\u8bc1\u96c6\r\ny_pred = model.predict(X_val_expanded).flatten()\r\n\r\n# \u8ba1\u7b97MSE, RMSE, R2\r\nmse = mean_squared_error(y_val, y_pred)\r\nrmse = np.sqrt(mse)\r\nr2 = r2_score(y_val, y_pred)\r\n\r\n# \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(history.history['loss'], label='Training Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss Over Epochs')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\r\nprint(\"MSE:\", mse)\r\nprint(\"RMSE:\", rmse)\r\nprint(\"R^2 Score:\", r2)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u9884\u6d4b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'# \u4fdd\u5b58\u6a21\u578b\r\nmodel.save("model/BiLSTM_MultiHeadAttention.model.h5")\r\nprint("\u6a21\u578b\u5df2\u4fdd\u5b58\u4e3aBiLSTM_MultiHeadAttention.h5")\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel = load_model(\'model/BiLSTM_MultiHeadAttention.model.h5\')\r\n# \u5c06\u6d4b\u8bd5\u96c6\u6570\u636e\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u5339\u914dBiLSTM\u7684\u8f93\u5165\u8981\u6c42\r\nX_test_expanded = np.expand_dims(X_test, axis=-1)\r\n\r\n# \u4f7f\u7528evaluate\u65b9\u6cd5\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u548cMSE\r\ntest_loss, test_mse = model.evaluate(X_test_expanded, y_test, verbose=1)\r\n\r\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\r\ny_test_pred = model.predict(X_test_expanded).flatten()\r\n\r\n# \u8ba1\u7b97R^2\u5206\u6570\r\nr2_test = r2_score(y_test, y_test_pred)\r\n\r\n# \u8f93\u51fa\u6240\u6709\u7ed3\u679c\r\nprint("Test Loss:", test_loss)\r\nprint("Test MSE:", test_mse)\r\nprint("R^2 Score on Test Set:", r2_test)\n'})}),"\n",(0,s.jsx)(e.h4,{id:"cma",children:"CNN-MultAE"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:"import tensorflow as tf\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\r\nfrom tensorflow.keras.layers import MultiHeadAttention, Reshape\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\nimport numpy as np\r\n\r\n# \u8f93\u5165\u5f62\u72b6\r\ninput_shape = (X_train_expanded.shape[1], 1)\r\ninputs = Input(shape=input_shape)\r\n\r\n# \u5377\u79ef\u5c42\r\nx = Conv1D(filters=16, kernel_size=3, strides=1, padding='valid')(inputs)\r\nx = MaxPooling1D(pool_size=2)(x)\r\n\r\n# \u8c03\u6574\u5377\u79ef\u548c\u6c60\u5316\u540e\u7684\u8f93\u51fa\u4ee5\u9002\u5e94\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u786e\u4fdd\u8f93\u51fa\u81f3\u5c11\u6709\u4e24\u4e2a\u7ef4\u5ea6\r\nsequence_length = x.shape[1]\r\nfeature_dim = x.shape[2]\r\nx = Reshape((sequence_length, feature_dim))(x)\r\n\r\n# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5c42\r\nattention_output = MultiHeadAttention(num_heads=8, key_dim=feature_dim)(x, x)\r\n\r\n# \u5c55\u5e73\u5904\u7406\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u51c6\u5907\u5168\u8fde\u63a5\u5c42\r\nx = Flatten()(attention_output)\r\n\r\n# Dropout\u5c42\r\nx = Dropout(0.3)(x)\r\n\r\n# \u5168\u8fde\u63a5\u5c42\r\noutputs = Dense(1)(x)\r\n\r\n# \u6a21\u578b\u5b9a\u4e49\u548c\u7f16\u8bd1\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\r\n\r\n# \u8bad\u7ec3\u6a21\u578b\r\nhistory = model.fit(X_train_expanded, y_train, validation_data=(X_val_expanded, y_val), epochs=300, batch_size=32, verbose=1)\r\n\r\n# \u9884\u6d4b\u9a8c\u8bc1\u96c6\r\ny_pred = model.predict(X_val_expanded).flatten()\r\n\r\n# \u8ba1\u7b97MSE, RMSE, R2\r\nmse = mean_squared_error(y_val, y_pred)\r\nrmse = np.sqrt(mse)\r\nr2 = r2_score(y_val, y_pred)\r\n\r\n# \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u635f\u5931\u66f2\u7ebf\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(history.history['loss'], label='Training Loss')\r\nplt.plot(history.history['val_loss'], label='Validation Loss')\r\nplt.title('Training and Validation Loss Over Epochs')\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.legend()\r\nplt.show()\r\n\r\nprint(\"MSE:\", mse)\r\nprint(\"RMSE:\", rmse)\r\nprint(\"R^2 Score:\", r2)\n"})}),"\n",(0,s.jsx)(e.p,{children:"\u9884\u6d4b"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-py",children:'# \u4fdd\u5b58\u6a21\u578b\r\nmodel.save("model/CNN_MultiHeadAttention.model.h5")\r\nprint("\u6a21\u578b\u5df2\u4fdd\u5b58\u4e3aCNN_MultiHeadAttention.modelh5")\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u6700\u4f73\u6a21\u578b\r\nmodel = load_model(\'model/CNN_MultiHeadAttention.model.h5\')\r\n# \u5c06\u6d4b\u8bd5\u96c6\u6570\u636e\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4ee5\u5339\u914dBiLSTM\u7684\u8f93\u5165\u8981\u6c42\r\nX_test_expanded = np.expand_dims(X_test, axis=-1)\r\n\r\n# \u4f7f\u7528evaluate\u65b9\u6cd5\u8ba1\u7b97\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u548cMSE\r\ntest_loss, test_mse = model.evaluate(X_test_expanded, y_test, verbose=1)\r\n\r\n# \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\r\ny_test_pred = model.predict(X_test_expanded).flatten()\r\n\r\n# \u8ba1\u7b97R^2\u5206\u6570\r\nr2_test = r2_score(y_test, y_test_pred)\r\n\r\n# \u8f93\u51fa\u6240\u6709\u7ed3\u679c\r\nprint("Test Loss:", test_loss)\r\nprint("Test MSE:", test_mse)\r\nprint("R^2 Score on Test Set:", r2_test)\n'})})]})}function p(r={}){const{wrapper:e}={...(0,a.R)(),...r.components};return e?(0,s.jsx)(e,{...r,children:(0,s.jsx)(_,{...r})}):_(r)}},8453:(r,e,n)=>{n.d(e,{R:()=>i,x:()=>l});var t=n(6540);const s={},a=t.createContext(s);function i(r){const e=t.useContext(a);return t.useMemo((function(){return"function"==typeof r?r(e):{...e,...r}}),[e,r])}function l(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(s):r.components||s:i(r.components),t.createElement(a.Provider,{value:e},r.children)}}}]);